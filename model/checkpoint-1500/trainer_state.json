{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0013333333333333333,
      "grad_norm": 1.7530442476272583,
      "learning_rate": 5.999999999999999e-06,
      "loss": 6.9917,
      "step": 1
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 1.6728376150131226,
      "learning_rate": 1.1999999999999999e-05,
      "loss": 7.4596,
      "step": 2
    },
    {
      "epoch": 0.004,
      "grad_norm": 1.8828002214431763,
      "learning_rate": 1.7999999999999997e-05,
      "loss": 7.1999,
      "step": 3
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 1.8747444152832031,
      "learning_rate": 2.3999999999999997e-05,
      "loss": 7.2079,
      "step": 4
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 1.9459002017974854,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 6.9923,
      "step": 5
    },
    {
      "epoch": 0.008,
      "grad_norm": 1.9134289026260376,
      "learning_rate": 3.5999999999999994e-05,
      "loss": 6.8627,
      "step": 6
    },
    {
      "epoch": 0.009333333333333334,
      "grad_norm": 1.9754605293273926,
      "learning_rate": 4.2e-05,
      "loss": 7.0634,
      "step": 7
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 2.0912890434265137,
      "learning_rate": 4.7999999999999994e-05,
      "loss": 7.4511,
      "step": 8
    },
    {
      "epoch": 0.012,
      "grad_norm": 2.1884586811065674,
      "learning_rate": 5.399999999999999e-05,
      "loss": 7.203,
      "step": 9
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 2.2166435718536377,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 6.9994,
      "step": 10
    },
    {
      "epoch": 0.014666666666666666,
      "grad_norm": 2.4735939502716064,
      "learning_rate": 6.599999999999999e-05,
      "loss": 7.28,
      "step": 11
    },
    {
      "epoch": 0.016,
      "grad_norm": 2.545711040496826,
      "learning_rate": 7.199999999999999e-05,
      "loss": 7.1085,
      "step": 12
    },
    {
      "epoch": 0.017333333333333333,
      "grad_norm": 2.5948541164398193,
      "learning_rate": 7.8e-05,
      "loss": 6.9995,
      "step": 13
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 2.887446880340576,
      "learning_rate": 8.4e-05,
      "loss": 7.0131,
      "step": 14
    },
    {
      "epoch": 0.02,
      "grad_norm": 3.019946813583374,
      "learning_rate": 8.999999999999999e-05,
      "loss": 7.0426,
      "step": 15
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 3.1693272590637207,
      "learning_rate": 9.599999999999999e-05,
      "loss": 6.9326,
      "step": 16
    },
    {
      "epoch": 0.02266666666666667,
      "grad_norm": 3.003511428833008,
      "learning_rate": 0.000102,
      "loss": 6.805,
      "step": 17
    },
    {
      "epoch": 0.024,
      "grad_norm": 3.2932403087615967,
      "learning_rate": 0.00010799999999999998,
      "loss": 6.2984,
      "step": 18
    },
    {
      "epoch": 0.025333333333333333,
      "grad_norm": 3.8172378540039062,
      "learning_rate": 0.00011399999999999999,
      "loss": 6.3436,
      "step": 19
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 3.575146436691284,
      "learning_rate": 0.00011999999999999999,
      "loss": 6.6225,
      "step": 20
    },
    {
      "epoch": 0.028,
      "grad_norm": 4.147861480712891,
      "learning_rate": 0.00012599999999999997,
      "loss": 5.9242,
      "step": 21
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 4.076561450958252,
      "learning_rate": 0.00013199999999999998,
      "loss": 6.2857,
      "step": 22
    },
    {
      "epoch": 0.030666666666666665,
      "grad_norm": 4.271839141845703,
      "learning_rate": 0.000138,
      "loss": 6.2015,
      "step": 23
    },
    {
      "epoch": 0.032,
      "grad_norm": 4.326339244842529,
      "learning_rate": 0.00014399999999999998,
      "loss": 5.7411,
      "step": 24
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 4.217104434967041,
      "learning_rate": 0.00015,
      "loss": 5.7132,
      "step": 25
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 4.32450532913208,
      "learning_rate": 0.000156,
      "loss": 5.4613,
      "step": 26
    },
    {
      "epoch": 0.036,
      "grad_norm": 3.6667637825012207,
      "learning_rate": 0.000162,
      "loss": 5.59,
      "step": 27
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 3.819756031036377,
      "learning_rate": 0.000168,
      "loss": 5.0615,
      "step": 28
    },
    {
      "epoch": 0.03866666666666667,
      "grad_norm": 4.098598957061768,
      "learning_rate": 0.00017399999999999997,
      "loss": 5.09,
      "step": 29
    },
    {
      "epoch": 0.04,
      "grad_norm": 3.927908420562744,
      "learning_rate": 0.00017999999999999998,
      "loss": 4.6112,
      "step": 30
    },
    {
      "epoch": 0.04133333333333333,
      "grad_norm": 4.157209873199463,
      "learning_rate": 0.000186,
      "loss": 4.9002,
      "step": 31
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 4.108109951019287,
      "learning_rate": 0.00019199999999999998,
      "loss": 4.7884,
      "step": 32
    },
    {
      "epoch": 0.044,
      "grad_norm": 4.180082321166992,
      "learning_rate": 0.000198,
      "loss": 4.6514,
      "step": 33
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 3.9739553928375244,
      "learning_rate": 0.000204,
      "loss": 4.2031,
      "step": 34
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 4.303609848022461,
      "learning_rate": 0.00020999999999999998,
      "loss": 4.3052,
      "step": 35
    },
    {
      "epoch": 0.048,
      "grad_norm": 5.5763654708862305,
      "learning_rate": 0.00021599999999999996,
      "loss": 3.9414,
      "step": 36
    },
    {
      "epoch": 0.04933333333333333,
      "grad_norm": 7.59786319732666,
      "learning_rate": 0.00022199999999999998,
      "loss": 3.728,
      "step": 37
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 5.543431758880615,
      "learning_rate": 0.00022799999999999999,
      "loss": 3.5594,
      "step": 38
    },
    {
      "epoch": 0.052,
      "grad_norm": 5.389588356018066,
      "learning_rate": 0.000234,
      "loss": 3.7166,
      "step": 39
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 3.8181262016296387,
      "learning_rate": 0.00023999999999999998,
      "loss": 3.4312,
      "step": 40
    },
    {
      "epoch": 0.05466666666666667,
      "grad_norm": 4.19036865234375,
      "learning_rate": 0.00024599999999999996,
      "loss": 3.7466,
      "step": 41
    },
    {
      "epoch": 0.056,
      "grad_norm": 4.319828033447266,
      "learning_rate": 0.00025199999999999995,
      "loss": 3.2053,
      "step": 42
    },
    {
      "epoch": 0.05733333333333333,
      "grad_norm": 4.316253185272217,
      "learning_rate": 0.000258,
      "loss": 3.2575,
      "step": 43
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 4.655346870422363,
      "learning_rate": 0.00026399999999999997,
      "loss": 3.0904,
      "step": 44
    },
    {
      "epoch": 0.06,
      "grad_norm": 3.284144401550293,
      "learning_rate": 0.00027,
      "loss": 2.7286,
      "step": 45
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 3.8860013484954834,
      "learning_rate": 0.000276,
      "loss": 2.9331,
      "step": 46
    },
    {
      "epoch": 0.06266666666666666,
      "grad_norm": 6.789692401885986,
      "learning_rate": 0.00028199999999999997,
      "loss": 2.7964,
      "step": 47
    },
    {
      "epoch": 0.064,
      "grad_norm": 6.495933532714844,
      "learning_rate": 0.00028799999999999995,
      "loss": 2.8628,
      "step": 48
    },
    {
      "epoch": 0.06533333333333333,
      "grad_norm": 3.1743288040161133,
      "learning_rate": 0.000294,
      "loss": 2.5707,
      "step": 49
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 3.6969919204711914,
      "learning_rate": 0.0003,
      "loss": 2.389,
      "step": 50
    },
    {
      "epoch": 0.068,
      "grad_norm": 3.7598726749420166,
      "learning_rate": 0.0002999189189189189,
      "loss": 2.2407,
      "step": 51
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 3.2402350902557373,
      "learning_rate": 0.0002998378378378378,
      "loss": 2.3993,
      "step": 52
    },
    {
      "epoch": 0.07066666666666667,
      "grad_norm": 2.373568058013916,
      "learning_rate": 0.00029975675675675674,
      "loss": 2.2197,
      "step": 53
    },
    {
      "epoch": 0.072,
      "grad_norm": 2.3747291564941406,
      "learning_rate": 0.0002996756756756756,
      "loss": 2.5922,
      "step": 54
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 4.066727161407471,
      "learning_rate": 0.00029959459459459457,
      "loss": 2.3552,
      "step": 55
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 2.224524736404419,
      "learning_rate": 0.0002995135135135135,
      "loss": 2.4628,
      "step": 56
    },
    {
      "epoch": 0.076,
      "grad_norm": 2.2393205165863037,
      "learning_rate": 0.0002994324324324324,
      "loss": 2.3258,
      "step": 57
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 2.3568711280822754,
      "learning_rate": 0.00029935135135135133,
      "loss": 2.5037,
      "step": 58
    },
    {
      "epoch": 0.07866666666666666,
      "grad_norm": 2.055771589279175,
      "learning_rate": 0.0002992702702702703,
      "loss": 2.8232,
      "step": 59
    },
    {
      "epoch": 0.08,
      "grad_norm": 2.4134271144866943,
      "learning_rate": 0.00029918918918918916,
      "loss": 2.058,
      "step": 60
    },
    {
      "epoch": 0.08133333333333333,
      "grad_norm": 2.3093464374542236,
      "learning_rate": 0.0002991081081081081,
      "loss": 2.1508,
      "step": 61
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 1.7018156051635742,
      "learning_rate": 0.000299027027027027,
      "loss": 2.2697,
      "step": 62
    },
    {
      "epoch": 0.084,
      "grad_norm": 2.304654359817505,
      "learning_rate": 0.00029894594594594593,
      "loss": 1.8876,
      "step": 63
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 1.758334994316101,
      "learning_rate": 0.00029886486486486487,
      "loss": 2.1657,
      "step": 64
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 1.6537948846817017,
      "learning_rate": 0.00029878378378378375,
      "loss": 2.23,
      "step": 65
    },
    {
      "epoch": 0.088,
      "grad_norm": 2.2383241653442383,
      "learning_rate": 0.0002987027027027027,
      "loss": 2.4312,
      "step": 66
    },
    {
      "epoch": 0.08933333333333333,
      "grad_norm": 1.935441017150879,
      "learning_rate": 0.00029862162162162164,
      "loss": 2.4698,
      "step": 67
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 2.0548603534698486,
      "learning_rate": 0.0002985405405405405,
      "loss": 2.093,
      "step": 68
    },
    {
      "epoch": 0.092,
      "grad_norm": 1.8506944179534912,
      "learning_rate": 0.00029845945945945946,
      "loss": 2.4624,
      "step": 69
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 1.789790391921997,
      "learning_rate": 0.00029837837837837835,
      "loss": 2.4746,
      "step": 70
    },
    {
      "epoch": 0.09466666666666666,
      "grad_norm": 1.4665073156356812,
      "learning_rate": 0.00029829729729729723,
      "loss": 2.1788,
      "step": 71
    },
    {
      "epoch": 0.096,
      "grad_norm": 1.7166324853897095,
      "learning_rate": 0.0002982162162162162,
      "loss": 1.9966,
      "step": 72
    },
    {
      "epoch": 0.09733333333333333,
      "grad_norm": 1.4226012229919434,
      "learning_rate": 0.0002981351351351351,
      "loss": 2.0103,
      "step": 73
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 1.7395445108413696,
      "learning_rate": 0.000298054054054054,
      "loss": 2.3333,
      "step": 74
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.8275134563446045,
      "learning_rate": 0.00029797297297297294,
      "loss": 2.2227,
      "step": 75
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 1.6291468143463135,
      "learning_rate": 0.00029789189189189183,
      "loss": 2.2551,
      "step": 76
    },
    {
      "epoch": 0.10266666666666667,
      "grad_norm": 2.0676305294036865,
      "learning_rate": 0.00029781081081081077,
      "loss": 2.2725,
      "step": 77
    },
    {
      "epoch": 0.104,
      "grad_norm": 1.4302048683166504,
      "learning_rate": 0.0002977297297297297,
      "loss": 2.1921,
      "step": 78
    },
    {
      "epoch": 0.10533333333333333,
      "grad_norm": 1.733900547027588,
      "learning_rate": 0.0002976486486486486,
      "loss": 2.474,
      "step": 79
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 1.5150680541992188,
      "learning_rate": 0.00029756756756756753,
      "loss": 2.143,
      "step": 80
    },
    {
      "epoch": 0.108,
      "grad_norm": 2.294008731842041,
      "learning_rate": 0.0002974864864864865,
      "loss": 2.3102,
      "step": 81
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 1.9489974975585938,
      "learning_rate": 0.00029740540540540536,
      "loss": 2.2847,
      "step": 82
    },
    {
      "epoch": 0.11066666666666666,
      "grad_norm": 1.7877960205078125,
      "learning_rate": 0.0002973243243243243,
      "loss": 2.0971,
      "step": 83
    },
    {
      "epoch": 0.112,
      "grad_norm": 1.3895890712738037,
      "learning_rate": 0.0002972432432432432,
      "loss": 2.0742,
      "step": 84
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 1.4017318487167358,
      "learning_rate": 0.00029716216216216213,
      "loss": 2.4538,
      "step": 85
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 1.4116781949996948,
      "learning_rate": 0.00029708108108108107,
      "loss": 2.3701,
      "step": 86
    },
    {
      "epoch": 0.116,
      "grad_norm": 1.8159066438674927,
      "learning_rate": 0.00029699999999999996,
      "loss": 2.4129,
      "step": 87
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 1.400152564048767,
      "learning_rate": 0.0002969189189189189,
      "loss": 2.1112,
      "step": 88
    },
    {
      "epoch": 0.11866666666666667,
      "grad_norm": 1.6109389066696167,
      "learning_rate": 0.00029683783783783784,
      "loss": 2.1552,
      "step": 89
    },
    {
      "epoch": 0.12,
      "grad_norm": 1.4796475172042847,
      "learning_rate": 0.0002967567567567567,
      "loss": 2.2133,
      "step": 90
    },
    {
      "epoch": 0.12133333333333333,
      "grad_norm": 1.7631521224975586,
      "learning_rate": 0.00029667567567567566,
      "loss": 2.3674,
      "step": 91
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 1.55689537525177,
      "learning_rate": 0.00029659459459459455,
      "loss": 2.5028,
      "step": 92
    },
    {
      "epoch": 0.124,
      "grad_norm": 1.5363378524780273,
      "learning_rate": 0.0002965135135135135,
      "loss": 1.867,
      "step": 93
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 1.8108060359954834,
      "learning_rate": 0.00029643243243243243,
      "loss": 2.1223,
      "step": 94
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 1.5015873908996582,
      "learning_rate": 0.0002963513513513513,
      "loss": 2.2372,
      "step": 95
    },
    {
      "epoch": 0.128,
      "grad_norm": 1.8640214204788208,
      "learning_rate": 0.00029627027027027026,
      "loss": 2.3582,
      "step": 96
    },
    {
      "epoch": 0.12933333333333333,
      "grad_norm": 1.4141201972961426,
      "learning_rate": 0.00029618918918918914,
      "loss": 2.1725,
      "step": 97
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 1.4971394538879395,
      "learning_rate": 0.0002961081081081081,
      "loss": 2.2887,
      "step": 98
    },
    {
      "epoch": 0.132,
      "grad_norm": 1.4959444999694824,
      "learning_rate": 0.000296027027027027,
      "loss": 2.2339,
      "step": 99
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 1.599486231803894,
      "learning_rate": 0.0002959459459459459,
      "loss": 1.7901,
      "step": 100
    },
    {
      "epoch": 0.13466666666666666,
      "grad_norm": 1.5853067636489868,
      "learning_rate": 0.00029586486486486485,
      "loss": 2.1017,
      "step": 101
    },
    {
      "epoch": 0.136,
      "grad_norm": 1.4060838222503662,
      "learning_rate": 0.0002957837837837838,
      "loss": 1.9991,
      "step": 102
    },
    {
      "epoch": 0.13733333333333334,
      "grad_norm": 1.685718059539795,
      "learning_rate": 0.0002957027027027027,
      "loss": 2.5453,
      "step": 103
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 1.5971695184707642,
      "learning_rate": 0.0002956216216216216,
      "loss": 2.1598,
      "step": 104
    },
    {
      "epoch": 0.14,
      "grad_norm": 1.4583508968353271,
      "learning_rate": 0.0002955405405405405,
      "loss": 2.1511,
      "step": 105
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 1.42423677444458,
      "learning_rate": 0.00029545945945945944,
      "loss": 2.3377,
      "step": 106
    },
    {
      "epoch": 0.14266666666666666,
      "grad_norm": 1.7085323333740234,
      "learning_rate": 0.0002953783783783784,
      "loss": 2.101,
      "step": 107
    },
    {
      "epoch": 0.144,
      "grad_norm": 2.5482096672058105,
      "learning_rate": 0.00029529729729729727,
      "loss": 1.8865,
      "step": 108
    },
    {
      "epoch": 0.14533333333333334,
      "grad_norm": 2.0256879329681396,
      "learning_rate": 0.0002952162162162162,
      "loss": 1.9354,
      "step": 109
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 1.6345410346984863,
      "learning_rate": 0.00029513513513513515,
      "loss": 2.2454,
      "step": 110
    },
    {
      "epoch": 0.148,
      "grad_norm": 1.5089110136032104,
      "learning_rate": 0.00029505405405405404,
      "loss": 2.1042,
      "step": 111
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 1.4591740369796753,
      "learning_rate": 0.0002949729729729729,
      "loss": 2.2914,
      "step": 112
    },
    {
      "epoch": 0.15066666666666667,
      "grad_norm": 1.3325963020324707,
      "learning_rate": 0.00029489189189189186,
      "loss": 2.0339,
      "step": 113
    },
    {
      "epoch": 0.152,
      "grad_norm": 1.5319820642471313,
      "learning_rate": 0.00029481081081081075,
      "loss": 1.9591,
      "step": 114
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 1.6308103799819946,
      "learning_rate": 0.0002947297297297297,
      "loss": 2.0377,
      "step": 115
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 1.5586155652999878,
      "learning_rate": 0.00029464864864864863,
      "loss": 1.9471,
      "step": 116
    },
    {
      "epoch": 0.156,
      "grad_norm": 1.4275693893432617,
      "learning_rate": 0.0002945675675675675,
      "loss": 1.9607,
      "step": 117
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 1.4447150230407715,
      "learning_rate": 0.00029448648648648646,
      "loss": 2.1721,
      "step": 118
    },
    {
      "epoch": 0.15866666666666668,
      "grad_norm": 1.77349853515625,
      "learning_rate": 0.0002944054054054054,
      "loss": 1.8664,
      "step": 119
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.5012304782867432,
      "learning_rate": 0.0002943243243243243,
      "loss": 1.9654,
      "step": 120
    },
    {
      "epoch": 0.16133333333333333,
      "grad_norm": 1.7064622640609741,
      "learning_rate": 0.0002942432432432432,
      "loss": 1.9275,
      "step": 121
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 1.6405904293060303,
      "learning_rate": 0.0002941621621621621,
      "loss": 2.04,
      "step": 122
    },
    {
      "epoch": 0.164,
      "grad_norm": 1.619753360748291,
      "learning_rate": 0.00029408108108108105,
      "loss": 1.9617,
      "step": 123
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 1.3956174850463867,
      "learning_rate": 0.000294,
      "loss": 1.9581,
      "step": 124
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.6240284442901611,
      "learning_rate": 0.0002939189189189189,
      "loss": 2.3565,
      "step": 125
    },
    {
      "epoch": 0.168,
      "grad_norm": 1.3450000286102295,
      "learning_rate": 0.0002938378378378378,
      "loss": 1.9188,
      "step": 126
    },
    {
      "epoch": 0.16933333333333334,
      "grad_norm": 1.6808300018310547,
      "learning_rate": 0.0002937567567567567,
      "loss": 2.4564,
      "step": 127
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 1.649902582168579,
      "learning_rate": 0.00029367567567567564,
      "loss": 2.3475,
      "step": 128
    },
    {
      "epoch": 0.172,
      "grad_norm": 1.5993455648422241,
      "learning_rate": 0.0002935945945945946,
      "loss": 2.5214,
      "step": 129
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 1.4168981313705444,
      "learning_rate": 0.00029351351351351347,
      "loss": 1.8172,
      "step": 130
    },
    {
      "epoch": 0.17466666666666666,
      "grad_norm": 1.3710507154464722,
      "learning_rate": 0.0002934324324324324,
      "loss": 2.0257,
      "step": 131
    },
    {
      "epoch": 0.176,
      "grad_norm": 1.4788466691970825,
      "learning_rate": 0.00029335135135135135,
      "loss": 2.0773,
      "step": 132
    },
    {
      "epoch": 0.17733333333333334,
      "grad_norm": 1.370531439781189,
      "learning_rate": 0.00029327027027027024,
      "loss": 1.7329,
      "step": 133
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 1.4140888452529907,
      "learning_rate": 0.0002931891891891892,
      "loss": 2.0809,
      "step": 134
    },
    {
      "epoch": 0.18,
      "grad_norm": 1.4115822315216064,
      "learning_rate": 0.00029310810810810806,
      "loss": 2.2595,
      "step": 135
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 1.532710075378418,
      "learning_rate": 0.000293027027027027,
      "loss": 2.4794,
      "step": 136
    },
    {
      "epoch": 0.18266666666666667,
      "grad_norm": 1.7686121463775635,
      "learning_rate": 0.00029294594594594594,
      "loss": 1.9521,
      "step": 137
    },
    {
      "epoch": 0.184,
      "grad_norm": 1.3828660249710083,
      "learning_rate": 0.00029286486486486483,
      "loss": 2.0616,
      "step": 138
    },
    {
      "epoch": 0.18533333333333332,
      "grad_norm": 1.3992712497711182,
      "learning_rate": 0.00029278378378378377,
      "loss": 2.1071,
      "step": 139
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 1.6415252685546875,
      "learning_rate": 0.0002927027027027027,
      "loss": 2.3564,
      "step": 140
    },
    {
      "epoch": 0.188,
      "grad_norm": 1.671992540359497,
      "learning_rate": 0.0002926216216216216,
      "loss": 1.662,
      "step": 141
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 1.599278450012207,
      "learning_rate": 0.00029254054054054054,
      "loss": 2.1398,
      "step": 142
    },
    {
      "epoch": 0.19066666666666668,
      "grad_norm": 1.449185848236084,
      "learning_rate": 0.0002924594594594594,
      "loss": 1.9142,
      "step": 143
    },
    {
      "epoch": 0.192,
      "grad_norm": 1.7382559776306152,
      "learning_rate": 0.00029237837837837836,
      "loss": 1.9829,
      "step": 144
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 2.0466485023498535,
      "learning_rate": 0.0002922972972972973,
      "loss": 2.0284,
      "step": 145
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 1.6478201150894165,
      "learning_rate": 0.0002922162162162162,
      "loss": 2.0403,
      "step": 146
    },
    {
      "epoch": 0.196,
      "grad_norm": 1.4060627222061157,
      "learning_rate": 0.00029213513513513513,
      "loss": 2.0026,
      "step": 147
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 1.6196311712265015,
      "learning_rate": 0.000292054054054054,
      "loss": 2.0522,
      "step": 148
    },
    {
      "epoch": 0.19866666666666666,
      "grad_norm": 1.415720820426941,
      "learning_rate": 0.00029197297297297296,
      "loss": 1.8367,
      "step": 149
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.3708797693252563,
      "learning_rate": 0.0002918918918918919,
      "loss": 2.3616,
      "step": 150
    },
    {
      "epoch": 0.20133333333333334,
      "grad_norm": 1.603875756263733,
      "learning_rate": 0.0002918108108108108,
      "loss": 1.8411,
      "step": 151
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 1.3335810899734497,
      "learning_rate": 0.00029172972972972967,
      "loss": 1.8476,
      "step": 152
    },
    {
      "epoch": 0.204,
      "grad_norm": 1.618055820465088,
      "learning_rate": 0.0002916486486486486,
      "loss": 1.9834,
      "step": 153
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 1.316913366317749,
      "learning_rate": 0.00029156756756756755,
      "loss": 1.8911,
      "step": 154
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 1.685929536819458,
      "learning_rate": 0.00029148648648648644,
      "loss": 2.0483,
      "step": 155
    },
    {
      "epoch": 0.208,
      "grad_norm": 1.5305558443069458,
      "learning_rate": 0.0002914054054054054,
      "loss": 2.0338,
      "step": 156
    },
    {
      "epoch": 0.20933333333333334,
      "grad_norm": 1.5529066324234009,
      "learning_rate": 0.00029132432432432426,
      "loss": 1.9007,
      "step": 157
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 1.5936444997787476,
      "learning_rate": 0.0002912432432432432,
      "loss": 1.7038,
      "step": 158
    },
    {
      "epoch": 0.212,
      "grad_norm": 1.6565196514129639,
      "learning_rate": 0.00029116216216216215,
      "loss": 1.9152,
      "step": 159
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 1.4421921968460083,
      "learning_rate": 0.00029108108108108103,
      "loss": 1.6825,
      "step": 160
    },
    {
      "epoch": 0.21466666666666667,
      "grad_norm": 1.6476855278015137,
      "learning_rate": 0.00029099999999999997,
      "loss": 2.1631,
      "step": 161
    },
    {
      "epoch": 0.216,
      "grad_norm": 1.6026002168655396,
      "learning_rate": 0.0002909189189189189,
      "loss": 2.42,
      "step": 162
    },
    {
      "epoch": 0.21733333333333332,
      "grad_norm": 1.4429713487625122,
      "learning_rate": 0.0002908378378378378,
      "loss": 1.7428,
      "step": 163
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 1.6349551677703857,
      "learning_rate": 0.00029075675675675674,
      "loss": 2.0034,
      "step": 164
    },
    {
      "epoch": 0.22,
      "grad_norm": 1.9781694412231445,
      "learning_rate": 0.0002906756756756756,
      "loss": 1.735,
      "step": 165
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 1.5174843072891235,
      "learning_rate": 0.00029059459459459457,
      "loss": 2.1979,
      "step": 166
    },
    {
      "epoch": 0.22266666666666668,
      "grad_norm": 1.8555582761764526,
      "learning_rate": 0.0002905135135135135,
      "loss": 2.2242,
      "step": 167
    },
    {
      "epoch": 0.224,
      "grad_norm": 1.4739327430725098,
      "learning_rate": 0.0002904324324324324,
      "loss": 1.9623,
      "step": 168
    },
    {
      "epoch": 0.22533333333333333,
      "grad_norm": 1.666995882987976,
      "learning_rate": 0.00029035135135135133,
      "loss": 2.0808,
      "step": 169
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 1.3919615745544434,
      "learning_rate": 0.0002902702702702702,
      "loss": 1.9226,
      "step": 170
    },
    {
      "epoch": 0.228,
      "grad_norm": 1.4842491149902344,
      "learning_rate": 0.00029018918918918916,
      "loss": 1.7193,
      "step": 171
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 1.3942996263504028,
      "learning_rate": 0.0002901081081081081,
      "loss": 1.7631,
      "step": 172
    },
    {
      "epoch": 0.23066666666666666,
      "grad_norm": 1.6900635957717896,
      "learning_rate": 0.000290027027027027,
      "loss": 2.2582,
      "step": 173
    },
    {
      "epoch": 0.232,
      "grad_norm": 1.3621762990951538,
      "learning_rate": 0.0002899459459459459,
      "loss": 1.8757,
      "step": 174
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 1.4708086252212524,
      "learning_rate": 0.00028986486486486487,
      "loss": 2.0826,
      "step": 175
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 1.5791910886764526,
      "learning_rate": 0.00028978378378378375,
      "loss": 2.2866,
      "step": 176
    },
    {
      "epoch": 0.236,
      "grad_norm": 1.5430221557617188,
      "learning_rate": 0.0002897027027027027,
      "loss": 2.2121,
      "step": 177
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 1.576736330986023,
      "learning_rate": 0.0002896216216216216,
      "loss": 1.889,
      "step": 178
    },
    {
      "epoch": 0.23866666666666667,
      "grad_norm": 1.3296993970870972,
      "learning_rate": 0.0002895405405405405,
      "loss": 1.7997,
      "step": 179
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.5280767679214478,
      "learning_rate": 0.00028945945945945946,
      "loss": 1.8346,
      "step": 180
    },
    {
      "epoch": 0.24133333333333334,
      "grad_norm": 1.4949140548706055,
      "learning_rate": 0.00028937837837837835,
      "loss": 1.5523,
      "step": 181
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 1.4376517534255981,
      "learning_rate": 0.0002892972972972973,
      "loss": 2.0459,
      "step": 182
    },
    {
      "epoch": 0.244,
      "grad_norm": 1.6049460172653198,
      "learning_rate": 0.0002892162162162162,
      "loss": 2.0505,
      "step": 183
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 1.6893960237503052,
      "learning_rate": 0.0002891351351351351,
      "loss": 2.4677,
      "step": 184
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 1.3081308603286743,
      "learning_rate": 0.00028905405405405405,
      "loss": 2.1004,
      "step": 185
    },
    {
      "epoch": 0.248,
      "grad_norm": 1.3691414594650269,
      "learning_rate": 0.00028897297297297294,
      "loss": 2.258,
      "step": 186
    },
    {
      "epoch": 0.24933333333333332,
      "grad_norm": 1.518257737159729,
      "learning_rate": 0.0002888918918918919,
      "loss": 1.6683,
      "step": 187
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 1.5764548778533936,
      "learning_rate": 0.0002888108108108108,
      "loss": 2.0648,
      "step": 188
    },
    {
      "epoch": 0.252,
      "grad_norm": 1.7078126668930054,
      "learning_rate": 0.0002887297297297297,
      "loss": 1.725,
      "step": 189
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 1.346234917640686,
      "learning_rate": 0.00028864864864864865,
      "loss": 1.8321,
      "step": 190
    },
    {
      "epoch": 0.25466666666666665,
      "grad_norm": 1.439349889755249,
      "learning_rate": 0.00028856756756756753,
      "loss": 2.0865,
      "step": 191
    },
    {
      "epoch": 0.256,
      "grad_norm": 1.621825098991394,
      "learning_rate": 0.0002884864864864864,
      "loss": 1.9572,
      "step": 192
    },
    {
      "epoch": 0.25733333333333336,
      "grad_norm": 1.5824347734451294,
      "learning_rate": 0.00028840540540540536,
      "loss": 2.0659,
      "step": 193
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 1.6806148290634155,
      "learning_rate": 0.0002883243243243243,
      "loss": 2.2685,
      "step": 194
    },
    {
      "epoch": 0.26,
      "grad_norm": 1.9743064641952515,
      "learning_rate": 0.0002882432432432432,
      "loss": 2.2686,
      "step": 195
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 1.3854607343673706,
      "learning_rate": 0.0002881621621621621,
      "loss": 1.7036,
      "step": 196
    },
    {
      "epoch": 0.26266666666666666,
      "grad_norm": 1.9875355958938599,
      "learning_rate": 0.00028808108108108107,
      "loss": 2.1977,
      "step": 197
    },
    {
      "epoch": 0.264,
      "grad_norm": 1.5336999893188477,
      "learning_rate": 0.00028799999999999995,
      "loss": 1.9996,
      "step": 198
    },
    {
      "epoch": 0.2653333333333333,
      "grad_norm": 1.7089309692382812,
      "learning_rate": 0.0002879189189189189,
      "loss": 1.9563,
      "step": 199
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.5327203273773193,
      "learning_rate": 0.0002878378378378378,
      "loss": 1.9659,
      "step": 200
    },
    {
      "epoch": 0.268,
      "grad_norm": 1.5249626636505127,
      "learning_rate": 0.0002877567567567567,
      "loss": 1.7984,
      "step": 201
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 1.5910879373550415,
      "learning_rate": 0.00028767567567567566,
      "loss": 1.8642,
      "step": 202
    },
    {
      "epoch": 0.27066666666666667,
      "grad_norm": 1.3911398649215698,
      "learning_rate": 0.00028759459459459455,
      "loss": 1.779,
      "step": 203
    },
    {
      "epoch": 0.272,
      "grad_norm": 1.4424396753311157,
      "learning_rate": 0.0002875135135135135,
      "loss": 2.0292,
      "step": 204
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 1.9363434314727783,
      "learning_rate": 0.00028743243243243243,
      "loss": 2.0279,
      "step": 205
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 1.5273808240890503,
      "learning_rate": 0.0002873513513513513,
      "loss": 2.2531,
      "step": 206
    },
    {
      "epoch": 0.276,
      "grad_norm": 1.5872628688812256,
      "learning_rate": 0.00028727027027027025,
      "loss": 2.1208,
      "step": 207
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 1.388000249862671,
      "learning_rate": 0.00028718918918918914,
      "loss": 2.0267,
      "step": 208
    },
    {
      "epoch": 0.2786666666666667,
      "grad_norm": 1.7576686143875122,
      "learning_rate": 0.0002871081081081081,
      "loss": 2.0039,
      "step": 209
    },
    {
      "epoch": 0.28,
      "grad_norm": 3.7595112323760986,
      "learning_rate": 0.000287027027027027,
      "loss": 2.0526,
      "step": 210
    },
    {
      "epoch": 0.2813333333333333,
      "grad_norm": 2.063255786895752,
      "learning_rate": 0.0002869459459459459,
      "loss": 2.2281,
      "step": 211
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 1.4966678619384766,
      "learning_rate": 0.00028686486486486485,
      "loss": 2.0988,
      "step": 212
    },
    {
      "epoch": 0.284,
      "grad_norm": 1.4408961534500122,
      "learning_rate": 0.00028678378378378373,
      "loss": 1.9981,
      "step": 213
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 1.5306053161621094,
      "learning_rate": 0.0002867027027027027,
      "loss": 1.939,
      "step": 214
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 1.8261032104492188,
      "learning_rate": 0.0002866216216216216,
      "loss": 1.9806,
      "step": 215
    },
    {
      "epoch": 0.288,
      "grad_norm": 1.5201406478881836,
      "learning_rate": 0.0002865405405405405,
      "loss": 1.9884,
      "step": 216
    },
    {
      "epoch": 0.28933333333333333,
      "grad_norm": 1.393939733505249,
      "learning_rate": 0.00028645945945945944,
      "loss": 2.3862,
      "step": 217
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 1.4634751081466675,
      "learning_rate": 0.0002863783783783784,
      "loss": 1.8169,
      "step": 218
    },
    {
      "epoch": 0.292,
      "grad_norm": 1.555287480354309,
      "learning_rate": 0.00028629729729729727,
      "loss": 2.2698,
      "step": 219
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 1.5568962097167969,
      "learning_rate": 0.0002862162162162162,
      "loss": 2.1631,
      "step": 220
    },
    {
      "epoch": 0.2946666666666667,
      "grad_norm": 1.3402125835418701,
      "learning_rate": 0.0002861351351351351,
      "loss": 2.1451,
      "step": 221
    },
    {
      "epoch": 0.296,
      "grad_norm": 1.3450273275375366,
      "learning_rate": 0.00028605405405405403,
      "loss": 2.0177,
      "step": 222
    },
    {
      "epoch": 0.29733333333333334,
      "grad_norm": 1.499646782875061,
      "learning_rate": 0.000285972972972973,
      "loss": 1.9615,
      "step": 223
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 1.5635466575622559,
      "learning_rate": 0.00028589189189189186,
      "loss": 2.1948,
      "step": 224
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.4999442100524902,
      "learning_rate": 0.0002858108108108108,
      "loss": 2.0591,
      "step": 225
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 1.384885549545288,
      "learning_rate": 0.00028572972972972974,
      "loss": 2.0195,
      "step": 226
    },
    {
      "epoch": 0.30266666666666664,
      "grad_norm": 1.483678936958313,
      "learning_rate": 0.00028564864864864863,
      "loss": 1.9385,
      "step": 227
    },
    {
      "epoch": 0.304,
      "grad_norm": 1.3736529350280762,
      "learning_rate": 0.00028556756756756757,
      "loss": 2.0237,
      "step": 228
    },
    {
      "epoch": 0.30533333333333335,
      "grad_norm": 1.4552125930786133,
      "learning_rate": 0.00028548648648648645,
      "loss": 1.9748,
      "step": 229
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 1.3877514600753784,
      "learning_rate": 0.0002854054054054054,
      "loss": 1.7658,
      "step": 230
    },
    {
      "epoch": 0.308,
      "grad_norm": 1.9227640628814697,
      "learning_rate": 0.0002853243243243243,
      "loss": 1.8956,
      "step": 231
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 1.7672358751296997,
      "learning_rate": 0.0002852432432432432,
      "loss": 1.8637,
      "step": 232
    },
    {
      "epoch": 0.31066666666666665,
      "grad_norm": 1.3619962930679321,
      "learning_rate": 0.0002851621621621621,
      "loss": 1.8055,
      "step": 233
    },
    {
      "epoch": 0.312,
      "grad_norm": 1.5230653285980225,
      "learning_rate": 0.00028508108108108105,
      "loss": 2.1611,
      "step": 234
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 1.441070795059204,
      "learning_rate": 0.000285,
      "loss": 2.2335,
      "step": 235
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 1.5286118984222412,
      "learning_rate": 0.0002849189189189189,
      "loss": 1.8508,
      "step": 236
    },
    {
      "epoch": 0.316,
      "grad_norm": 1.5704731941223145,
      "learning_rate": 0.0002848378378378378,
      "loss": 2.0439,
      "step": 237
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 1.6510275602340698,
      "learning_rate": 0.0002847567567567567,
      "loss": 2.1648,
      "step": 238
    },
    {
      "epoch": 0.31866666666666665,
      "grad_norm": 1.5593777894973755,
      "learning_rate": 0.00028467567567567564,
      "loss": 2.4261,
      "step": 239
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.4675681591033936,
      "learning_rate": 0.0002845945945945946,
      "loss": 2.2166,
      "step": 240
    },
    {
      "epoch": 0.32133333333333336,
      "grad_norm": 1.4622960090637207,
      "learning_rate": 0.00028451351351351347,
      "loss": 2.2002,
      "step": 241
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 1.2822798490524292,
      "learning_rate": 0.0002844324324324324,
      "loss": 1.8144,
      "step": 242
    },
    {
      "epoch": 0.324,
      "grad_norm": 1.3303253650665283,
      "learning_rate": 0.0002843513513513513,
      "loss": 1.7978,
      "step": 243
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 1.9334195852279663,
      "learning_rate": 0.00028427027027027024,
      "loss": 2.0035,
      "step": 244
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 1.417314052581787,
      "learning_rate": 0.0002841891891891892,
      "loss": 1.897,
      "step": 245
    },
    {
      "epoch": 0.328,
      "grad_norm": 1.482377529144287,
      "learning_rate": 0.00028410810810810806,
      "loss": 1.9222,
      "step": 246
    },
    {
      "epoch": 0.3293333333333333,
      "grad_norm": 1.5992385149002075,
      "learning_rate": 0.000284027027027027,
      "loss": 2.5431,
      "step": 247
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 2.6943717002868652,
      "learning_rate": 0.00028394594594594594,
      "loss": 2.0553,
      "step": 248
    },
    {
      "epoch": 0.332,
      "grad_norm": 1.3544955253601074,
      "learning_rate": 0.00028386486486486483,
      "loss": 2.1604,
      "step": 249
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.677613615989685,
      "learning_rate": 0.00028378378378378377,
      "loss": 1.9701,
      "step": 250
    },
    {
      "epoch": 0.33466666666666667,
      "grad_norm": 1.606157898902893,
      "learning_rate": 0.00028370270270270266,
      "loss": 1.9989,
      "step": 251
    },
    {
      "epoch": 0.336,
      "grad_norm": 1.4805529117584229,
      "learning_rate": 0.0002836216216216216,
      "loss": 1.7091,
      "step": 252
    },
    {
      "epoch": 0.3373333333333333,
      "grad_norm": 1.7165392637252808,
      "learning_rate": 0.00028354054054054054,
      "loss": 2.2327,
      "step": 253
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 1.4689245223999023,
      "learning_rate": 0.0002834594594594594,
      "loss": 2.2143,
      "step": 254
    },
    {
      "epoch": 0.34,
      "grad_norm": 1.921590805053711,
      "learning_rate": 0.00028337837837837836,
      "loss": 1.8919,
      "step": 255
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 1.5291597843170166,
      "learning_rate": 0.0002832972972972973,
      "loss": 2.116,
      "step": 256
    },
    {
      "epoch": 0.3426666666666667,
      "grad_norm": 1.5842727422714233,
      "learning_rate": 0.0002832162162162162,
      "loss": 2.1501,
      "step": 257
    },
    {
      "epoch": 0.344,
      "grad_norm": 1.8148531913757324,
      "learning_rate": 0.00028313513513513513,
      "loss": 2.0151,
      "step": 258
    },
    {
      "epoch": 0.3453333333333333,
      "grad_norm": 1.6662325859069824,
      "learning_rate": 0.000283054054054054,
      "loss": 1.977,
      "step": 259
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 1.5072444677352905,
      "learning_rate": 0.00028297297297297296,
      "loss": 1.8366,
      "step": 260
    },
    {
      "epoch": 0.348,
      "grad_norm": 1.4957356452941895,
      "learning_rate": 0.0002828918918918919,
      "loss": 1.7202,
      "step": 261
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 1.4025870561599731,
      "learning_rate": 0.0002828108108108108,
      "loss": 2.2101,
      "step": 262
    },
    {
      "epoch": 0.3506666666666667,
      "grad_norm": 1.3639280796051025,
      "learning_rate": 0.0002827297297297297,
      "loss": 1.8131,
      "step": 263
    },
    {
      "epoch": 0.352,
      "grad_norm": 1.3960955142974854,
      "learning_rate": 0.0002826486486486486,
      "loss": 1.897,
      "step": 264
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 1.4181442260742188,
      "learning_rate": 0.00028256756756756755,
      "loss": 1.9002,
      "step": 265
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 1.5828187465667725,
      "learning_rate": 0.0002824864864864865,
      "loss": 2.1105,
      "step": 266
    },
    {
      "epoch": 0.356,
      "grad_norm": 1.5522948503494263,
      "learning_rate": 0.0002824054054054054,
      "loss": 1.9799,
      "step": 267
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 1.8922085762023926,
      "learning_rate": 0.0002823243243243243,
      "loss": 1.4809,
      "step": 268
    },
    {
      "epoch": 0.3586666666666667,
      "grad_norm": 1.559508204460144,
      "learning_rate": 0.00028224324324324326,
      "loss": 1.7712,
      "step": 269
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.7854725122451782,
      "learning_rate": 0.00028216216216216214,
      "loss": 2.2486,
      "step": 270
    },
    {
      "epoch": 0.36133333333333334,
      "grad_norm": 1.4735171794891357,
      "learning_rate": 0.0002820810810810811,
      "loss": 1.8683,
      "step": 271
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 1.3731591701507568,
      "learning_rate": 0.00028199999999999997,
      "loss": 2.0278,
      "step": 272
    },
    {
      "epoch": 0.364,
      "grad_norm": 1.4395371675491333,
      "learning_rate": 0.00028191891891891886,
      "loss": 1.9665,
      "step": 273
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 1.467203974723816,
      "learning_rate": 0.0002818378378378378,
      "loss": 1.9575,
      "step": 274
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 1.635581135749817,
      "learning_rate": 0.00028175675675675674,
      "loss": 1.9389,
      "step": 275
    },
    {
      "epoch": 0.368,
      "grad_norm": 1.4615427255630493,
      "learning_rate": 0.0002816756756756756,
      "loss": 2.1816,
      "step": 276
    },
    {
      "epoch": 0.36933333333333335,
      "grad_norm": 1.656909465789795,
      "learning_rate": 0.00028159459459459456,
      "loss": 2.3567,
      "step": 277
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 1.7373586893081665,
      "learning_rate": 0.0002815135135135135,
      "loss": 1.4727,
      "step": 278
    },
    {
      "epoch": 0.372,
      "grad_norm": 1.7426153421401978,
      "learning_rate": 0.0002814324324324324,
      "loss": 2.2227,
      "step": 279
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 1.3897476196289062,
      "learning_rate": 0.00028135135135135133,
      "loss": 1.8675,
      "step": 280
    },
    {
      "epoch": 0.37466666666666665,
      "grad_norm": 1.709834098815918,
      "learning_rate": 0.0002812702702702702,
      "loss": 2.3764,
      "step": 281
    },
    {
      "epoch": 0.376,
      "grad_norm": 1.5125759840011597,
      "learning_rate": 0.00028118918918918916,
      "loss": 2.0852,
      "step": 282
    },
    {
      "epoch": 0.37733333333333335,
      "grad_norm": 1.7348320484161377,
      "learning_rate": 0.0002811081081081081,
      "loss": 1.9202,
      "step": 283
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 1.5631229877471924,
      "learning_rate": 0.000281027027027027,
      "loss": 1.4906,
      "step": 284
    },
    {
      "epoch": 0.38,
      "grad_norm": 1.6816973686218262,
      "learning_rate": 0.0002809459459459459,
      "loss": 2.2054,
      "step": 285
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 1.4971787929534912,
      "learning_rate": 0.0002808648648648648,
      "loss": 1.685,
      "step": 286
    },
    {
      "epoch": 0.38266666666666665,
      "grad_norm": 1.7609213590621948,
      "learning_rate": 0.00028078378378378375,
      "loss": 2.1061,
      "step": 287
    },
    {
      "epoch": 0.384,
      "grad_norm": 1.5097217559814453,
      "learning_rate": 0.0002807027027027027,
      "loss": 1.653,
      "step": 288
    },
    {
      "epoch": 0.38533333333333336,
      "grad_norm": 1.5634942054748535,
      "learning_rate": 0.0002806216216216216,
      "loss": 2.1993,
      "step": 289
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 2.1091437339782715,
      "learning_rate": 0.0002805405405405405,
      "loss": 1.9348,
      "step": 290
    },
    {
      "epoch": 0.388,
      "grad_norm": 1.5800089836120605,
      "learning_rate": 0.00028045945945945946,
      "loss": 2.1307,
      "step": 291
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 1.5849138498306274,
      "learning_rate": 0.00028037837837837834,
      "loss": 1.9966,
      "step": 292
    },
    {
      "epoch": 0.39066666666666666,
      "grad_norm": 1.4796254634857178,
      "learning_rate": 0.0002802972972972973,
      "loss": 2.1928,
      "step": 293
    },
    {
      "epoch": 0.392,
      "grad_norm": 1.6186397075653076,
      "learning_rate": 0.00028021621621621617,
      "loss": 2.0749,
      "step": 294
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 1.4966411590576172,
      "learning_rate": 0.0002801351351351351,
      "loss": 1.8323,
      "step": 295
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 1.4080712795257568,
      "learning_rate": 0.00028005405405405405,
      "loss": 1.9737,
      "step": 296
    },
    {
      "epoch": 0.396,
      "grad_norm": 1.6747318506240845,
      "learning_rate": 0.00027997297297297294,
      "loss": 1.9782,
      "step": 297
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 1.3555628061294556,
      "learning_rate": 0.0002798918918918919,
      "loss": 2.0132,
      "step": 298
    },
    {
      "epoch": 0.39866666666666667,
      "grad_norm": 1.5095257759094238,
      "learning_rate": 0.0002798108108108108,
      "loss": 2.028,
      "step": 299
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.5272868871688843,
      "learning_rate": 0.0002797297297297297,
      "loss": 1.7715,
      "step": 300
    },
    {
      "epoch": 0.4013333333333333,
      "grad_norm": 1.4900413751602173,
      "learning_rate": 0.00027964864864864864,
      "loss": 1.7662,
      "step": 301
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 1.478946566581726,
      "learning_rate": 0.00027956756756756753,
      "loss": 1.9725,
      "step": 302
    },
    {
      "epoch": 0.404,
      "grad_norm": 1.5692839622497559,
      "learning_rate": 0.00027948648648648647,
      "loss": 1.9786,
      "step": 303
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 1.5601072311401367,
      "learning_rate": 0.0002794054054054054,
      "loss": 2.0926,
      "step": 304
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 1.7368569374084473,
      "learning_rate": 0.0002793243243243243,
      "loss": 1.7586,
      "step": 305
    },
    {
      "epoch": 0.408,
      "grad_norm": 1.4387532472610474,
      "learning_rate": 0.00027924324324324324,
      "loss": 1.7439,
      "step": 306
    },
    {
      "epoch": 0.4093333333333333,
      "grad_norm": 1.629684329032898,
      "learning_rate": 0.0002791621621621621,
      "loss": 2.2936,
      "step": 307
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 1.3872740268707275,
      "learning_rate": 0.00027908108108108106,
      "loss": 1.8787,
      "step": 308
    },
    {
      "epoch": 0.412,
      "grad_norm": 1.7116374969482422,
      "learning_rate": 0.000279,
      "loss": 1.7716,
      "step": 309
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 1.5510958433151245,
      "learning_rate": 0.0002789189189189189,
      "loss": 1.8938,
      "step": 310
    },
    {
      "epoch": 0.4146666666666667,
      "grad_norm": 1.6433966159820557,
      "learning_rate": 0.00027883783783783783,
      "loss": 2.0468,
      "step": 311
    },
    {
      "epoch": 0.416,
      "grad_norm": 1.6440802812576294,
      "learning_rate": 0.0002787567567567567,
      "loss": 2.2423,
      "step": 312
    },
    {
      "epoch": 0.41733333333333333,
      "grad_norm": 2.017868757247925,
      "learning_rate": 0.00027867567567567566,
      "loss": 1.8513,
      "step": 313
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 1.8002614974975586,
      "learning_rate": 0.00027859459459459454,
      "loss": 1.6811,
      "step": 314
    },
    {
      "epoch": 0.42,
      "grad_norm": 1.5148757696151733,
      "learning_rate": 0.0002785135135135135,
      "loss": 1.8697,
      "step": 315
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 1.5383806228637695,
      "learning_rate": 0.00027843243243243237,
      "loss": 1.8957,
      "step": 316
    },
    {
      "epoch": 0.4226666666666667,
      "grad_norm": 1.7148637771606445,
      "learning_rate": 0.0002783513513513513,
      "loss": 1.9668,
      "step": 317
    },
    {
      "epoch": 0.424,
      "grad_norm": 1.591740369796753,
      "learning_rate": 0.00027827027027027025,
      "loss": 2.4039,
      "step": 318
    },
    {
      "epoch": 0.42533333333333334,
      "grad_norm": 1.3869433403015137,
      "learning_rate": 0.00027818918918918914,
      "loss": 2.0082,
      "step": 319
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 1.4315314292907715,
      "learning_rate": 0.0002781081081081081,
      "loss": 1.9694,
      "step": 320
    },
    {
      "epoch": 0.428,
      "grad_norm": 1.2760343551635742,
      "learning_rate": 0.000278027027027027,
      "loss": 1.6783,
      "step": 321
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 1.3991619348526,
      "learning_rate": 0.0002779459459459459,
      "loss": 2.0315,
      "step": 322
    },
    {
      "epoch": 0.43066666666666664,
      "grad_norm": 1.4684548377990723,
      "learning_rate": 0.00027786486486486485,
      "loss": 1.8848,
      "step": 323
    },
    {
      "epoch": 0.432,
      "grad_norm": 1.4747231006622314,
      "learning_rate": 0.00027778378378378373,
      "loss": 1.9212,
      "step": 324
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 1.3975818157196045,
      "learning_rate": 0.00027770270270270267,
      "loss": 1.7511,
      "step": 325
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 1.5842907428741455,
      "learning_rate": 0.0002776216216216216,
      "loss": 1.8788,
      "step": 326
    },
    {
      "epoch": 0.436,
      "grad_norm": 1.503185510635376,
      "learning_rate": 0.0002775405405405405,
      "loss": 1.6584,
      "step": 327
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 1.5750998258590698,
      "learning_rate": 0.00027745945945945944,
      "loss": 2.1111,
      "step": 328
    },
    {
      "epoch": 0.43866666666666665,
      "grad_norm": 1.5285451412200928,
      "learning_rate": 0.0002773783783783783,
      "loss": 1.8405,
      "step": 329
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.608475923538208,
      "learning_rate": 0.00027729729729729727,
      "loss": 1.9844,
      "step": 330
    },
    {
      "epoch": 0.44133333333333336,
      "grad_norm": 1.4850702285766602,
      "learning_rate": 0.0002772162162162162,
      "loss": 1.7258,
      "step": 331
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 1.7564722299575806,
      "learning_rate": 0.0002771351351351351,
      "loss": 1.9379,
      "step": 332
    },
    {
      "epoch": 0.444,
      "grad_norm": 1.855940580368042,
      "learning_rate": 0.00027705405405405403,
      "loss": 1.7064,
      "step": 333
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 1.484481692314148,
      "learning_rate": 0.000276972972972973,
      "loss": 2.0846,
      "step": 334
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 1.3509804010391235,
      "learning_rate": 0.00027689189189189186,
      "loss": 1.6212,
      "step": 335
    },
    {
      "epoch": 0.448,
      "grad_norm": 1.4149447679519653,
      "learning_rate": 0.0002768108108108108,
      "loss": 1.821,
      "step": 336
    },
    {
      "epoch": 0.4493333333333333,
      "grad_norm": 1.3730971813201904,
      "learning_rate": 0.0002767297297297297,
      "loss": 1.5814,
      "step": 337
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 1.4462043046951294,
      "learning_rate": 0.0002766486486486486,
      "loss": 1.7618,
      "step": 338
    },
    {
      "epoch": 0.452,
      "grad_norm": 1.314758062362671,
      "learning_rate": 0.00027656756756756757,
      "loss": 1.6383,
      "step": 339
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 1.2717039585113525,
      "learning_rate": 0.00027648648648648645,
      "loss": 2.151,
      "step": 340
    },
    {
      "epoch": 0.45466666666666666,
      "grad_norm": 1.6373931169509888,
      "learning_rate": 0.0002764054054054054,
      "loss": 1.9099,
      "step": 341
    },
    {
      "epoch": 0.456,
      "grad_norm": 1.4642207622528076,
      "learning_rate": 0.00027632432432432433,
      "loss": 2.049,
      "step": 342
    },
    {
      "epoch": 0.4573333333333333,
      "grad_norm": 1.448569416999817,
      "learning_rate": 0.0002762432432432432,
      "loss": 2.1628,
      "step": 343
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 1.675606608390808,
      "learning_rate": 0.00027616216216216216,
      "loss": 1.8508,
      "step": 344
    },
    {
      "epoch": 0.46,
      "grad_norm": 1.5630934238433838,
      "learning_rate": 0.00027608108108108105,
      "loss": 1.8594,
      "step": 345
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 1.54082190990448,
      "learning_rate": 0.000276,
      "loss": 1.6598,
      "step": 346
    },
    {
      "epoch": 0.46266666666666667,
      "grad_norm": 1.5847666263580322,
      "learning_rate": 0.0002759189189189189,
      "loss": 1.7647,
      "step": 347
    },
    {
      "epoch": 0.464,
      "grad_norm": 2.252837896347046,
      "learning_rate": 0.0002758378378378378,
      "loss": 2.0447,
      "step": 348
    },
    {
      "epoch": 0.4653333333333333,
      "grad_norm": 1.7738744020462036,
      "learning_rate": 0.00027575675675675675,
      "loss": 1.7703,
      "step": 349
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 1.4995344877243042,
      "learning_rate": 0.00027567567567567564,
      "loss": 1.6419,
      "step": 350
    },
    {
      "epoch": 0.468,
      "grad_norm": 2.088444471359253,
      "learning_rate": 0.0002755945945945946,
      "loss": 1.7707,
      "step": 351
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 1.5837825536727905,
      "learning_rate": 0.00027551351351351347,
      "loss": 1.9583,
      "step": 352
    },
    {
      "epoch": 0.4706666666666667,
      "grad_norm": 1.4897202253341675,
      "learning_rate": 0.0002754324324324324,
      "loss": 2.1723,
      "step": 353
    },
    {
      "epoch": 0.472,
      "grad_norm": 1.6157208681106567,
      "learning_rate": 0.0002753513513513513,
      "loss": 1.9101,
      "step": 354
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 1.5389573574066162,
      "learning_rate": 0.00027527027027027023,
      "loss": 1.5783,
      "step": 355
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 1.9239134788513184,
      "learning_rate": 0.0002751891891891892,
      "loss": 1.777,
      "step": 356
    },
    {
      "epoch": 0.476,
      "grad_norm": 1.561647891998291,
      "learning_rate": 0.00027510810810810806,
      "loss": 2.1997,
      "step": 357
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 1.3427608013153076,
      "learning_rate": 0.000275027027027027,
      "loss": 1.9833,
      "step": 358
    },
    {
      "epoch": 0.4786666666666667,
      "grad_norm": 1.5586169958114624,
      "learning_rate": 0.0002749459459459459,
      "loss": 1.9579,
      "step": 359
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.6433379650115967,
      "learning_rate": 0.0002748648648648648,
      "loss": 2.1288,
      "step": 360
    },
    {
      "epoch": 0.48133333333333334,
      "grad_norm": 1.488326907157898,
      "learning_rate": 0.00027478378378378377,
      "loss": 1.7491,
      "step": 361
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 2.3020966053009033,
      "learning_rate": 0.00027470270270270265,
      "loss": 1.7871,
      "step": 362
    },
    {
      "epoch": 0.484,
      "grad_norm": 1.4963040351867676,
      "learning_rate": 0.0002746216216216216,
      "loss": 1.7932,
      "step": 363
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 1.3775100708007812,
      "learning_rate": 0.00027454054054054053,
      "loss": 1.6512,
      "step": 364
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 1.4809203147888184,
      "learning_rate": 0.0002744594594594594,
      "loss": 1.8159,
      "step": 365
    },
    {
      "epoch": 0.488,
      "grad_norm": 1.7213751077651978,
      "learning_rate": 0.00027437837837837836,
      "loss": 1.9699,
      "step": 366
    },
    {
      "epoch": 0.48933333333333334,
      "grad_norm": 1.4962499141693115,
      "learning_rate": 0.00027429729729729725,
      "loss": 1.9571,
      "step": 367
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 1.6025334596633911,
      "learning_rate": 0.0002742162162162162,
      "loss": 1.7701,
      "step": 368
    },
    {
      "epoch": 0.492,
      "grad_norm": 1.3400206565856934,
      "learning_rate": 0.00027413513513513513,
      "loss": 2.0831,
      "step": 369
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 1.4907212257385254,
      "learning_rate": 0.000274054054054054,
      "loss": 2.2128,
      "step": 370
    },
    {
      "epoch": 0.49466666666666664,
      "grad_norm": 1.5758998394012451,
      "learning_rate": 0.00027397297297297295,
      "loss": 1.7,
      "step": 371
    },
    {
      "epoch": 0.496,
      "grad_norm": 1.546570897102356,
      "learning_rate": 0.0002738918918918919,
      "loss": 2.1435,
      "step": 372
    },
    {
      "epoch": 0.49733333333333335,
      "grad_norm": 1.3214598894119263,
      "learning_rate": 0.0002738108108108108,
      "loss": 1.8569,
      "step": 373
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 1.3149590492248535,
      "learning_rate": 0.0002737297297297297,
      "loss": 1.9221,
      "step": 374
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.342584490776062,
      "learning_rate": 0.0002736486486486486,
      "loss": 1.7816,
      "step": 375
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 1.4887545108795166,
      "learning_rate": 0.00027356756756756755,
      "loss": 2.1133,
      "step": 376
    },
    {
      "epoch": 0.5026666666666667,
      "grad_norm": 1.3835586309432983,
      "learning_rate": 0.0002734864864864865,
      "loss": 1.8216,
      "step": 377
    },
    {
      "epoch": 0.504,
      "grad_norm": 1.4690678119659424,
      "learning_rate": 0.0002734054054054054,
      "loss": 1.3367,
      "step": 378
    },
    {
      "epoch": 0.5053333333333333,
      "grad_norm": 1.2613881826400757,
      "learning_rate": 0.0002733243243243243,
      "loss": 1.7806,
      "step": 379
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 1.3714239597320557,
      "learning_rate": 0.0002732432432432432,
      "loss": 2.2605,
      "step": 380
    },
    {
      "epoch": 0.508,
      "grad_norm": 1.3467215299606323,
      "learning_rate": 0.00027316216216216214,
      "loss": 1.6024,
      "step": 381
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 1.4649760723114014,
      "learning_rate": 0.0002730810810810811,
      "loss": 2.2123,
      "step": 382
    },
    {
      "epoch": 0.5106666666666667,
      "grad_norm": 1.441234827041626,
      "learning_rate": 0.00027299999999999997,
      "loss": 2.0638,
      "step": 383
    },
    {
      "epoch": 0.512,
      "grad_norm": 1.4048378467559814,
      "learning_rate": 0.0002729189189189189,
      "loss": 1.7315,
      "step": 384
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 1.4051415920257568,
      "learning_rate": 0.00027283783783783785,
      "loss": 1.9432,
      "step": 385
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 1.4907106161117554,
      "learning_rate": 0.00027275675675675673,
      "loss": 1.9714,
      "step": 386
    },
    {
      "epoch": 0.516,
      "grad_norm": 1.469191312789917,
      "learning_rate": 0.0002726756756756757,
      "loss": 1.8541,
      "step": 387
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 1.5486761331558228,
      "learning_rate": 0.00027259459459459456,
      "loss": 1.7008,
      "step": 388
    },
    {
      "epoch": 0.5186666666666667,
      "grad_norm": 2.1276614665985107,
      "learning_rate": 0.0002725135135135135,
      "loss": 2.0226,
      "step": 389
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.4798322916030884,
      "learning_rate": 0.00027243243243243244,
      "loss": 1.6628,
      "step": 390
    },
    {
      "epoch": 0.5213333333333333,
      "grad_norm": 1.487714409828186,
      "learning_rate": 0.00027235135135135133,
      "loss": 1.916,
      "step": 391
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 1.4506350755691528,
      "learning_rate": 0.0002722702702702702,
      "loss": 1.9987,
      "step": 392
    },
    {
      "epoch": 0.524,
      "grad_norm": 1.570002555847168,
      "learning_rate": 0.00027218918918918916,
      "loss": 1.9957,
      "step": 393
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 1.516372799873352,
      "learning_rate": 0.0002721081081081081,
      "loss": 1.9758,
      "step": 394
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 1.5441575050354004,
      "learning_rate": 0.000272027027027027,
      "loss": 2.0508,
      "step": 395
    },
    {
      "epoch": 0.528,
      "grad_norm": 1.5559760332107544,
      "learning_rate": 0.0002719459459459459,
      "loss": 2.1393,
      "step": 396
    },
    {
      "epoch": 0.5293333333333333,
      "grad_norm": 1.5624394416809082,
      "learning_rate": 0.0002718648648648648,
      "loss": 2.1254,
      "step": 397
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 1.6203763484954834,
      "learning_rate": 0.00027178378378378375,
      "loss": 1.937,
      "step": 398
    },
    {
      "epoch": 0.532,
      "grad_norm": 1.46638023853302,
      "learning_rate": 0.0002717027027027027,
      "loss": 1.8332,
      "step": 399
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 1.5599241256713867,
      "learning_rate": 0.0002716216216216216,
      "loss": 1.7749,
      "step": 400
    },
    {
      "epoch": 0.5346666666666666,
      "grad_norm": 1.684438705444336,
      "learning_rate": 0.0002715405405405405,
      "loss": 2.4334,
      "step": 401
    },
    {
      "epoch": 0.536,
      "grad_norm": 1.4272172451019287,
      "learning_rate": 0.0002714594594594594,
      "loss": 1.9067,
      "step": 402
    },
    {
      "epoch": 0.5373333333333333,
      "grad_norm": 1.4622514247894287,
      "learning_rate": 0.00027137837837837834,
      "loss": 2.0651,
      "step": 403
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 1.3921480178833008,
      "learning_rate": 0.0002712972972972973,
      "loss": 1.7324,
      "step": 404
    },
    {
      "epoch": 0.54,
      "grad_norm": 1.4208029508590698,
      "learning_rate": 0.00027121621621621617,
      "loss": 2.0184,
      "step": 405
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 1.7692627906799316,
      "learning_rate": 0.0002711351351351351,
      "loss": 2.2866,
      "step": 406
    },
    {
      "epoch": 0.5426666666666666,
      "grad_norm": 1.3943793773651123,
      "learning_rate": 0.00027105405405405405,
      "loss": 1.9737,
      "step": 407
    },
    {
      "epoch": 0.544,
      "grad_norm": 1.4595991373062134,
      "learning_rate": 0.00027097297297297294,
      "loss": 1.7632,
      "step": 408
    },
    {
      "epoch": 0.5453333333333333,
      "grad_norm": 1.4506183862686157,
      "learning_rate": 0.0002708918918918919,
      "loss": 1.7427,
      "step": 409
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 1.561444640159607,
      "learning_rate": 0.00027081081081081076,
      "loss": 1.7587,
      "step": 410
    },
    {
      "epoch": 0.548,
      "grad_norm": 1.9788174629211426,
      "learning_rate": 0.0002707297297297297,
      "loss": 2.3358,
      "step": 411
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 1.638604998588562,
      "learning_rate": 0.00027064864864864864,
      "loss": 2.0519,
      "step": 412
    },
    {
      "epoch": 0.5506666666666666,
      "grad_norm": 1.7003378868103027,
      "learning_rate": 0.00027056756756756753,
      "loss": 1.3611,
      "step": 413
    },
    {
      "epoch": 0.552,
      "grad_norm": 1.940561294555664,
      "learning_rate": 0.00027048648648648647,
      "loss": 1.728,
      "step": 414
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 1.3629951477050781,
      "learning_rate": 0.0002704054054054054,
      "loss": 1.7779,
      "step": 415
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 1.565556287765503,
      "learning_rate": 0.0002703243243243243,
      "loss": 1.7595,
      "step": 416
    },
    {
      "epoch": 0.556,
      "grad_norm": 1.5780819654464722,
      "learning_rate": 0.00027024324324324324,
      "loss": 1.8249,
      "step": 417
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 1.7236148118972778,
      "learning_rate": 0.0002701621621621621,
      "loss": 2.2309,
      "step": 418
    },
    {
      "epoch": 0.5586666666666666,
      "grad_norm": 1.5496373176574707,
      "learning_rate": 0.00027008108108108106,
      "loss": 1.8125,
      "step": 419
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.5862340927124023,
      "learning_rate": 0.00027,
      "loss": 1.9768,
      "step": 420
    },
    {
      "epoch": 0.5613333333333334,
      "grad_norm": 1.3754340410232544,
      "learning_rate": 0.0002699189189189189,
      "loss": 1.9642,
      "step": 421
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 1.2664860486984253,
      "learning_rate": 0.00026983783783783783,
      "loss": 1.6126,
      "step": 422
    },
    {
      "epoch": 0.564,
      "grad_norm": 1.2554208040237427,
      "learning_rate": 0.0002697567567567567,
      "loss": 1.8549,
      "step": 423
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 1.292521595954895,
      "learning_rate": 0.00026967567567567566,
      "loss": 1.8041,
      "step": 424
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 1.507060170173645,
      "learning_rate": 0.0002695945945945946,
      "loss": 2.0127,
      "step": 425
    },
    {
      "epoch": 0.568,
      "grad_norm": 1.4416433572769165,
      "learning_rate": 0.0002695135135135135,
      "loss": 2.0105,
      "step": 426
    },
    {
      "epoch": 0.5693333333333334,
      "grad_norm": 1.433067798614502,
      "learning_rate": 0.0002694324324324324,
      "loss": 1.685,
      "step": 427
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 1.3581761121749878,
      "learning_rate": 0.00026935135135135136,
      "loss": 1.6817,
      "step": 428
    },
    {
      "epoch": 0.572,
      "grad_norm": 1.388298749923706,
      "learning_rate": 0.00026927027027027025,
      "loss": 2.0535,
      "step": 429
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 1.4463121891021729,
      "learning_rate": 0.0002691891891891892,
      "loss": 1.8064,
      "step": 430
    },
    {
      "epoch": 0.5746666666666667,
      "grad_norm": 1.6024425029754639,
      "learning_rate": 0.0002691081081081081,
      "loss": 1.7944,
      "step": 431
    },
    {
      "epoch": 0.576,
      "grad_norm": 1.4149154424667358,
      "learning_rate": 0.000269027027027027,
      "loss": 2.0173,
      "step": 432
    },
    {
      "epoch": 0.5773333333333334,
      "grad_norm": 1.7846745252609253,
      "learning_rate": 0.0002689459459459459,
      "loss": 2.2209,
      "step": 433
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 1.2946773767471313,
      "learning_rate": 0.00026886486486486484,
      "loss": 1.4878,
      "step": 434
    },
    {
      "epoch": 0.58,
      "grad_norm": 1.5553390979766846,
      "learning_rate": 0.00026878378378378373,
      "loss": 1.8609,
      "step": 435
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 1.3239185810089111,
      "learning_rate": 0.00026870270270270267,
      "loss": 1.3786,
      "step": 436
    },
    {
      "epoch": 0.5826666666666667,
      "grad_norm": 1.4408732652664185,
      "learning_rate": 0.0002686216216216216,
      "loss": 2.1202,
      "step": 437
    },
    {
      "epoch": 0.584,
      "grad_norm": 1.621235728263855,
      "learning_rate": 0.0002685405405405405,
      "loss": 2.0416,
      "step": 438
    },
    {
      "epoch": 0.5853333333333334,
      "grad_norm": 1.6910581588745117,
      "learning_rate": 0.00026845945945945944,
      "loss": 1.5874,
      "step": 439
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 1.2663311958312988,
      "learning_rate": 0.0002683783783783783,
      "loss": 1.822,
      "step": 440
    },
    {
      "epoch": 0.588,
      "grad_norm": 1.4114301204681396,
      "learning_rate": 0.00026829729729729726,
      "loss": 1.9132,
      "step": 441
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 1.958696722984314,
      "learning_rate": 0.0002682162162162162,
      "loss": 1.7829,
      "step": 442
    },
    {
      "epoch": 0.5906666666666667,
      "grad_norm": 1.500594973564148,
      "learning_rate": 0.0002681351351351351,
      "loss": 1.9828,
      "step": 443
    },
    {
      "epoch": 0.592,
      "grad_norm": 1.3366608619689941,
      "learning_rate": 0.00026805405405405403,
      "loss": 1.8052,
      "step": 444
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 1.4324333667755127,
      "learning_rate": 0.0002679729729729729,
      "loss": 2.1212,
      "step": 445
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 1.4995149374008179,
      "learning_rate": 0.00026789189189189186,
      "loss": 2.1665,
      "step": 446
    },
    {
      "epoch": 0.596,
      "grad_norm": 1.3894962072372437,
      "learning_rate": 0.0002678108108108108,
      "loss": 2.0805,
      "step": 447
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 1.493155598640442,
      "learning_rate": 0.0002677297297297297,
      "loss": 2.0973,
      "step": 448
    },
    {
      "epoch": 0.5986666666666667,
      "grad_norm": 1.6446647644042969,
      "learning_rate": 0.0002676486486486486,
      "loss": 2.2669,
      "step": 449
    },
    {
      "epoch": 0.6,
      "grad_norm": 2.5414698123931885,
      "learning_rate": 0.00026756756756756756,
      "loss": 1.3192,
      "step": 450
    },
    {
      "epoch": 0.6013333333333334,
      "grad_norm": 1.6105540990829468,
      "learning_rate": 0.00026748648648648645,
      "loss": 2.3099,
      "step": 451
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 1.4006487131118774,
      "learning_rate": 0.0002674054054054054,
      "loss": 1.7687,
      "step": 452
    },
    {
      "epoch": 0.604,
      "grad_norm": 1.3759300708770752,
      "learning_rate": 0.0002673243243243243,
      "loss": 1.8989,
      "step": 453
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 1.4360766410827637,
      "learning_rate": 0.0002672432432432432,
      "loss": 2.1031,
      "step": 454
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 1.5973998308181763,
      "learning_rate": 0.00026716216216216216,
      "loss": 1.8919,
      "step": 455
    },
    {
      "epoch": 0.608,
      "grad_norm": 1.834856629371643,
      "learning_rate": 0.00026708108108108104,
      "loss": 1.8491,
      "step": 456
    },
    {
      "epoch": 0.6093333333333333,
      "grad_norm": 1.6041094064712524,
      "learning_rate": 0.000267,
      "loss": 1.9199,
      "step": 457
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 1.452864170074463,
      "learning_rate": 0.0002669189189189189,
      "loss": 1.9079,
      "step": 458
    },
    {
      "epoch": 0.612,
      "grad_norm": 1.556098461151123,
      "learning_rate": 0.0002668378378378378,
      "loss": 1.8508,
      "step": 459
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 1.5177308320999146,
      "learning_rate": 0.00026675675675675675,
      "loss": 1.7504,
      "step": 460
    },
    {
      "epoch": 0.6146666666666667,
      "grad_norm": 1.574257731437683,
      "learning_rate": 0.00026667567567567564,
      "loss": 1.6399,
      "step": 461
    },
    {
      "epoch": 0.616,
      "grad_norm": 1.7014999389648438,
      "learning_rate": 0.0002665945945945946,
      "loss": 1.7208,
      "step": 462
    },
    {
      "epoch": 0.6173333333333333,
      "grad_norm": 2.4457147121429443,
      "learning_rate": 0.0002665135135135135,
      "loss": 1.9311,
      "step": 463
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 1.9412583112716675,
      "learning_rate": 0.0002664324324324324,
      "loss": 1.8567,
      "step": 464
    },
    {
      "epoch": 0.62,
      "grad_norm": 1.6017632484436035,
      "learning_rate": 0.00026635135135135135,
      "loss": 1.8411,
      "step": 465
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 1.9408318996429443,
      "learning_rate": 0.00026627027027027023,
      "loss": 1.886,
      "step": 466
    },
    {
      "epoch": 0.6226666666666667,
      "grad_norm": 1.4565376043319702,
      "learning_rate": 0.00026618918918918917,
      "loss": 1.8983,
      "step": 467
    },
    {
      "epoch": 0.624,
      "grad_norm": 1.5790841579437256,
      "learning_rate": 0.0002661081081081081,
      "loss": 2.1639,
      "step": 468
    },
    {
      "epoch": 0.6253333333333333,
      "grad_norm": 1.4469399452209473,
      "learning_rate": 0.000266027027027027,
      "loss": 2.1221,
      "step": 469
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 1.729941487312317,
      "learning_rate": 0.00026594594594594594,
      "loss": 2.1165,
      "step": 470
    },
    {
      "epoch": 0.628,
      "grad_norm": 1.3696783781051636,
      "learning_rate": 0.0002658648648648649,
      "loss": 1.8257,
      "step": 471
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 1.4486782550811768,
      "learning_rate": 0.00026578378378378377,
      "loss": 1.8362,
      "step": 472
    },
    {
      "epoch": 0.6306666666666667,
      "grad_norm": 1.57842218875885,
      "learning_rate": 0.00026570270270270265,
      "loss": 2.0228,
      "step": 473
    },
    {
      "epoch": 0.632,
      "grad_norm": 1.4587631225585938,
      "learning_rate": 0.0002656216216216216,
      "loss": 2.065,
      "step": 474
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 1.5920847654342651,
      "learning_rate": 0.0002655405405405405,
      "loss": 2.0437,
      "step": 475
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 1.4970301389694214,
      "learning_rate": 0.0002654594594594594,
      "loss": 2.1212,
      "step": 476
    },
    {
      "epoch": 0.636,
      "grad_norm": 1.4491462707519531,
      "learning_rate": 0.00026537837837837836,
      "loss": 1.7563,
      "step": 477
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 1.4413079023361206,
      "learning_rate": 0.00026529729729729725,
      "loss": 2.0442,
      "step": 478
    },
    {
      "epoch": 0.6386666666666667,
      "grad_norm": 1.5356889963150024,
      "learning_rate": 0.0002652162162162162,
      "loss": 2.1896,
      "step": 479
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.422276496887207,
      "learning_rate": 0.0002651351351351351,
      "loss": 1.98,
      "step": 480
    },
    {
      "epoch": 0.6413333333333333,
      "grad_norm": 1.4844071865081787,
      "learning_rate": 0.000265054054054054,
      "loss": 1.9573,
      "step": 481
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 1.4684998989105225,
      "learning_rate": 0.00026497297297297295,
      "loss": 1.8932,
      "step": 482
    },
    {
      "epoch": 0.644,
      "grad_norm": 1.4730446338653564,
      "learning_rate": 0.00026489189189189184,
      "loss": 1.97,
      "step": 483
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 1.669956088066101,
      "learning_rate": 0.0002648108108108108,
      "loss": 1.873,
      "step": 484
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 1.5352671146392822,
      "learning_rate": 0.0002647297297297297,
      "loss": 1.9843,
      "step": 485
    },
    {
      "epoch": 0.648,
      "grad_norm": 1.896766185760498,
      "learning_rate": 0.0002646486486486486,
      "loss": 1.9225,
      "step": 486
    },
    {
      "epoch": 0.6493333333333333,
      "grad_norm": 1.9163188934326172,
      "learning_rate": 0.00026456756756756755,
      "loss": 1.4285,
      "step": 487
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 1.3168753385543823,
      "learning_rate": 0.0002644864864864865,
      "loss": 1.9322,
      "step": 488
    },
    {
      "epoch": 0.652,
      "grad_norm": 1.5438501834869385,
      "learning_rate": 0.00026440540540540537,
      "loss": 1.7727,
      "step": 489
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 1.864567518234253,
      "learning_rate": 0.0002643243243243243,
      "loss": 2.2071,
      "step": 490
    },
    {
      "epoch": 0.6546666666666666,
      "grad_norm": 1.7138621807098389,
      "learning_rate": 0.0002642432432432432,
      "loss": 1.9052,
      "step": 491
    },
    {
      "epoch": 0.656,
      "grad_norm": 1.5057283639907837,
      "learning_rate": 0.00026416216216216214,
      "loss": 1.8348,
      "step": 492
    },
    {
      "epoch": 0.6573333333333333,
      "grad_norm": 1.5530368089675903,
      "learning_rate": 0.0002640810810810811,
      "loss": 1.7184,
      "step": 493
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 1.4693691730499268,
      "learning_rate": 0.00026399999999999997,
      "loss": 1.8816,
      "step": 494
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.462179183959961,
      "learning_rate": 0.0002639189189189189,
      "loss": 2.0015,
      "step": 495
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 1.371444821357727,
      "learning_rate": 0.0002638378378378378,
      "loss": 2.1607,
      "step": 496
    },
    {
      "epoch": 0.6626666666666666,
      "grad_norm": 1.3999768495559692,
      "learning_rate": 0.00026375675675675673,
      "loss": 2.2484,
      "step": 497
    },
    {
      "epoch": 0.664,
      "grad_norm": 1.5094735622406006,
      "learning_rate": 0.0002636756756756757,
      "loss": 2.2131,
      "step": 498
    },
    {
      "epoch": 0.6653333333333333,
      "grad_norm": 1.3627004623413086,
      "learning_rate": 0.00026359459459459456,
      "loss": 2.0651,
      "step": 499
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 1.3543212413787842,
      "learning_rate": 0.0002635135135135135,
      "loss": 1.899,
      "step": 500
    },
    {
      "epoch": 0.668,
      "grad_norm": 1.3750618696212769,
      "learning_rate": 0.00026343243243243244,
      "loss": 1.7151,
      "step": 501
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 1.3693351745605469,
      "learning_rate": 0.0002633513513513513,
      "loss": 2.1606,
      "step": 502
    },
    {
      "epoch": 0.6706666666666666,
      "grad_norm": 1.6234564781188965,
      "learning_rate": 0.00026327027027027027,
      "loss": 2.1601,
      "step": 503
    },
    {
      "epoch": 0.672,
      "grad_norm": 1.3611067533493042,
      "learning_rate": 0.00026318918918918915,
      "loss": 1.9674,
      "step": 504
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 1.3659663200378418,
      "learning_rate": 0.0002631081081081081,
      "loss": 1.9302,
      "step": 505
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 1.5951671600341797,
      "learning_rate": 0.00026302702702702703,
      "loss": 1.9277,
      "step": 506
    },
    {
      "epoch": 0.676,
      "grad_norm": 1.6908482313156128,
      "learning_rate": 0.0002629459459459459,
      "loss": 2.5243,
      "step": 507
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 1.4665589332580566,
      "learning_rate": 0.00026286486486486486,
      "loss": 1.7257,
      "step": 508
    },
    {
      "epoch": 0.6786666666666666,
      "grad_norm": 1.5183185338974,
      "learning_rate": 0.0002627837837837838,
      "loss": 2.1353,
      "step": 509
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.4292470216751099,
      "learning_rate": 0.0002627027027027027,
      "loss": 1.8735,
      "step": 510
    },
    {
      "epoch": 0.6813333333333333,
      "grad_norm": 1.4774690866470337,
      "learning_rate": 0.00026262162162162163,
      "loss": 2.0051,
      "step": 511
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 1.4446860551834106,
      "learning_rate": 0.0002625405405405405,
      "loss": 2.2274,
      "step": 512
    },
    {
      "epoch": 0.684,
      "grad_norm": 1.4849374294281006,
      "learning_rate": 0.0002624594594594594,
      "loss": 2.1121,
      "step": 513
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 1.4815813302993774,
      "learning_rate": 0.00026237837837837834,
      "loss": 1.5515,
      "step": 514
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 1.332651972770691,
      "learning_rate": 0.0002622972972972973,
      "loss": 1.7459,
      "step": 515
    },
    {
      "epoch": 0.688,
      "grad_norm": 1.665840744972229,
      "learning_rate": 0.00026221621621621617,
      "loss": 2.1309,
      "step": 516
    },
    {
      "epoch": 0.6893333333333334,
      "grad_norm": 1.5364928245544434,
      "learning_rate": 0.0002621351351351351,
      "loss": 1.8272,
      "step": 517
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 1.4467188119888306,
      "learning_rate": 0.000262054054054054,
      "loss": 1.6706,
      "step": 518
    },
    {
      "epoch": 0.692,
      "grad_norm": 1.4181104898452759,
      "learning_rate": 0.00026197297297297293,
      "loss": 2.091,
      "step": 519
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 1.4135560989379883,
      "learning_rate": 0.0002618918918918919,
      "loss": 1.9202,
      "step": 520
    },
    {
      "epoch": 0.6946666666666667,
      "grad_norm": 1.39498770236969,
      "learning_rate": 0.00026181081081081076,
      "loss": 1.7391,
      "step": 521
    },
    {
      "epoch": 0.696,
      "grad_norm": 1.3258957862854004,
      "learning_rate": 0.0002617297297297297,
      "loss": 1.9321,
      "step": 522
    },
    {
      "epoch": 0.6973333333333334,
      "grad_norm": 1.4636192321777344,
      "learning_rate": 0.00026164864864864864,
      "loss": 1.8187,
      "step": 523
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 1.479133129119873,
      "learning_rate": 0.00026156756756756753,
      "loss": 1.7318,
      "step": 524
    },
    {
      "epoch": 0.7,
      "grad_norm": 1.4740666151046753,
      "learning_rate": 0.00026148648648648647,
      "loss": 1.9344,
      "step": 525
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 1.4441187381744385,
      "learning_rate": 0.00026140540540540535,
      "loss": 1.8905,
      "step": 526
    },
    {
      "epoch": 0.7026666666666667,
      "grad_norm": 1.6996902227401733,
      "learning_rate": 0.0002613243243243243,
      "loss": 1.8729,
      "step": 527
    },
    {
      "epoch": 0.704,
      "grad_norm": 1.5091074705123901,
      "learning_rate": 0.00026124324324324323,
      "loss": 1.9922,
      "step": 528
    },
    {
      "epoch": 0.7053333333333334,
      "grad_norm": 1.3082175254821777,
      "learning_rate": 0.0002611621621621621,
      "loss": 1.694,
      "step": 529
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 1.4264967441558838,
      "learning_rate": 0.00026108108108108106,
      "loss": 2.0255,
      "step": 530
    },
    {
      "epoch": 0.708,
      "grad_norm": 1.577086329460144,
      "learning_rate": 0.000261,
      "loss": 2.045,
      "step": 531
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 1.5692559480667114,
      "learning_rate": 0.0002609189189189189,
      "loss": 2.105,
      "step": 532
    },
    {
      "epoch": 0.7106666666666667,
      "grad_norm": 1.470023512840271,
      "learning_rate": 0.00026083783783783783,
      "loss": 1.9821,
      "step": 533
    },
    {
      "epoch": 0.712,
      "grad_norm": 1.716705322265625,
      "learning_rate": 0.0002607567567567567,
      "loss": 1.7064,
      "step": 534
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 1.3910081386566162,
      "learning_rate": 0.00026067567567567565,
      "loss": 2.0662,
      "step": 535
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 1.483235239982605,
      "learning_rate": 0.0002605945945945946,
      "loss": 1.8233,
      "step": 536
    },
    {
      "epoch": 0.716,
      "grad_norm": 1.310320496559143,
      "learning_rate": 0.0002605135135135135,
      "loss": 1.9796,
      "step": 537
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 1.4731959104537964,
      "learning_rate": 0.0002604324324324324,
      "loss": 2.0719,
      "step": 538
    },
    {
      "epoch": 0.7186666666666667,
      "grad_norm": 1.5352309942245483,
      "learning_rate": 0.0002603513513513513,
      "loss": 2.2138,
      "step": 539
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.3150100708007812,
      "learning_rate": 0.00026027027027027025,
      "loss": 1.7871,
      "step": 540
    },
    {
      "epoch": 0.7213333333333334,
      "grad_norm": 1.3609758615493774,
      "learning_rate": 0.0002601891891891892,
      "loss": 2.006,
      "step": 541
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 1.4749512672424316,
      "learning_rate": 0.0002601081081081081,
      "loss": 1.9068,
      "step": 542
    },
    {
      "epoch": 0.724,
      "grad_norm": 1.4686260223388672,
      "learning_rate": 0.000260027027027027,
      "loss": 1.769,
      "step": 543
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 1.4442987442016602,
      "learning_rate": 0.00025994594594594596,
      "loss": 1.9809,
      "step": 544
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 1.689428687095642,
      "learning_rate": 0.00025986486486486484,
      "loss": 1.9221,
      "step": 545
    },
    {
      "epoch": 0.728,
      "grad_norm": 1.7846280336380005,
      "learning_rate": 0.0002597837837837838,
      "loss": 1.8064,
      "step": 546
    },
    {
      "epoch": 0.7293333333333333,
      "grad_norm": 1.4910627603530884,
      "learning_rate": 0.00025970270270270267,
      "loss": 2.3074,
      "step": 547
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 1.5727282762527466,
      "learning_rate": 0.0002596216216216216,
      "loss": 1.9961,
      "step": 548
    },
    {
      "epoch": 0.732,
      "grad_norm": 2.97065806388855,
      "learning_rate": 0.00025954054054054055,
      "loss": 2.0091,
      "step": 549
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 2.4655258655548096,
      "learning_rate": 0.00025945945945945944,
      "loss": 2.131,
      "step": 550
    },
    {
      "epoch": 0.7346666666666667,
      "grad_norm": 1.5715000629425049,
      "learning_rate": 0.0002593783783783784,
      "loss": 1.695,
      "step": 551
    },
    {
      "epoch": 0.736,
      "grad_norm": 1.499438762664795,
      "learning_rate": 0.0002592972972972973,
      "loss": 1.7462,
      "step": 552
    },
    {
      "epoch": 0.7373333333333333,
      "grad_norm": 1.4140304327011108,
      "learning_rate": 0.0002592162162162162,
      "loss": 1.8138,
      "step": 553
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 1.4414758682250977,
      "learning_rate": 0.0002591351351351351,
      "loss": 1.8007,
      "step": 554
    },
    {
      "epoch": 0.74,
      "grad_norm": 1.6161868572235107,
      "learning_rate": 0.00025905405405405403,
      "loss": 2.0717,
      "step": 555
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 1.5082701444625854,
      "learning_rate": 0.0002589729729729729,
      "loss": 1.9929,
      "step": 556
    },
    {
      "epoch": 0.7426666666666667,
      "grad_norm": 1.4765933752059937,
      "learning_rate": 0.00025889189189189186,
      "loss": 2.1699,
      "step": 557
    },
    {
      "epoch": 0.744,
      "grad_norm": 1.407335638999939,
      "learning_rate": 0.0002588108108108108,
      "loss": 1.6194,
      "step": 558
    },
    {
      "epoch": 0.7453333333333333,
      "grad_norm": 1.4941047430038452,
      "learning_rate": 0.0002587297297297297,
      "loss": 1.7889,
      "step": 559
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 1.4874567985534668,
      "learning_rate": 0.0002586486486486486,
      "loss": 2.1535,
      "step": 560
    },
    {
      "epoch": 0.748,
      "grad_norm": 1.580894112586975,
      "learning_rate": 0.0002585675675675675,
      "loss": 1.5001,
      "step": 561
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 1.504141926765442,
      "learning_rate": 0.00025848648648648645,
      "loss": 2.0694,
      "step": 562
    },
    {
      "epoch": 0.7506666666666667,
      "grad_norm": 1.4551918506622314,
      "learning_rate": 0.0002584054054054054,
      "loss": 1.7685,
      "step": 563
    },
    {
      "epoch": 0.752,
      "grad_norm": 1.3392902612686157,
      "learning_rate": 0.0002583243243243243,
      "loss": 1.9632,
      "step": 564
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 1.5082323551177979,
      "learning_rate": 0.0002582432432432432,
      "loss": 2.0536,
      "step": 565
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 1.535361409187317,
      "learning_rate": 0.00025816216216216216,
      "loss": 2.1322,
      "step": 566
    },
    {
      "epoch": 0.756,
      "grad_norm": 1.3812683820724487,
      "learning_rate": 0.00025808108108108104,
      "loss": 1.8746,
      "step": 567
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 1.308640718460083,
      "learning_rate": 0.000258,
      "loss": 1.856,
      "step": 568
    },
    {
      "epoch": 0.7586666666666667,
      "grad_norm": 1.2548383474349976,
      "learning_rate": 0.00025791891891891887,
      "loss": 1.8677,
      "step": 569
    },
    {
      "epoch": 0.76,
      "grad_norm": 1.6056057214736938,
      "learning_rate": 0.0002578378378378378,
      "loss": 1.8831,
      "step": 570
    },
    {
      "epoch": 0.7613333333333333,
      "grad_norm": 1.485212802886963,
      "learning_rate": 0.00025775675675675675,
      "loss": 1.9043,
      "step": 571
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 1.4169312715530396,
      "learning_rate": 0.00025767567567567564,
      "loss": 1.742,
      "step": 572
    },
    {
      "epoch": 0.764,
      "grad_norm": 1.4488145112991333,
      "learning_rate": 0.0002575945945945946,
      "loss": 1.8551,
      "step": 573
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 1.4587548971176147,
      "learning_rate": 0.0002575135135135135,
      "loss": 1.7106,
      "step": 574
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 1.3379148244857788,
      "learning_rate": 0.0002574324324324324,
      "loss": 1.9487,
      "step": 575
    },
    {
      "epoch": 0.768,
      "grad_norm": 1.3830792903900146,
      "learning_rate": 0.00025735135135135134,
      "loss": 2.0746,
      "step": 576
    },
    {
      "epoch": 0.7693333333333333,
      "grad_norm": 1.2958502769470215,
      "learning_rate": 0.00025727027027027023,
      "loss": 1.8612,
      "step": 577
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 1.400643229484558,
      "learning_rate": 0.00025718918918918917,
      "loss": 1.8019,
      "step": 578
    },
    {
      "epoch": 0.772,
      "grad_norm": 1.475960373878479,
      "learning_rate": 0.0002571081081081081,
      "loss": 1.7973,
      "step": 579
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 1.37800931930542,
      "learning_rate": 0.000257027027027027,
      "loss": 1.6433,
      "step": 580
    },
    {
      "epoch": 0.7746666666666666,
      "grad_norm": 1.3400055170059204,
      "learning_rate": 0.00025694594594594594,
      "loss": 1.9171,
      "step": 581
    },
    {
      "epoch": 0.776,
      "grad_norm": 1.2730672359466553,
      "learning_rate": 0.0002568648648648648,
      "loss": 1.6776,
      "step": 582
    },
    {
      "epoch": 0.7773333333333333,
      "grad_norm": 1.3238519430160522,
      "learning_rate": 0.00025678378378378376,
      "loss": 1.4927,
      "step": 583
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 1.7027751207351685,
      "learning_rate": 0.0002567027027027027,
      "loss": 1.9431,
      "step": 584
    },
    {
      "epoch": 0.78,
      "grad_norm": 1.512217402458191,
      "learning_rate": 0.0002566216216216216,
      "loss": 2.0619,
      "step": 585
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 1.4573091268539429,
      "learning_rate": 0.00025654054054054053,
      "loss": 2.0093,
      "step": 586
    },
    {
      "epoch": 0.7826666666666666,
      "grad_norm": 1.3731653690338135,
      "learning_rate": 0.00025645945945945947,
      "loss": 1.6399,
      "step": 587
    },
    {
      "epoch": 0.784,
      "grad_norm": 1.5468906164169312,
      "learning_rate": 0.00025637837837837836,
      "loss": 1.7303,
      "step": 588
    },
    {
      "epoch": 0.7853333333333333,
      "grad_norm": 1.443556547164917,
      "learning_rate": 0.0002562972972972973,
      "loss": 1.9744,
      "step": 589
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 1.5490301847457886,
      "learning_rate": 0.0002562162162162162,
      "loss": 2.063,
      "step": 590
    },
    {
      "epoch": 0.788,
      "grad_norm": 1.928316354751587,
      "learning_rate": 0.0002561351351351351,
      "loss": 1.9974,
      "step": 591
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 1.4981070756912231,
      "learning_rate": 0.00025605405405405406,
      "loss": 1.8623,
      "step": 592
    },
    {
      "epoch": 0.7906666666666666,
      "grad_norm": 1.5384989976882935,
      "learning_rate": 0.00025597297297297295,
      "loss": 2.0026,
      "step": 593
    },
    {
      "epoch": 0.792,
      "grad_norm": 1.4147616624832153,
      "learning_rate": 0.00025589189189189184,
      "loss": 1.7616,
      "step": 594
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 1.5685945749282837,
      "learning_rate": 0.0002558108108108108,
      "loss": 2.1988,
      "step": 595
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 1.452767252922058,
      "learning_rate": 0.0002557297297297297,
      "loss": 1.8587,
      "step": 596
    },
    {
      "epoch": 0.796,
      "grad_norm": 1.599075198173523,
      "learning_rate": 0.0002556486486486486,
      "loss": 2.2997,
      "step": 597
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 1.5366979837417603,
      "learning_rate": 0.00025556756756756754,
      "loss": 2.103,
      "step": 598
    },
    {
      "epoch": 0.7986666666666666,
      "grad_norm": 1.5638408660888672,
      "learning_rate": 0.00025548648648648643,
      "loss": 2.0704,
      "step": 599
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.45563805103302,
      "learning_rate": 0.00025540540540540537,
      "loss": 2.1604,
      "step": 600
    },
    {
      "epoch": 0.8013333333333333,
      "grad_norm": 1.7191038131713867,
      "learning_rate": 0.0002553243243243243,
      "loss": 1.9359,
      "step": 601
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 1.5444209575653076,
      "learning_rate": 0.0002552432432432432,
      "loss": 1.9868,
      "step": 602
    },
    {
      "epoch": 0.804,
      "grad_norm": 1.4275873899459839,
      "learning_rate": 0.00025516216216216214,
      "loss": 1.969,
      "step": 603
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 1.3194209337234497,
      "learning_rate": 0.0002550810810810811,
      "loss": 2.0769,
      "step": 604
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 1.406609296798706,
      "learning_rate": 0.00025499999999999996,
      "loss": 1.8665,
      "step": 605
    },
    {
      "epoch": 0.808,
      "grad_norm": 1.523098349571228,
      "learning_rate": 0.0002549189189189189,
      "loss": 1.9693,
      "step": 606
    },
    {
      "epoch": 0.8093333333333333,
      "grad_norm": 1.452111840248108,
      "learning_rate": 0.0002548378378378378,
      "loss": 2.0299,
      "step": 607
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 1.443839430809021,
      "learning_rate": 0.00025475675675675673,
      "loss": 2.0395,
      "step": 608
    },
    {
      "epoch": 0.812,
      "grad_norm": 1.6848998069763184,
      "learning_rate": 0.00025467567567567567,
      "loss": 1.8322,
      "step": 609
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 1.5887433290481567,
      "learning_rate": 0.00025459459459459456,
      "loss": 2.0758,
      "step": 610
    },
    {
      "epoch": 0.8146666666666667,
      "grad_norm": 1.468198299407959,
      "learning_rate": 0.0002545135135135135,
      "loss": 1.561,
      "step": 611
    },
    {
      "epoch": 0.816,
      "grad_norm": 1.384617567062378,
      "learning_rate": 0.0002544324324324324,
      "loss": 1.6545,
      "step": 612
    },
    {
      "epoch": 0.8173333333333334,
      "grad_norm": 1.4359573125839233,
      "learning_rate": 0.0002543513513513513,
      "loss": 1.8118,
      "step": 613
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 1.5578012466430664,
      "learning_rate": 0.00025427027027027026,
      "loss": 2.082,
      "step": 614
    },
    {
      "epoch": 0.82,
      "grad_norm": 1.5698055028915405,
      "learning_rate": 0.00025418918918918915,
      "loss": 1.858,
      "step": 615
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 1.3657810688018799,
      "learning_rate": 0.0002541081081081081,
      "loss": 2.0397,
      "step": 616
    },
    {
      "epoch": 0.8226666666666667,
      "grad_norm": 1.4687925577163696,
      "learning_rate": 0.00025402702702702703,
      "loss": 2.0221,
      "step": 617
    },
    {
      "epoch": 0.824,
      "grad_norm": 1.4392610788345337,
      "learning_rate": 0.0002539459459459459,
      "loss": 1.8321,
      "step": 618
    },
    {
      "epoch": 0.8253333333333334,
      "grad_norm": 1.3530150651931763,
      "learning_rate": 0.00025386486486486486,
      "loss": 1.7731,
      "step": 619
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 1.8038296699523926,
      "learning_rate": 0.00025378378378378374,
      "loss": 1.8898,
      "step": 620
    },
    {
      "epoch": 0.828,
      "grad_norm": 1.8508764505386353,
      "learning_rate": 0.0002537027027027027,
      "loss": 2.3983,
      "step": 621
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 1.4630762338638306,
      "learning_rate": 0.0002536216216216216,
      "loss": 1.7079,
      "step": 622
    },
    {
      "epoch": 0.8306666666666667,
      "grad_norm": 1.3661420345306396,
      "learning_rate": 0.0002535405405405405,
      "loss": 2.1553,
      "step": 623
    },
    {
      "epoch": 0.832,
      "grad_norm": 1.4513920545578003,
      "learning_rate": 0.00025345945945945945,
      "loss": 1.9038,
      "step": 624
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 1.4157249927520752,
      "learning_rate": 0.0002533783783783784,
      "loss": 1.4819,
      "step": 625
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 1.5415914058685303,
      "learning_rate": 0.0002532972972972973,
      "loss": 1.6101,
      "step": 626
    },
    {
      "epoch": 0.836,
      "grad_norm": 1.3550870418548584,
      "learning_rate": 0.0002532162162162162,
      "loss": 1.9801,
      "step": 627
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 1.3836344480514526,
      "learning_rate": 0.0002531351351351351,
      "loss": 2.0071,
      "step": 628
    },
    {
      "epoch": 0.8386666666666667,
      "grad_norm": 1.2948020696640015,
      "learning_rate": 0.00025305405405405405,
      "loss": 1.7099,
      "step": 629
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.6412140130996704,
      "learning_rate": 0.000252972972972973,
      "loss": 2.1339,
      "step": 630
    },
    {
      "epoch": 0.8413333333333334,
      "grad_norm": 1.6629612445831299,
      "learning_rate": 0.00025289189189189187,
      "loss": 2.0896,
      "step": 631
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 1.3765593767166138,
      "learning_rate": 0.0002528108108108108,
      "loss": 1.8455,
      "step": 632
    },
    {
      "epoch": 0.844,
      "grad_norm": 1.4573179483413696,
      "learning_rate": 0.0002527297297297297,
      "loss": 1.8162,
      "step": 633
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 1.6818135976791382,
      "learning_rate": 0.0002526486486486486,
      "loss": 1.5901,
      "step": 634
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 1.3593894243240356,
      "learning_rate": 0.0002525675675675675,
      "loss": 2.0105,
      "step": 635
    },
    {
      "epoch": 0.848,
      "grad_norm": 1.4701592922210693,
      "learning_rate": 0.00025248648648648647,
      "loss": 2.1748,
      "step": 636
    },
    {
      "epoch": 0.8493333333333334,
      "grad_norm": 1.7420960664749146,
      "learning_rate": 0.00025240540540540535,
      "loss": 1.8713,
      "step": 637
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 1.3800389766693115,
      "learning_rate": 0.0002523243243243243,
      "loss": 1.9105,
      "step": 638
    },
    {
      "epoch": 0.852,
      "grad_norm": 1.418957233428955,
      "learning_rate": 0.00025224324324324323,
      "loss": 2.2248,
      "step": 639
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 1.3076484203338623,
      "learning_rate": 0.0002521621621621621,
      "loss": 1.7658,
      "step": 640
    },
    {
      "epoch": 0.8546666666666667,
      "grad_norm": 1.4131100177764893,
      "learning_rate": 0.00025208108108108106,
      "loss": 2.1499,
      "step": 641
    },
    {
      "epoch": 0.856,
      "grad_norm": 1.3463832139968872,
      "learning_rate": 0.00025199999999999995,
      "loss": 1.6784,
      "step": 642
    },
    {
      "epoch": 0.8573333333333333,
      "grad_norm": 1.299514889717102,
      "learning_rate": 0.0002519189189189189,
      "loss": 1.8379,
      "step": 643
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 1.4446686506271362,
      "learning_rate": 0.0002518378378378378,
      "loss": 1.5962,
      "step": 644
    },
    {
      "epoch": 0.86,
      "grad_norm": 1.367240071296692,
      "learning_rate": 0.0002517567567567567,
      "loss": 1.7505,
      "step": 645
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 1.4965630769729614,
      "learning_rate": 0.00025167567567567565,
      "loss": 2.1266,
      "step": 646
    },
    {
      "epoch": 0.8626666666666667,
      "grad_norm": 1.4456290006637573,
      "learning_rate": 0.0002515945945945946,
      "loss": 1.7336,
      "step": 647
    },
    {
      "epoch": 0.864,
      "grad_norm": 1.4413105249404907,
      "learning_rate": 0.0002515135135135135,
      "loss": 1.8942,
      "step": 648
    },
    {
      "epoch": 0.8653333333333333,
      "grad_norm": 1.4728444814682007,
      "learning_rate": 0.0002514324324324324,
      "loss": 1.7999,
      "step": 649
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 1.54373300075531,
      "learning_rate": 0.0002513513513513513,
      "loss": 1.9132,
      "step": 650
    },
    {
      "epoch": 0.868,
      "grad_norm": 1.5126914978027344,
      "learning_rate": 0.00025127027027027025,
      "loss": 2.1502,
      "step": 651
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 1.468144416809082,
      "learning_rate": 0.0002511891891891892,
      "loss": 1.6982,
      "step": 652
    },
    {
      "epoch": 0.8706666666666667,
      "grad_norm": 1.4252080917358398,
      "learning_rate": 0.00025110810810810807,
      "loss": 1.9324,
      "step": 653
    },
    {
      "epoch": 0.872,
      "grad_norm": 1.64601731300354,
      "learning_rate": 0.000251027027027027,
      "loss": 1.9871,
      "step": 654
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 1.488089680671692,
      "learning_rate": 0.0002509459459459459,
      "loss": 1.816,
      "step": 655
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 1.5976173877716064,
      "learning_rate": 0.00025086486486486484,
      "loss": 2.0516,
      "step": 656
    },
    {
      "epoch": 0.876,
      "grad_norm": 1.3124197721481323,
      "learning_rate": 0.0002507837837837838,
      "loss": 1.5419,
      "step": 657
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 1.5190755128860474,
      "learning_rate": 0.00025070270270270267,
      "loss": 1.9615,
      "step": 658
    },
    {
      "epoch": 0.8786666666666667,
      "grad_norm": 1.3039239645004272,
      "learning_rate": 0.0002506216216216216,
      "loss": 1.8722,
      "step": 659
    },
    {
      "epoch": 0.88,
      "grad_norm": 1.3453303575515747,
      "learning_rate": 0.00025054054054054055,
      "loss": 1.9434,
      "step": 660
    },
    {
      "epoch": 0.8813333333333333,
      "grad_norm": 1.3437150716781616,
      "learning_rate": 0.00025045945945945943,
      "loss": 1.5477,
      "step": 661
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 1.4257036447525024,
      "learning_rate": 0.0002503783783783784,
      "loss": 1.7317,
      "step": 662
    },
    {
      "epoch": 0.884,
      "grad_norm": 1.3657938241958618,
      "learning_rate": 0.00025029729729729726,
      "loss": 1.9392,
      "step": 663
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 1.307702660560608,
      "learning_rate": 0.0002502162162162162,
      "loss": 1.9424,
      "step": 664
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 1.4516746997833252,
      "learning_rate": 0.00025013513513513514,
      "loss": 1.6528,
      "step": 665
    },
    {
      "epoch": 0.888,
      "grad_norm": 1.4425030946731567,
      "learning_rate": 0.000250054054054054,
      "loss": 2.2335,
      "step": 666
    },
    {
      "epoch": 0.8893333333333333,
      "grad_norm": 1.4773588180541992,
      "learning_rate": 0.00024997297297297297,
      "loss": 2.1806,
      "step": 667
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 1.4213542938232422,
      "learning_rate": 0.0002498918918918919,
      "loss": 1.9031,
      "step": 668
    },
    {
      "epoch": 0.892,
      "grad_norm": 1.2688894271850586,
      "learning_rate": 0.0002498108108108108,
      "loss": 2.0025,
      "step": 669
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 1.4779330492019653,
      "learning_rate": 0.00024972972972972973,
      "loss": 2.0413,
      "step": 670
    },
    {
      "epoch": 0.8946666666666667,
      "grad_norm": 1.471189022064209,
      "learning_rate": 0.0002496486486486486,
      "loss": 1.78,
      "step": 671
    },
    {
      "epoch": 0.896,
      "grad_norm": 1.489668369293213,
      "learning_rate": 0.00024956756756756756,
      "loss": 1.8042,
      "step": 672
    },
    {
      "epoch": 0.8973333333333333,
      "grad_norm": 1.4782503843307495,
      "learning_rate": 0.0002494864864864865,
      "loss": 2.2615,
      "step": 673
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 1.438194990158081,
      "learning_rate": 0.0002494054054054054,
      "loss": 1.9859,
      "step": 674
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.4878863096237183,
      "learning_rate": 0.0002493243243243243,
      "loss": 1.9257,
      "step": 675
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 1.4632062911987305,
      "learning_rate": 0.0002492432432432432,
      "loss": 1.8001,
      "step": 676
    },
    {
      "epoch": 0.9026666666666666,
      "grad_norm": 1.3921033143997192,
      "learning_rate": 0.0002491621621621621,
      "loss": 1.9528,
      "step": 677
    },
    {
      "epoch": 0.904,
      "grad_norm": 1.4612394571304321,
      "learning_rate": 0.00024908108108108104,
      "loss": 2.0193,
      "step": 678
    },
    {
      "epoch": 0.9053333333333333,
      "grad_norm": 2.0421700477600098,
      "learning_rate": 0.000249,
      "loss": 1.785,
      "step": 679
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 1.6497176885604858,
      "learning_rate": 0.00024891891891891887,
      "loss": 1.7446,
      "step": 680
    },
    {
      "epoch": 0.908,
      "grad_norm": 1.6649131774902344,
      "learning_rate": 0.0002488378378378378,
      "loss": 2.1041,
      "step": 681
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 1.4384187459945679,
      "learning_rate": 0.00024875675675675675,
      "loss": 1.5333,
      "step": 682
    },
    {
      "epoch": 0.9106666666666666,
      "grad_norm": 1.3769289255142212,
      "learning_rate": 0.00024867567567567563,
      "loss": 1.516,
      "step": 683
    },
    {
      "epoch": 0.912,
      "grad_norm": 1.4962666034698486,
      "learning_rate": 0.0002485945945945946,
      "loss": 2.2421,
      "step": 684
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 1.4448152780532837,
      "learning_rate": 0.00024851351351351346,
      "loss": 1.7536,
      "step": 685
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 1.3704222440719604,
      "learning_rate": 0.0002484324324324324,
      "loss": 1.6839,
      "step": 686
    },
    {
      "epoch": 0.916,
      "grad_norm": 1.831425666809082,
      "learning_rate": 0.00024835135135135134,
      "loss": 1.6423,
      "step": 687
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 1.629331111907959,
      "learning_rate": 0.00024827027027027023,
      "loss": 2.0944,
      "step": 688
    },
    {
      "epoch": 0.9186666666666666,
      "grad_norm": 1.3938294649124146,
      "learning_rate": 0.00024818918918918917,
      "loss": 1.8563,
      "step": 689
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.6013892889022827,
      "learning_rate": 0.0002481081081081081,
      "loss": 1.9058,
      "step": 690
    },
    {
      "epoch": 0.9213333333333333,
      "grad_norm": 1.4935041666030884,
      "learning_rate": 0.000248027027027027,
      "loss": 1.664,
      "step": 691
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 1.4184657335281372,
      "learning_rate": 0.00024794594594594593,
      "loss": 1.8806,
      "step": 692
    },
    {
      "epoch": 0.924,
      "grad_norm": 1.620107889175415,
      "learning_rate": 0.0002478648648648648,
      "loss": 1.6865,
      "step": 693
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 1.4345980882644653,
      "learning_rate": 0.00024778378378378376,
      "loss": 1.9953,
      "step": 694
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 1.567958950996399,
      "learning_rate": 0.0002477027027027027,
      "loss": 1.909,
      "step": 695
    },
    {
      "epoch": 0.928,
      "grad_norm": 1.4935777187347412,
      "learning_rate": 0.0002476216216216216,
      "loss": 1.9625,
      "step": 696
    },
    {
      "epoch": 0.9293333333333333,
      "grad_norm": 1.369215488433838,
      "learning_rate": 0.00024754054054054053,
      "loss": 1.6435,
      "step": 697
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 1.4648003578186035,
      "learning_rate": 0.0002474594594594594,
      "loss": 1.9521,
      "step": 698
    },
    {
      "epoch": 0.932,
      "grad_norm": 1.4717979431152344,
      "learning_rate": 0.00024737837837837836,
      "loss": 1.8704,
      "step": 699
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 1.6473147869110107,
      "learning_rate": 0.0002472972972972973,
      "loss": 2.1739,
      "step": 700
    },
    {
      "epoch": 0.9346666666666666,
      "grad_norm": 1.6397554874420166,
      "learning_rate": 0.0002472162162162162,
      "loss": 2.2395,
      "step": 701
    },
    {
      "epoch": 0.936,
      "grad_norm": 1.3046467304229736,
      "learning_rate": 0.0002471351351351351,
      "loss": 1.7926,
      "step": 702
    },
    {
      "epoch": 0.9373333333333334,
      "grad_norm": 1.5502225160598755,
      "learning_rate": 0.00024705405405405406,
      "loss": 1.478,
      "step": 703
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 1.3346667289733887,
      "learning_rate": 0.00024697297297297295,
      "loss": 1.8734,
      "step": 704
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.4010757207870483,
      "learning_rate": 0.0002468918918918919,
      "loss": 2.0834,
      "step": 705
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 1.4988855123519897,
      "learning_rate": 0.0002468108108108108,
      "loss": 1.9481,
      "step": 706
    },
    {
      "epoch": 0.9426666666666667,
      "grad_norm": 1.5606715679168701,
      "learning_rate": 0.0002467297297297297,
      "loss": 2.0103,
      "step": 707
    },
    {
      "epoch": 0.944,
      "grad_norm": 1.628296136856079,
      "learning_rate": 0.00024664864864864866,
      "loss": 1.7518,
      "step": 708
    },
    {
      "epoch": 0.9453333333333334,
      "grad_norm": 1.5076762437820435,
      "learning_rate": 0.00024656756756756754,
      "loss": 2.0398,
      "step": 709
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 1.4736460447311401,
      "learning_rate": 0.0002464864864864865,
      "loss": 1.9469,
      "step": 710
    },
    {
      "epoch": 0.948,
      "grad_norm": 1.7819457054138184,
      "learning_rate": 0.0002464054054054054,
      "loss": 2.1029,
      "step": 711
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 1.9474587440490723,
      "learning_rate": 0.0002463243243243243,
      "loss": 1.3706,
      "step": 712
    },
    {
      "epoch": 0.9506666666666667,
      "grad_norm": 1.8798779249191284,
      "learning_rate": 0.00024624324324324325,
      "loss": 1.964,
      "step": 713
    },
    {
      "epoch": 0.952,
      "grad_norm": 1.3578780889511108,
      "learning_rate": 0.00024616216216216214,
      "loss": 1.4346,
      "step": 714
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 1.7576271295547485,
      "learning_rate": 0.000246081081081081,
      "loss": 2.0825,
      "step": 715
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 1.540101170539856,
      "learning_rate": 0.00024599999999999996,
      "loss": 1.6719,
      "step": 716
    },
    {
      "epoch": 0.956,
      "grad_norm": 1.6305334568023682,
      "learning_rate": 0.0002459189189189189,
      "loss": 1.9749,
      "step": 717
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 1.673224925994873,
      "learning_rate": 0.0002458378378378378,
      "loss": 1.6606,
      "step": 718
    },
    {
      "epoch": 0.9586666666666667,
      "grad_norm": 1.4109514951705933,
      "learning_rate": 0.00024575675675675673,
      "loss": 1.4747,
      "step": 719
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.4316083192825317,
      "learning_rate": 0.00024567567567567567,
      "loss": 1.7373,
      "step": 720
    },
    {
      "epoch": 0.9613333333333334,
      "grad_norm": 1.4697614908218384,
      "learning_rate": 0.00024559459459459456,
      "loss": 1.9714,
      "step": 721
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 1.5748341083526611,
      "learning_rate": 0.0002455135135135135,
      "loss": 1.8994,
      "step": 722
    },
    {
      "epoch": 0.964,
      "grad_norm": 1.3555817604064941,
      "learning_rate": 0.0002454324324324324,
      "loss": 1.8699,
      "step": 723
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 1.576737403869629,
      "learning_rate": 0.0002453513513513513,
      "loss": 1.9891,
      "step": 724
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 1.449564814567566,
      "learning_rate": 0.00024527027027027026,
      "loss": 1.8079,
      "step": 725
    },
    {
      "epoch": 0.968,
      "grad_norm": 1.3859788179397583,
      "learning_rate": 0.00024518918918918915,
      "loss": 1.8275,
      "step": 726
    },
    {
      "epoch": 0.9693333333333334,
      "grad_norm": 1.423826813697815,
      "learning_rate": 0.0002451081081081081,
      "loss": 2.1057,
      "step": 727
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 1.4834892749786377,
      "learning_rate": 0.000245027027027027,
      "loss": 1.9616,
      "step": 728
    },
    {
      "epoch": 0.972,
      "grad_norm": 1.5759472846984863,
      "learning_rate": 0.0002449459459459459,
      "loss": 1.8087,
      "step": 729
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 1.553039789199829,
      "learning_rate": 0.00024486486486486486,
      "loss": 1.8478,
      "step": 730
    },
    {
      "epoch": 0.9746666666666667,
      "grad_norm": 1.5125222206115723,
      "learning_rate": 0.00024478378378378374,
      "loss": 1.7492,
      "step": 731
    },
    {
      "epoch": 0.976,
      "grad_norm": 1.5835583209991455,
      "learning_rate": 0.0002447027027027027,
      "loss": 1.6473,
      "step": 732
    },
    {
      "epoch": 0.9773333333333334,
      "grad_norm": 1.4533390998840332,
      "learning_rate": 0.0002446216216216216,
      "loss": 2.1335,
      "step": 733
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 1.6080049276351929,
      "learning_rate": 0.0002445405405405405,
      "loss": 2.0218,
      "step": 734
    },
    {
      "epoch": 0.98,
      "grad_norm": 1.3130885362625122,
      "learning_rate": 0.00024445945945945945,
      "loss": 1.9417,
      "step": 735
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 1.4195356369018555,
      "learning_rate": 0.00024437837837837834,
      "loss": 1.9409,
      "step": 736
    },
    {
      "epoch": 0.9826666666666667,
      "grad_norm": 1.4682071208953857,
      "learning_rate": 0.0002442972972972973,
      "loss": 1.8465,
      "step": 737
    },
    {
      "epoch": 0.984,
      "grad_norm": 1.4962180852890015,
      "learning_rate": 0.0002442162162162162,
      "loss": 1.8998,
      "step": 738
    },
    {
      "epoch": 0.9853333333333333,
      "grad_norm": 1.5635160207748413,
      "learning_rate": 0.00024413513513513513,
      "loss": 2.0115,
      "step": 739
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 1.6223094463348389,
      "learning_rate": 0.00024405405405405404,
      "loss": 1.5146,
      "step": 740
    },
    {
      "epoch": 0.988,
      "grad_norm": 1.4088019132614136,
      "learning_rate": 0.00024397297297297296,
      "loss": 2.0186,
      "step": 741
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 1.5026659965515137,
      "learning_rate": 0.00024389189189189187,
      "loss": 1.9156,
      "step": 742
    },
    {
      "epoch": 0.9906666666666667,
      "grad_norm": 1.378208875656128,
      "learning_rate": 0.0002438108108108108,
      "loss": 1.4362,
      "step": 743
    },
    {
      "epoch": 0.992,
      "grad_norm": 1.495317816734314,
      "learning_rate": 0.00024372972972972972,
      "loss": 1.9086,
      "step": 744
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 1.424443244934082,
      "learning_rate": 0.00024364864864864864,
      "loss": 1.7635,
      "step": 745
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 1.485363245010376,
      "learning_rate": 0.00024356756756756755,
      "loss": 1.8651,
      "step": 746
    },
    {
      "epoch": 0.996,
      "grad_norm": 1.458247184753418,
      "learning_rate": 0.00024348648648648646,
      "loss": 1.7319,
      "step": 747
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 1.2074649333953857,
      "learning_rate": 0.0002434054054054054,
      "loss": 1.3563,
      "step": 748
    },
    {
      "epoch": 0.9986666666666667,
      "grad_norm": 1.6134089231491089,
      "learning_rate": 0.00024332432432432432,
      "loss": 1.971,
      "step": 749
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.3398804664611816,
      "learning_rate": 0.00024324324324324323,
      "loss": 1.902,
      "step": 750
    },
    {
      "epoch": 1.0013333333333334,
      "grad_norm": 1.408348560333252,
      "learning_rate": 0.00024316216216216214,
      "loss": 1.7465,
      "step": 751
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 1.5074905157089233,
      "learning_rate": 0.00024308108108108108,
      "loss": 2.0521,
      "step": 752
    },
    {
      "epoch": 1.004,
      "grad_norm": 1.4213334321975708,
      "learning_rate": 0.000243,
      "loss": 1.5055,
      "step": 753
    },
    {
      "epoch": 1.0053333333333334,
      "grad_norm": 3.6375699043273926,
      "learning_rate": 0.00024291891891891888,
      "loss": 1.6186,
      "step": 754
    },
    {
      "epoch": 1.0066666666666666,
      "grad_norm": 1.4413005113601685,
      "learning_rate": 0.0002428378378378378,
      "loss": 1.6035,
      "step": 755
    },
    {
      "epoch": 1.008,
      "grad_norm": 1.4031474590301514,
      "learning_rate": 0.0002427567567567567,
      "loss": 1.5216,
      "step": 756
    },
    {
      "epoch": 1.0093333333333334,
      "grad_norm": 1.4333438873291016,
      "learning_rate": 0.00024267567567567565,
      "loss": 1.5863,
      "step": 757
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 1.764144778251648,
      "learning_rate": 0.00024259459459459456,
      "loss": 1.9656,
      "step": 758
    },
    {
      "epoch": 1.012,
      "grad_norm": 2.299920082092285,
      "learning_rate": 0.00024251351351351348,
      "loss": 1.6599,
      "step": 759
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 1.4413783550262451,
      "learning_rate": 0.0002424324324324324,
      "loss": 1.6344,
      "step": 760
    },
    {
      "epoch": 1.0146666666666666,
      "grad_norm": 1.6247576475143433,
      "learning_rate": 0.00024235135135135133,
      "loss": 1.7313,
      "step": 761
    },
    {
      "epoch": 1.016,
      "grad_norm": 1.3656902313232422,
      "learning_rate": 0.00024227027027027024,
      "loss": 1.5673,
      "step": 762
    },
    {
      "epoch": 1.0173333333333334,
      "grad_norm": 1.5222004652023315,
      "learning_rate": 0.00024218918918918916,
      "loss": 1.8544,
      "step": 763
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 1.2797291278839111,
      "learning_rate": 0.00024210810810810807,
      "loss": 1.6611,
      "step": 764
    },
    {
      "epoch": 1.02,
      "grad_norm": 1.4039864540100098,
      "learning_rate": 0.000242027027027027,
      "loss": 2.0753,
      "step": 765
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 1.4448219537734985,
      "learning_rate": 0.00024194594594594592,
      "loss": 1.9284,
      "step": 766
    },
    {
      "epoch": 1.0226666666666666,
      "grad_norm": 1.4241158962249756,
      "learning_rate": 0.00024186486486486484,
      "loss": 1.8513,
      "step": 767
    },
    {
      "epoch": 1.024,
      "grad_norm": 1.5352284908294678,
      "learning_rate": 0.00024178378378378375,
      "loss": 1.6338,
      "step": 768
    },
    {
      "epoch": 1.0253333333333334,
      "grad_norm": 1.322462558746338,
      "learning_rate": 0.00024170270270270266,
      "loss": 1.6497,
      "step": 769
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 1.4586737155914307,
      "learning_rate": 0.0002416216216216216,
      "loss": 1.6075,
      "step": 770
    },
    {
      "epoch": 1.028,
      "grad_norm": 1.5970654487609863,
      "learning_rate": 0.00024154054054054052,
      "loss": 1.9201,
      "step": 771
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 1.7906848192214966,
      "learning_rate": 0.00024145945945945943,
      "loss": 2.1225,
      "step": 772
    },
    {
      "epoch": 1.0306666666666666,
      "grad_norm": 1.3393043279647827,
      "learning_rate": 0.00024137837837837834,
      "loss": 1.9478,
      "step": 773
    },
    {
      "epoch": 1.032,
      "grad_norm": 1.4488916397094727,
      "learning_rate": 0.00024129729729729729,
      "loss": 1.4803,
      "step": 774
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 1.3610953092575073,
      "learning_rate": 0.0002412162162162162,
      "loss": 1.9266,
      "step": 775
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 1.8059594631195068,
      "learning_rate": 0.0002411351351351351,
      "loss": 1.6551,
      "step": 776
    },
    {
      "epoch": 1.036,
      "grad_norm": 1.395171046257019,
      "learning_rate": 0.00024105405405405402,
      "loss": 1.6881,
      "step": 777
    },
    {
      "epoch": 1.0373333333333334,
      "grad_norm": 1.4347847700119019,
      "learning_rate": 0.00024097297297297297,
      "loss": 1.9459,
      "step": 778
    },
    {
      "epoch": 1.0386666666666666,
      "grad_norm": 1.6345973014831543,
      "learning_rate": 0.00024089189189189188,
      "loss": 2.0654,
      "step": 779
    },
    {
      "epoch": 1.04,
      "grad_norm": 1.2640113830566406,
      "learning_rate": 0.0002408108108108108,
      "loss": 1.6304,
      "step": 780
    },
    {
      "epoch": 1.0413333333333332,
      "grad_norm": 1.5364383459091187,
      "learning_rate": 0.0002407297297297297,
      "loss": 1.9623,
      "step": 781
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 1.595749855041504,
      "learning_rate": 0.00024064864864864865,
      "loss": 1.5844,
      "step": 782
    },
    {
      "epoch": 1.044,
      "grad_norm": 1.646409273147583,
      "learning_rate": 0.00024056756756756756,
      "loss": 1.9343,
      "step": 783
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 1.3969818353652954,
      "learning_rate": 0.00024048648648648647,
      "loss": 1.7529,
      "step": 784
    },
    {
      "epoch": 1.0466666666666666,
      "grad_norm": 1.64857816696167,
      "learning_rate": 0.00024040540540540539,
      "loss": 1.777,
      "step": 785
    },
    {
      "epoch": 1.048,
      "grad_norm": 1.433465838432312,
      "learning_rate": 0.00024032432432432433,
      "loss": 1.704,
      "step": 786
    },
    {
      "epoch": 1.0493333333333332,
      "grad_norm": 1.4405767917633057,
      "learning_rate": 0.00024024324324324324,
      "loss": 1.8348,
      "step": 787
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 1.433933138847351,
      "learning_rate": 0.00024016216216216215,
      "loss": 1.7455,
      "step": 788
    },
    {
      "epoch": 1.052,
      "grad_norm": 1.5067131519317627,
      "learning_rate": 0.00024008108108108107,
      "loss": 2.0491,
      "step": 789
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 1.5021178722381592,
      "learning_rate": 0.00023999999999999998,
      "loss": 2.0885,
      "step": 790
    },
    {
      "epoch": 1.0546666666666666,
      "grad_norm": 1.4550503492355347,
      "learning_rate": 0.00023991891891891892,
      "loss": 1.6933,
      "step": 791
    },
    {
      "epoch": 1.056,
      "grad_norm": 1.312738060951233,
      "learning_rate": 0.00023983783783783783,
      "loss": 1.9135,
      "step": 792
    },
    {
      "epoch": 1.0573333333333332,
      "grad_norm": 1.71073579788208,
      "learning_rate": 0.00023975675675675675,
      "loss": 1.9643,
      "step": 793
    },
    {
      "epoch": 1.0586666666666666,
      "grad_norm": 1.3862688541412354,
      "learning_rate": 0.00023967567567567566,
      "loss": 1.6263,
      "step": 794
    },
    {
      "epoch": 1.06,
      "grad_norm": 1.486188530921936,
      "learning_rate": 0.00023959459459459455,
      "loss": 1.6893,
      "step": 795
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 1.6716599464416504,
      "learning_rate": 0.00023951351351351349,
      "loss": 2.0409,
      "step": 796
    },
    {
      "epoch": 1.0626666666666666,
      "grad_norm": 1.425392985343933,
      "learning_rate": 0.0002394324324324324,
      "loss": 1.8654,
      "step": 797
    },
    {
      "epoch": 1.064,
      "grad_norm": 1.4829457998275757,
      "learning_rate": 0.0002393513513513513,
      "loss": 1.8241,
      "step": 798
    },
    {
      "epoch": 1.0653333333333332,
      "grad_norm": 1.4979708194732666,
      "learning_rate": 0.00023927027027027023,
      "loss": 1.68,
      "step": 799
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 1.4811229705810547,
      "learning_rate": 0.00023918918918918917,
      "loss": 1.7789,
      "step": 800
    },
    {
      "epoch": 1.068,
      "grad_norm": 1.4358078241348267,
      "learning_rate": 0.00023910810810810808,
      "loss": 1.8111,
      "step": 801
    },
    {
      "epoch": 1.0693333333333332,
      "grad_norm": 1.6652013063430786,
      "learning_rate": 0.000239027027027027,
      "loss": 1.7484,
      "step": 802
    },
    {
      "epoch": 1.0706666666666667,
      "grad_norm": 1.5268653631210327,
      "learning_rate": 0.0002389459459459459,
      "loss": 1.4541,
      "step": 803
    },
    {
      "epoch": 1.072,
      "grad_norm": 1.4964299201965332,
      "learning_rate": 0.00023886486486486485,
      "loss": 1.3603,
      "step": 804
    },
    {
      "epoch": 1.0733333333333333,
      "grad_norm": 1.5956610441207886,
      "learning_rate": 0.00023878378378378376,
      "loss": 2.1196,
      "step": 805
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 1.6589837074279785,
      "learning_rate": 0.00023870270270270267,
      "loss": 1.9784,
      "step": 806
    },
    {
      "epoch": 1.076,
      "grad_norm": 1.4699925184249878,
      "learning_rate": 0.00023862162162162159,
      "loss": 1.9302,
      "step": 807
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 1.4584640264511108,
      "learning_rate": 0.00023854054054054053,
      "loss": 1.6863,
      "step": 808
    },
    {
      "epoch": 1.0786666666666667,
      "grad_norm": 1.530771017074585,
      "learning_rate": 0.00023845945945945944,
      "loss": 2.3646,
      "step": 809
    },
    {
      "epoch": 1.08,
      "grad_norm": 1.6114284992218018,
      "learning_rate": 0.00023837837837837835,
      "loss": 1.6836,
      "step": 810
    },
    {
      "epoch": 1.0813333333333333,
      "grad_norm": 1.4381695985794067,
      "learning_rate": 0.00023829729729729727,
      "loss": 1.9806,
      "step": 811
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 1.4724488258361816,
      "learning_rate": 0.0002382162162162162,
      "loss": 1.9232,
      "step": 812
    },
    {
      "epoch": 1.084,
      "grad_norm": 1.252781629562378,
      "learning_rate": 0.00023813513513513512,
      "loss": 1.6445,
      "step": 813
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 1.3842682838439941,
      "learning_rate": 0.00023805405405405403,
      "loss": 1.5649,
      "step": 814
    },
    {
      "epoch": 1.0866666666666667,
      "grad_norm": 1.5686990022659302,
      "learning_rate": 0.00023797297297297295,
      "loss": 1.8211,
      "step": 815
    },
    {
      "epoch": 1.088,
      "grad_norm": 1.7471990585327148,
      "learning_rate": 0.00023789189189189186,
      "loss": 1.9942,
      "step": 816
    },
    {
      "epoch": 1.0893333333333333,
      "grad_norm": 1.4342405796051025,
      "learning_rate": 0.0002378108108108108,
      "loss": 1.8248,
      "step": 817
    },
    {
      "epoch": 1.0906666666666667,
      "grad_norm": 1.3651033639907837,
      "learning_rate": 0.00023772972972972971,
      "loss": 1.9398,
      "step": 818
    },
    {
      "epoch": 1.092,
      "grad_norm": 1.5923906564712524,
      "learning_rate": 0.00023764864864864863,
      "loss": 2.1078,
      "step": 819
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 1.3232165575027466,
      "learning_rate": 0.00023756756756756754,
      "loss": 1.6778,
      "step": 820
    },
    {
      "epoch": 1.0946666666666667,
      "grad_norm": 1.3908164501190186,
      "learning_rate": 0.00023748648648648648,
      "loss": 1.7723,
      "step": 821
    },
    {
      "epoch": 1.096,
      "grad_norm": 1.3921033143997192,
      "learning_rate": 0.0002374054054054054,
      "loss": 1.8087,
      "step": 822
    },
    {
      "epoch": 1.0973333333333333,
      "grad_norm": 1.2685809135437012,
      "learning_rate": 0.0002373243243243243,
      "loss": 1.5923,
      "step": 823
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 1.4524452686309814,
      "learning_rate": 0.00023724324324324322,
      "loss": 1.8069,
      "step": 824
    },
    {
      "epoch": 1.1,
      "grad_norm": 1.4133762121200562,
      "learning_rate": 0.00023716216216216216,
      "loss": 2.0658,
      "step": 825
    },
    {
      "epoch": 1.1013333333333333,
      "grad_norm": 1.4950151443481445,
      "learning_rate": 0.00023708108108108107,
      "loss": 1.9877,
      "step": 826
    },
    {
      "epoch": 1.1026666666666667,
      "grad_norm": 1.3586223125457764,
      "learning_rate": 0.000237,
      "loss": 1.7672,
      "step": 827
    },
    {
      "epoch": 1.104,
      "grad_norm": 1.4508435726165771,
      "learning_rate": 0.0002369189189189189,
      "loss": 1.7708,
      "step": 828
    },
    {
      "epoch": 1.1053333333333333,
      "grad_norm": 1.4296473264694214,
      "learning_rate": 0.00023683783783783784,
      "loss": 1.7394,
      "step": 829
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 1.597670078277588,
      "learning_rate": 0.00023675675675675675,
      "loss": 2.0182,
      "step": 830
    },
    {
      "epoch": 1.108,
      "grad_norm": 1.469604730606079,
      "learning_rate": 0.00023667567567567567,
      "loss": 1.6869,
      "step": 831
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 1.6828089952468872,
      "learning_rate": 0.00023659459459459458,
      "loss": 1.7363,
      "step": 832
    },
    {
      "epoch": 1.1106666666666667,
      "grad_norm": 1.6703635454177856,
      "learning_rate": 0.00023651351351351352,
      "loss": 2.2637,
      "step": 833
    },
    {
      "epoch": 1.112,
      "grad_norm": 1.6016159057617188,
      "learning_rate": 0.00023643243243243243,
      "loss": 1.7718,
      "step": 834
    },
    {
      "epoch": 1.1133333333333333,
      "grad_norm": 1.6541646718978882,
      "learning_rate": 0.00023635135135135132,
      "loss": 1.7093,
      "step": 835
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 1.5743967294692993,
      "learning_rate": 0.00023627027027027023,
      "loss": 1.5667,
      "step": 836
    },
    {
      "epoch": 1.116,
      "grad_norm": 1.5953422784805298,
      "learning_rate": 0.00023618918918918915,
      "loss": 1.659,
      "step": 837
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 1.5966150760650635,
      "learning_rate": 0.00023610810810810806,
      "loss": 1.9092,
      "step": 838
    },
    {
      "epoch": 1.1186666666666667,
      "grad_norm": 1.540844202041626,
      "learning_rate": 0.000236027027027027,
      "loss": 2.042,
      "step": 839
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.4909685850143433,
      "learning_rate": 0.00023594594594594591,
      "loss": 2.1257,
      "step": 840
    },
    {
      "epoch": 1.1213333333333333,
      "grad_norm": 1.5582157373428345,
      "learning_rate": 0.00023586486486486483,
      "loss": 1.8406,
      "step": 841
    },
    {
      "epoch": 1.1226666666666667,
      "grad_norm": 1.7464548349380493,
      "learning_rate": 0.00023578378378378374,
      "loss": 1.8737,
      "step": 842
    },
    {
      "epoch": 1.124,
      "grad_norm": 1.529389500617981,
      "learning_rate": 0.00023570270270270268,
      "loss": 1.9793,
      "step": 843
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 1.54947030544281,
      "learning_rate": 0.0002356216216216216,
      "loss": 1.6611,
      "step": 844
    },
    {
      "epoch": 1.1266666666666667,
      "grad_norm": 1.4572824239730835,
      "learning_rate": 0.0002355405405405405,
      "loss": 1.677,
      "step": 845
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 1.5038423538208008,
      "learning_rate": 0.00023545945945945942,
      "loss": 2.3057,
      "step": 846
    },
    {
      "epoch": 1.1293333333333333,
      "grad_norm": 1.4898500442504883,
      "learning_rate": 0.00023537837837837836,
      "loss": 2.2989,
      "step": 847
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 1.3676003217697144,
      "learning_rate": 0.00023529729729729727,
      "loss": 1.8221,
      "step": 848
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 1.49986732006073,
      "learning_rate": 0.0002352162162162162,
      "loss": 1.7461,
      "step": 849
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 1.341357946395874,
      "learning_rate": 0.0002351351351351351,
      "loss": 1.893,
      "step": 850
    },
    {
      "epoch": 1.1346666666666667,
      "grad_norm": 1.4221998453140259,
      "learning_rate": 0.00023505405405405404,
      "loss": 1.7564,
      "step": 851
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 1.3187421560287476,
      "learning_rate": 0.00023497297297297296,
      "loss": 1.7464,
      "step": 852
    },
    {
      "epoch": 1.1373333333333333,
      "grad_norm": 1.3339520692825317,
      "learning_rate": 0.00023489189189189187,
      "loss": 1.527,
      "step": 853
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 1.459049105644226,
      "learning_rate": 0.00023481081081081078,
      "loss": 2.0482,
      "step": 854
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 1.5577969551086426,
      "learning_rate": 0.00023472972972972972,
      "loss": 1.8177,
      "step": 855
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 1.4892284870147705,
      "learning_rate": 0.00023464864864864864,
      "loss": 1.903,
      "step": 856
    },
    {
      "epoch": 1.1426666666666667,
      "grad_norm": 1.3444453477859497,
      "learning_rate": 0.00023456756756756755,
      "loss": 1.5889,
      "step": 857
    },
    {
      "epoch": 1.144,
      "grad_norm": 1.484468698501587,
      "learning_rate": 0.00023448648648648646,
      "loss": 1.9324,
      "step": 858
    },
    {
      "epoch": 1.1453333333333333,
      "grad_norm": 1.5323385000228882,
      "learning_rate": 0.0002344054054054054,
      "loss": 1.9077,
      "step": 859
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 1.4078844785690308,
      "learning_rate": 0.00023432432432432432,
      "loss": 2.1499,
      "step": 860
    },
    {
      "epoch": 1.148,
      "grad_norm": 1.531351089477539,
      "learning_rate": 0.00023424324324324323,
      "loss": 1.9136,
      "step": 861
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 1.54977285861969,
      "learning_rate": 0.00023416216216216214,
      "loss": 1.6459,
      "step": 862
    },
    {
      "epoch": 1.1506666666666667,
      "grad_norm": 1.5643491744995117,
      "learning_rate": 0.00023408108108108106,
      "loss": 2.2534,
      "step": 863
    },
    {
      "epoch": 1.152,
      "grad_norm": 1.508648157119751,
      "learning_rate": 0.000234,
      "loss": 1.4607,
      "step": 864
    },
    {
      "epoch": 1.1533333333333333,
      "grad_norm": 1.6040420532226562,
      "learning_rate": 0.0002339189189189189,
      "loss": 2.4137,
      "step": 865
    },
    {
      "epoch": 1.1546666666666667,
      "grad_norm": 1.5370863676071167,
      "learning_rate": 0.00023383783783783782,
      "loss": 2.1321,
      "step": 866
    },
    {
      "epoch": 1.156,
      "grad_norm": 1.4729899168014526,
      "learning_rate": 0.00023375675675675674,
      "loss": 1.7639,
      "step": 867
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 1.447877287864685,
      "learning_rate": 0.00023367567567567568,
      "loss": 1.9086,
      "step": 868
    },
    {
      "epoch": 1.1586666666666667,
      "grad_norm": 1.4522061347961426,
      "learning_rate": 0.0002335945945945946,
      "loss": 1.8795,
      "step": 869
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.509948968887329,
      "learning_rate": 0.0002335135135135135,
      "loss": 1.6736,
      "step": 870
    },
    {
      "epoch": 1.1613333333333333,
      "grad_norm": 1.4389302730560303,
      "learning_rate": 0.00023343243243243242,
      "loss": 2.0523,
      "step": 871
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 1.423686146736145,
      "learning_rate": 0.00023335135135135136,
      "loss": 2.0512,
      "step": 872
    },
    {
      "epoch": 1.164,
      "grad_norm": 1.479415774345398,
      "learning_rate": 0.00023327027027027027,
      "loss": 1.788,
      "step": 873
    },
    {
      "epoch": 1.1653333333333333,
      "grad_norm": 1.4371572732925415,
      "learning_rate": 0.00023318918918918918,
      "loss": 1.6921,
      "step": 874
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 2.197584390640259,
      "learning_rate": 0.00023310810810810807,
      "loss": 1.7216,
      "step": 875
    },
    {
      "epoch": 1.168,
      "grad_norm": 1.7007167339324951,
      "learning_rate": 0.00023302702702702698,
      "loss": 2.0504,
      "step": 876
    },
    {
      "epoch": 1.1693333333333333,
      "grad_norm": 1.4380991458892822,
      "learning_rate": 0.00023294594594594592,
      "loss": 1.6095,
      "step": 877
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 1.419344425201416,
      "learning_rate": 0.00023286486486486484,
      "loss": 1.6195,
      "step": 878
    },
    {
      "epoch": 1.172,
      "grad_norm": 1.4595978260040283,
      "learning_rate": 0.00023278378378378375,
      "loss": 1.8335,
      "step": 879
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 1.4238277673721313,
      "learning_rate": 0.00023270270270270266,
      "loss": 1.351,
      "step": 880
    },
    {
      "epoch": 1.1746666666666667,
      "grad_norm": 1.3344719409942627,
      "learning_rate": 0.0002326216216216216,
      "loss": 1.5295,
      "step": 881
    },
    {
      "epoch": 1.176,
      "grad_norm": 1.5088862180709839,
      "learning_rate": 0.00023254054054054052,
      "loss": 1.8344,
      "step": 882
    },
    {
      "epoch": 1.1773333333333333,
      "grad_norm": 1.4063854217529297,
      "learning_rate": 0.00023245945945945943,
      "loss": 1.7452,
      "step": 883
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 1.5402042865753174,
      "learning_rate": 0.00023237837837837834,
      "loss": 1.8618,
      "step": 884
    },
    {
      "epoch": 1.18,
      "grad_norm": 1.3162099123001099,
      "learning_rate": 0.00023229729729729726,
      "loss": 1.4956,
      "step": 885
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 1.4563469886779785,
      "learning_rate": 0.0002322162162162162,
      "loss": 1.9949,
      "step": 886
    },
    {
      "epoch": 1.1826666666666668,
      "grad_norm": 1.877667784690857,
      "learning_rate": 0.0002321351351351351,
      "loss": 1.6859,
      "step": 887
    },
    {
      "epoch": 1.184,
      "grad_norm": 1.4758327007293701,
      "learning_rate": 0.00023205405405405402,
      "loss": 1.9378,
      "step": 888
    },
    {
      "epoch": 1.1853333333333333,
      "grad_norm": 1.6071269512176514,
      "learning_rate": 0.00023197297297297294,
      "loss": 2.311,
      "step": 889
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 1.9780659675598145,
      "learning_rate": 0.00023189189189189188,
      "loss": 1.8106,
      "step": 890
    },
    {
      "epoch": 1.188,
      "grad_norm": 1.4788395166397095,
      "learning_rate": 0.0002318108108108108,
      "loss": 1.7878,
      "step": 891
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 1.5025861263275146,
      "learning_rate": 0.0002317297297297297,
      "loss": 2.0959,
      "step": 892
    },
    {
      "epoch": 1.1906666666666668,
      "grad_norm": 1.4906576871871948,
      "learning_rate": 0.00023164864864864862,
      "loss": 1.416,
      "step": 893
    },
    {
      "epoch": 1.192,
      "grad_norm": 1.5610822439193726,
      "learning_rate": 0.00023156756756756756,
      "loss": 2.2179,
      "step": 894
    },
    {
      "epoch": 1.1933333333333334,
      "grad_norm": 1.3973298072814941,
      "learning_rate": 0.00023148648648648647,
      "loss": 1.7153,
      "step": 895
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 1.4872280359268188,
      "learning_rate": 0.00023140540540540538,
      "loss": 1.665,
      "step": 896
    },
    {
      "epoch": 1.196,
      "grad_norm": 1.6314753293991089,
      "learning_rate": 0.0002313243243243243,
      "loss": 1.9721,
      "step": 897
    },
    {
      "epoch": 1.1973333333333334,
      "grad_norm": 1.5505863428115845,
      "learning_rate": 0.00023124324324324324,
      "loss": 1.8479,
      "step": 898
    },
    {
      "epoch": 1.1986666666666665,
      "grad_norm": 1.4277172088623047,
      "learning_rate": 0.00023116216216216215,
      "loss": 1.7098,
      "step": 899
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.4974786043167114,
      "learning_rate": 0.00023108108108108106,
      "loss": 1.9454,
      "step": 900
    },
    {
      "epoch": 1.2013333333333334,
      "grad_norm": 1.3700464963912964,
      "learning_rate": 0.00023099999999999998,
      "loss": 1.5331,
      "step": 901
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 1.4755914211273193,
      "learning_rate": 0.00023091891891891892,
      "loss": 1.7111,
      "step": 902
    },
    {
      "epoch": 1.204,
      "grad_norm": 1.5708736181259155,
      "learning_rate": 0.00023083783783783783,
      "loss": 2.1847,
      "step": 903
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 1.683018684387207,
      "learning_rate": 0.00023075675675675674,
      "loss": 2.1984,
      "step": 904
    },
    {
      "epoch": 1.2066666666666666,
      "grad_norm": 1.471536636352539,
      "learning_rate": 0.00023067567567567566,
      "loss": 1.9013,
      "step": 905
    },
    {
      "epoch": 1.208,
      "grad_norm": 1.455082654953003,
      "learning_rate": 0.00023059459459459457,
      "loss": 1.816,
      "step": 906
    },
    {
      "epoch": 1.2093333333333334,
      "grad_norm": 1.369653582572937,
      "learning_rate": 0.0002305135135135135,
      "loss": 1.5153,
      "step": 907
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 1.4865485429763794,
      "learning_rate": 0.00023043243243243242,
      "loss": 2.0489,
      "step": 908
    },
    {
      "epoch": 1.212,
      "grad_norm": 1.5158761739730835,
      "learning_rate": 0.00023035135135135134,
      "loss": 1.5526,
      "step": 909
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 1.442191243171692,
      "learning_rate": 0.00023027027027027025,
      "loss": 1.5706,
      "step": 910
    },
    {
      "epoch": 1.2146666666666666,
      "grad_norm": 1.548971176147461,
      "learning_rate": 0.0002301891891891892,
      "loss": 1.9695,
      "step": 911
    },
    {
      "epoch": 1.216,
      "grad_norm": 1.4948976039886475,
      "learning_rate": 0.0002301081081081081,
      "loss": 1.6454,
      "step": 912
    },
    {
      "epoch": 1.2173333333333334,
      "grad_norm": 1.423679232597351,
      "learning_rate": 0.00023002702702702702,
      "loss": 1.5836,
      "step": 913
    },
    {
      "epoch": 1.2186666666666666,
      "grad_norm": 1.3991055488586426,
      "learning_rate": 0.00022994594594594593,
      "loss": 1.7391,
      "step": 914
    },
    {
      "epoch": 1.22,
      "grad_norm": 1.5373073816299438,
      "learning_rate": 0.00022986486486486482,
      "loss": 2.025,
      "step": 915
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 1.464783787727356,
      "learning_rate": 0.00022978378378378376,
      "loss": 1.8444,
      "step": 916
    },
    {
      "epoch": 1.2226666666666666,
      "grad_norm": 1.4488109350204468,
      "learning_rate": 0.00022970270270270267,
      "loss": 1.8194,
      "step": 917
    },
    {
      "epoch": 1.224,
      "grad_norm": 1.6000003814697266,
      "learning_rate": 0.00022962162162162158,
      "loss": 1.8815,
      "step": 918
    },
    {
      "epoch": 1.2253333333333334,
      "grad_norm": 1.4270201921463013,
      "learning_rate": 0.0002295405405405405,
      "loss": 1.6201,
      "step": 919
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 1.497399926185608,
      "learning_rate": 0.00022945945945945944,
      "loss": 1.728,
      "step": 920
    },
    {
      "epoch": 1.228,
      "grad_norm": 1.3697763681411743,
      "learning_rate": 0.00022937837837837835,
      "loss": 1.8401,
      "step": 921
    },
    {
      "epoch": 1.2293333333333334,
      "grad_norm": 1.3127152919769287,
      "learning_rate": 0.00022929729729729726,
      "loss": 1.4284,
      "step": 922
    },
    {
      "epoch": 1.2306666666666666,
      "grad_norm": 1.4390182495117188,
      "learning_rate": 0.00022921621621621618,
      "loss": 1.6607,
      "step": 923
    },
    {
      "epoch": 1.232,
      "grad_norm": 1.397477388381958,
      "learning_rate": 0.00022913513513513512,
      "loss": 1.7196,
      "step": 924
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 1.523682713508606,
      "learning_rate": 0.00022905405405405403,
      "loss": 1.4952,
      "step": 925
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 1.5161809921264648,
      "learning_rate": 0.00022897297297297294,
      "loss": 1.991,
      "step": 926
    },
    {
      "epoch": 1.236,
      "grad_norm": 1.5102583169937134,
      "learning_rate": 0.00022889189189189186,
      "loss": 1.614,
      "step": 927
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 1.417924404144287,
      "learning_rate": 0.0002288108108108108,
      "loss": 1.7172,
      "step": 928
    },
    {
      "epoch": 1.2386666666666666,
      "grad_norm": 1.508287787437439,
      "learning_rate": 0.0002287297297297297,
      "loss": 1.8679,
      "step": 929
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.5682704448699951,
      "learning_rate": 0.00022864864864864862,
      "loss": 1.9814,
      "step": 930
    },
    {
      "epoch": 1.2413333333333334,
      "grad_norm": 1.6604515314102173,
      "learning_rate": 0.00022856756756756754,
      "loss": 1.927,
      "step": 931
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 1.3904681205749512,
      "learning_rate": 0.00022848648648648645,
      "loss": 1.6817,
      "step": 932
    },
    {
      "epoch": 1.244,
      "grad_norm": 1.4997297525405884,
      "learning_rate": 0.0002284054054054054,
      "loss": 1.9598,
      "step": 933
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 1.421456217765808,
      "learning_rate": 0.0002283243243243243,
      "loss": 1.7717,
      "step": 934
    },
    {
      "epoch": 1.2466666666666666,
      "grad_norm": 1.463935375213623,
      "learning_rate": 0.00022824324324324322,
      "loss": 1.8017,
      "step": 935
    },
    {
      "epoch": 1.248,
      "grad_norm": 1.6105533838272095,
      "learning_rate": 0.00022816216216216213,
      "loss": 1.8922,
      "step": 936
    },
    {
      "epoch": 1.2493333333333334,
      "grad_norm": 1.5087604522705078,
      "learning_rate": 0.00022808108108108107,
      "loss": 1.9864,
      "step": 937
    },
    {
      "epoch": 1.2506666666666666,
      "grad_norm": 1.5335050821304321,
      "learning_rate": 0.00022799999999999999,
      "loss": 1.8797,
      "step": 938
    },
    {
      "epoch": 1.252,
      "grad_norm": 1.5691930055618286,
      "learning_rate": 0.0002279189189189189,
      "loss": 2.1088,
      "step": 939
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 1.4665533304214478,
      "learning_rate": 0.0002278378378378378,
      "loss": 1.9077,
      "step": 940
    },
    {
      "epoch": 1.2546666666666666,
      "grad_norm": 1.5426275730133057,
      "learning_rate": 0.00022775675675675675,
      "loss": 1.9464,
      "step": 941
    },
    {
      "epoch": 1.256,
      "grad_norm": 1.4887669086456299,
      "learning_rate": 0.00022767567567567567,
      "loss": 2.0407,
      "step": 942
    },
    {
      "epoch": 1.2573333333333334,
      "grad_norm": 1.4157296419143677,
      "learning_rate": 0.00022759459459459458,
      "loss": 1.8177,
      "step": 943
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 1.3547781705856323,
      "learning_rate": 0.0002275135135135135,
      "loss": 1.758,
      "step": 944
    },
    {
      "epoch": 1.26,
      "grad_norm": 1.3999457359313965,
      "learning_rate": 0.00022743243243243243,
      "loss": 1.786,
      "step": 945
    },
    {
      "epoch": 1.2613333333333334,
      "grad_norm": 1.5276392698287964,
      "learning_rate": 0.00022735135135135135,
      "loss": 1.5762,
      "step": 946
    },
    {
      "epoch": 1.2626666666666666,
      "grad_norm": 1.7173391580581665,
      "learning_rate": 0.00022727027027027026,
      "loss": 1.8438,
      "step": 947
    },
    {
      "epoch": 1.264,
      "grad_norm": 1.431963324546814,
      "learning_rate": 0.00022718918918918917,
      "loss": 2.0725,
      "step": 948
    },
    {
      "epoch": 1.2653333333333334,
      "grad_norm": 1.535868525505066,
      "learning_rate": 0.0002271081081081081,
      "loss": 2.1241,
      "step": 949
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 1.5332918167114258,
      "learning_rate": 0.00022702702702702703,
      "loss": 2.0103,
      "step": 950
    },
    {
      "epoch": 1.268,
      "grad_norm": 1.4597653150558472,
      "learning_rate": 0.00022694594594594594,
      "loss": 1.8971,
      "step": 951
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 1.499356985092163,
      "learning_rate": 0.00022686486486486485,
      "loss": 1.9921,
      "step": 952
    },
    {
      "epoch": 1.2706666666666666,
      "grad_norm": 1.3939718008041382,
      "learning_rate": 0.00022678378378378377,
      "loss": 1.708,
      "step": 953
    },
    {
      "epoch": 1.272,
      "grad_norm": 1.4162263870239258,
      "learning_rate": 0.0002267027027027027,
      "loss": 1.3563,
      "step": 954
    },
    {
      "epoch": 1.2733333333333334,
      "grad_norm": 1.4207731485366821,
      "learning_rate": 0.0002266216216216216,
      "loss": 1.5754,
      "step": 955
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 1.7534242868423462,
      "learning_rate": 0.0002265405405405405,
      "loss": 1.9525,
      "step": 956
    },
    {
      "epoch": 1.276,
      "grad_norm": 1.5401304960250854,
      "learning_rate": 0.00022645945945945942,
      "loss": 1.4974,
      "step": 957
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 1.610554814338684,
      "learning_rate": 0.00022637837837837833,
      "loss": 2.0276,
      "step": 958
    },
    {
      "epoch": 1.2786666666666666,
      "grad_norm": 1.5340418815612793,
      "learning_rate": 0.00022629729729729727,
      "loss": 1.4272,
      "step": 959
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.618280291557312,
      "learning_rate": 0.00022621621621621619,
      "loss": 1.6198,
      "step": 960
    },
    {
      "epoch": 1.2813333333333334,
      "grad_norm": 1.5518821477890015,
      "learning_rate": 0.0002261351351351351,
      "loss": 1.9501,
      "step": 961
    },
    {
      "epoch": 1.2826666666666666,
      "grad_norm": 1.434780478477478,
      "learning_rate": 0.000226054054054054,
      "loss": 1.3675,
      "step": 962
    },
    {
      "epoch": 1.284,
      "grad_norm": 1.416761875152588,
      "learning_rate": 0.00022597297297297295,
      "loss": 1.9888,
      "step": 963
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 1.4779174327850342,
      "learning_rate": 0.00022589189189189187,
      "loss": 2.0137,
      "step": 964
    },
    {
      "epoch": 1.2866666666666666,
      "grad_norm": 1.4180470705032349,
      "learning_rate": 0.00022581081081081078,
      "loss": 1.4702,
      "step": 965
    },
    {
      "epoch": 1.288,
      "grad_norm": 1.852632761001587,
      "learning_rate": 0.0002257297297297297,
      "loss": 1.8585,
      "step": 966
    },
    {
      "epoch": 1.2893333333333334,
      "grad_norm": 1.5212887525558472,
      "learning_rate": 0.00022564864864864863,
      "loss": 1.702,
      "step": 967
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 1.6926201581954956,
      "learning_rate": 0.00022556756756756755,
      "loss": 1.6217,
      "step": 968
    },
    {
      "epoch": 1.292,
      "grad_norm": 1.5806902647018433,
      "learning_rate": 0.00022548648648648646,
      "loss": 1.8454,
      "step": 969
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 1.758249044418335,
      "learning_rate": 0.00022540540540540537,
      "loss": 1.9348,
      "step": 970
    },
    {
      "epoch": 1.2946666666666666,
      "grad_norm": 1.628580093383789,
      "learning_rate": 0.00022532432432432431,
      "loss": 1.5748,
      "step": 971
    },
    {
      "epoch": 1.296,
      "grad_norm": 1.7939211130142212,
      "learning_rate": 0.00022524324324324323,
      "loss": 1.6932,
      "step": 972
    },
    {
      "epoch": 1.2973333333333334,
      "grad_norm": 1.489558458328247,
      "learning_rate": 0.00022516216216216214,
      "loss": 1.9062,
      "step": 973
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 1.7206556797027588,
      "learning_rate": 0.00022508108108108105,
      "loss": 2.0095,
      "step": 974
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.4513927698135376,
      "learning_rate": 0.000225,
      "loss": 1.9113,
      "step": 975
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 1.5819809436798096,
      "learning_rate": 0.0002249189189189189,
      "loss": 1.9586,
      "step": 976
    },
    {
      "epoch": 1.3026666666666666,
      "grad_norm": 1.423283338546753,
      "learning_rate": 0.00022483783783783782,
      "loss": 2.0795,
      "step": 977
    },
    {
      "epoch": 1.304,
      "grad_norm": 1.4899005889892578,
      "learning_rate": 0.00022475675675675673,
      "loss": 1.7091,
      "step": 978
    },
    {
      "epoch": 1.3053333333333335,
      "grad_norm": 1.5308756828308105,
      "learning_rate": 0.00022467567567567565,
      "loss": 1.9368,
      "step": 979
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 1.411594271659851,
      "learning_rate": 0.0002245945945945946,
      "loss": 1.8264,
      "step": 980
    },
    {
      "epoch": 1.308,
      "grad_norm": 1.498944878578186,
      "learning_rate": 0.0002245135135135135,
      "loss": 1.9614,
      "step": 981
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 1.4960862398147583,
      "learning_rate": 0.00022443243243243241,
      "loss": 1.7567,
      "step": 982
    },
    {
      "epoch": 1.3106666666666666,
      "grad_norm": 1.5055387020111084,
      "learning_rate": 0.00022435135135135133,
      "loss": 1.6777,
      "step": 983
    },
    {
      "epoch": 1.312,
      "grad_norm": 1.3575232028961182,
      "learning_rate": 0.00022427027027027027,
      "loss": 1.8743,
      "step": 984
    },
    {
      "epoch": 1.3133333333333335,
      "grad_norm": 1.3680843114852905,
      "learning_rate": 0.00022418918918918918,
      "loss": 1.7271,
      "step": 985
    },
    {
      "epoch": 1.3146666666666667,
      "grad_norm": 1.4458152055740356,
      "learning_rate": 0.0002241081081081081,
      "loss": 1.8367,
      "step": 986
    },
    {
      "epoch": 1.316,
      "grad_norm": 1.4585448503494263,
      "learning_rate": 0.000224027027027027,
      "loss": 1.57,
      "step": 987
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 1.4506689310073853,
      "learning_rate": 0.00022394594594594595,
      "loss": 1.7782,
      "step": 988
    },
    {
      "epoch": 1.3186666666666667,
      "grad_norm": 1.3590315580368042,
      "learning_rate": 0.00022386486486486486,
      "loss": 1.8277,
      "step": 989
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.5813556909561157,
      "learning_rate": 0.00022378378378378377,
      "loss": 2.2123,
      "step": 990
    },
    {
      "epoch": 1.3213333333333335,
      "grad_norm": 1.4623771905899048,
      "learning_rate": 0.0002237027027027027,
      "loss": 1.6864,
      "step": 991
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 1.504077672958374,
      "learning_rate": 0.00022362162162162163,
      "loss": 1.6927,
      "step": 992
    },
    {
      "epoch": 1.324,
      "grad_norm": 1.5689656734466553,
      "learning_rate": 0.00022354054054054054,
      "loss": 2.0051,
      "step": 993
    },
    {
      "epoch": 1.3253333333333333,
      "grad_norm": 1.6966639757156372,
      "learning_rate": 0.00022345945945945945,
      "loss": 2.0181,
      "step": 994
    },
    {
      "epoch": 1.3266666666666667,
      "grad_norm": 1.4932183027267456,
      "learning_rate": 0.00022337837837837837,
      "loss": 1.713,
      "step": 995
    },
    {
      "epoch": 1.328,
      "grad_norm": 1.474938988685608,
      "learning_rate": 0.00022329729729729725,
      "loss": 1.8227,
      "step": 996
    },
    {
      "epoch": 1.3293333333333333,
      "grad_norm": 1.6256383657455444,
      "learning_rate": 0.0002232162162162162,
      "loss": 2.1209,
      "step": 997
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 1.4446345567703247,
      "learning_rate": 0.0002231351351351351,
      "loss": 1.9828,
      "step": 998
    },
    {
      "epoch": 1.332,
      "grad_norm": 1.4978605508804321,
      "learning_rate": 0.00022305405405405402,
      "loss": 1.4367,
      "step": 999
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.3416813611984253,
      "learning_rate": 0.00022297297297297293,
      "loss": 1.8496,
      "step": 1000
    },
    {
      "epoch": 1.3346666666666667,
      "grad_norm": 1.381150722503662,
      "learning_rate": 0.00022289189189189185,
      "loss": 1.724,
      "step": 1001
    },
    {
      "epoch": 1.336,
      "grad_norm": 1.8933570384979248,
      "learning_rate": 0.0002228108108108108,
      "loss": 1.845,
      "step": 1002
    },
    {
      "epoch": 1.3373333333333333,
      "grad_norm": 1.2850435972213745,
      "learning_rate": 0.0002227297297297297,
      "loss": 1.5534,
      "step": 1003
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 1.405256986618042,
      "learning_rate": 0.00022264864864864861,
      "loss": 1.7344,
      "step": 1004
    },
    {
      "epoch": 1.34,
      "grad_norm": 1.300010323524475,
      "learning_rate": 0.00022256756756756753,
      "loss": 1.7253,
      "step": 1005
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 1.4123151302337646,
      "learning_rate": 0.00022248648648648647,
      "loss": 1.9119,
      "step": 1006
    },
    {
      "epoch": 1.3426666666666667,
      "grad_norm": 1.654331922531128,
      "learning_rate": 0.00022240540540540538,
      "loss": 1.7188,
      "step": 1007
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 1.4749797582626343,
      "learning_rate": 0.0002223243243243243,
      "loss": 2.1771,
      "step": 1008
    },
    {
      "epoch": 1.3453333333333333,
      "grad_norm": 1.4329184293746948,
      "learning_rate": 0.0002222432432432432,
      "loss": 1.6597,
      "step": 1009
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 1.4578886032104492,
      "learning_rate": 0.00022216216216216215,
      "loss": 1.6929,
      "step": 1010
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 1.559527039527893,
      "learning_rate": 0.00022208108108108106,
      "loss": 1.9314,
      "step": 1011
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 1.5930321216583252,
      "learning_rate": 0.00022199999999999998,
      "loss": 1.6599,
      "step": 1012
    },
    {
      "epoch": 1.3506666666666667,
      "grad_norm": 1.3525683879852295,
      "learning_rate": 0.0002219189189189189,
      "loss": 1.5995,
      "step": 1013
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 1.466424822807312,
      "learning_rate": 0.00022183783783783783,
      "loss": 2.1364,
      "step": 1014
    },
    {
      "epoch": 1.3533333333333333,
      "grad_norm": 1.4591853618621826,
      "learning_rate": 0.00022175675675675674,
      "loss": 1.8218,
      "step": 1015
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 1.7585580348968506,
      "learning_rate": 0.00022167567567567566,
      "loss": 2.0305,
      "step": 1016
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 1.5609912872314453,
      "learning_rate": 0.00022159459459459457,
      "loss": 1.86,
      "step": 1017
    },
    {
      "epoch": 1.3573333333333333,
      "grad_norm": 1.615376353263855,
      "learning_rate": 0.0002215135135135135,
      "loss": 2.1396,
      "step": 1018
    },
    {
      "epoch": 1.3586666666666667,
      "grad_norm": 1.51796293258667,
      "learning_rate": 0.00022143243243243242,
      "loss": 1.8266,
      "step": 1019
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 1.467287540435791,
      "learning_rate": 0.00022135135135135134,
      "loss": 1.5052,
      "step": 1020
    },
    {
      "epoch": 1.3613333333333333,
      "grad_norm": 1.3999836444854736,
      "learning_rate": 0.00022127027027027025,
      "loss": 1.5903,
      "step": 1021
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 1.5042750835418701,
      "learning_rate": 0.00022118918918918916,
      "loss": 2.0854,
      "step": 1022
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 1.3776075839996338,
      "learning_rate": 0.0002211081081081081,
      "loss": 1.7147,
      "step": 1023
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 1.3998290300369263,
      "learning_rate": 0.00022102702702702702,
      "loss": 1.5053,
      "step": 1024
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 1.5711889266967773,
      "learning_rate": 0.00022094594594594593,
      "loss": 1.8735,
      "step": 1025
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 1.278328537940979,
      "learning_rate": 0.00022086486486486484,
      "loss": 1.4669,
      "step": 1026
    },
    {
      "epoch": 1.3693333333333333,
      "grad_norm": 1.6419036388397217,
      "learning_rate": 0.00022078378378378378,
      "loss": 1.894,
      "step": 1027
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 1.523084282875061,
      "learning_rate": 0.0002207027027027027,
      "loss": 1.9327,
      "step": 1028
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 1.4777640104293823,
      "learning_rate": 0.0002206216216216216,
      "loss": 1.8583,
      "step": 1029
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 1.571488380432129,
      "learning_rate": 0.00022054054054054052,
      "loss": 1.8676,
      "step": 1030
    },
    {
      "epoch": 1.3746666666666667,
      "grad_norm": 1.4052565097808838,
      "learning_rate": 0.00022045945945945946,
      "loss": 1.5611,
      "step": 1031
    },
    {
      "epoch": 1.376,
      "grad_norm": 1.5394514799118042,
      "learning_rate": 0.00022037837837837838,
      "loss": 2.0279,
      "step": 1032
    },
    {
      "epoch": 1.3773333333333333,
      "grad_norm": 1.4093350172042847,
      "learning_rate": 0.0002202972972972973,
      "loss": 1.9464,
      "step": 1033
    },
    {
      "epoch": 1.3786666666666667,
      "grad_norm": 1.4399970769882202,
      "learning_rate": 0.0002202162162162162,
      "loss": 2.0482,
      "step": 1034
    },
    {
      "epoch": 1.38,
      "grad_norm": 1.3214539289474487,
      "learning_rate": 0.00022013513513513514,
      "loss": 1.5843,
      "step": 1035
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 1.4666168689727783,
      "learning_rate": 0.00022005405405405403,
      "loss": 1.7992,
      "step": 1036
    },
    {
      "epoch": 1.3826666666666667,
      "grad_norm": 1.4202666282653809,
      "learning_rate": 0.00021997297297297294,
      "loss": 1.6443,
      "step": 1037
    },
    {
      "epoch": 1.384,
      "grad_norm": 1.4421271085739136,
      "learning_rate": 0.00021989189189189186,
      "loss": 1.8623,
      "step": 1038
    },
    {
      "epoch": 1.3853333333333333,
      "grad_norm": 1.3653578758239746,
      "learning_rate": 0.00021981081081081077,
      "loss": 1.891,
      "step": 1039
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 1.5026283264160156,
      "learning_rate": 0.0002197297297297297,
      "loss": 1.7076,
      "step": 1040
    },
    {
      "epoch": 1.388,
      "grad_norm": 1.456918478012085,
      "learning_rate": 0.00021964864864864862,
      "loss": 1.7189,
      "step": 1041
    },
    {
      "epoch": 1.3893333333333333,
      "grad_norm": 1.4829877614974976,
      "learning_rate": 0.00021956756756756754,
      "loss": 1.8239,
      "step": 1042
    },
    {
      "epoch": 1.3906666666666667,
      "grad_norm": 1.4784868955612183,
      "learning_rate": 0.00021948648648648645,
      "loss": 1.867,
      "step": 1043
    },
    {
      "epoch": 1.392,
      "grad_norm": 1.529232144355774,
      "learning_rate": 0.0002194054054054054,
      "loss": 1.7435,
      "step": 1044
    },
    {
      "epoch": 1.3933333333333333,
      "grad_norm": 1.4055510759353638,
      "learning_rate": 0.0002193243243243243,
      "loss": 1.6823,
      "step": 1045
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 1.4472622871398926,
      "learning_rate": 0.00021924324324324322,
      "loss": 1.9135,
      "step": 1046
    },
    {
      "epoch": 1.396,
      "grad_norm": 1.444305419921875,
      "learning_rate": 0.00021916216216216213,
      "loss": 1.8113,
      "step": 1047
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 1.370874285697937,
      "learning_rate": 0.00021908108108108104,
      "loss": 1.6943,
      "step": 1048
    },
    {
      "epoch": 1.3986666666666667,
      "grad_norm": 1.4115720987319946,
      "learning_rate": 0.00021899999999999998,
      "loss": 1.5102,
      "step": 1049
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.5156322717666626,
      "learning_rate": 0.0002189189189189189,
      "loss": 1.6298,
      "step": 1050
    },
    {
      "epoch": 1.4013333333333333,
      "grad_norm": 1.3728677034378052,
      "learning_rate": 0.0002188378378378378,
      "loss": 1.6619,
      "step": 1051
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 1.5199857950210571,
      "learning_rate": 0.00021875675675675672,
      "loss": 1.8733,
      "step": 1052
    },
    {
      "epoch": 1.404,
      "grad_norm": 1.3264267444610596,
      "learning_rate": 0.00021867567567567566,
      "loss": 1.4225,
      "step": 1053
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 1.485425353050232,
      "learning_rate": 0.00021859459459459458,
      "loss": 1.7738,
      "step": 1054
    },
    {
      "epoch": 1.4066666666666667,
      "grad_norm": 1.4622581005096436,
      "learning_rate": 0.0002185135135135135,
      "loss": 1.9245,
      "step": 1055
    },
    {
      "epoch": 1.408,
      "grad_norm": 1.5034167766571045,
      "learning_rate": 0.0002184324324324324,
      "loss": 1.9011,
      "step": 1056
    },
    {
      "epoch": 1.4093333333333333,
      "grad_norm": 1.3912562131881714,
      "learning_rate": 0.00021835135135135134,
      "loss": 1.778,
      "step": 1057
    },
    {
      "epoch": 1.4106666666666667,
      "grad_norm": 1.3913432359695435,
      "learning_rate": 0.00021827027027027026,
      "loss": 1.9744,
      "step": 1058
    },
    {
      "epoch": 1.412,
      "grad_norm": 1.3766443729400635,
      "learning_rate": 0.00021818918918918917,
      "loss": 1.6482,
      "step": 1059
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 1.43132483959198,
      "learning_rate": 0.00021810810810810808,
      "loss": 1.5164,
      "step": 1060
    },
    {
      "epoch": 1.4146666666666667,
      "grad_norm": 1.5331428050994873,
      "learning_rate": 0.00021802702702702702,
      "loss": 2.1321,
      "step": 1061
    },
    {
      "epoch": 1.416,
      "grad_norm": 1.418537974357605,
      "learning_rate": 0.00021794594594594594,
      "loss": 1.5812,
      "step": 1062
    },
    {
      "epoch": 1.4173333333333333,
      "grad_norm": 1.4944543838500977,
      "learning_rate": 0.00021786486486486485,
      "loss": 1.5168,
      "step": 1063
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 1.4275727272033691,
      "learning_rate": 0.00021778378378378376,
      "loss": 2.0478,
      "step": 1064
    },
    {
      "epoch": 1.42,
      "grad_norm": 1.5961482524871826,
      "learning_rate": 0.0002177027027027027,
      "loss": 1.9247,
      "step": 1065
    },
    {
      "epoch": 1.4213333333333333,
      "grad_norm": 1.434523582458496,
      "learning_rate": 0.00021762162162162162,
      "loss": 2.0068,
      "step": 1066
    },
    {
      "epoch": 1.4226666666666667,
      "grad_norm": 1.5034972429275513,
      "learning_rate": 0.00021754054054054053,
      "loss": 1.9867,
      "step": 1067
    },
    {
      "epoch": 1.424,
      "grad_norm": 1.6030954122543335,
      "learning_rate": 0.00021745945945945944,
      "loss": 1.7682,
      "step": 1068
    },
    {
      "epoch": 1.4253333333333333,
      "grad_norm": 1.4864248037338257,
      "learning_rate": 0.00021737837837837836,
      "loss": 1.7605,
      "step": 1069
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 1.3641090393066406,
      "learning_rate": 0.0002172972972972973,
      "loss": 1.4288,
      "step": 1070
    },
    {
      "epoch": 1.428,
      "grad_norm": 1.5758540630340576,
      "learning_rate": 0.0002172162162162162,
      "loss": 2.0723,
      "step": 1071
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 1.4395314455032349,
      "learning_rate": 0.00021713513513513512,
      "loss": 1.7688,
      "step": 1072
    },
    {
      "epoch": 1.4306666666666668,
      "grad_norm": 1.414398193359375,
      "learning_rate": 0.00021705405405405404,
      "loss": 1.7445,
      "step": 1073
    },
    {
      "epoch": 1.432,
      "grad_norm": 1.5973085165023804,
      "learning_rate": 0.00021697297297297298,
      "loss": 1.9052,
      "step": 1074
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 1.5749037265777588,
      "learning_rate": 0.0002168918918918919,
      "loss": 1.6956,
      "step": 1075
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 1.612045168876648,
      "learning_rate": 0.00021681081081081078,
      "loss": 2.2077,
      "step": 1076
    },
    {
      "epoch": 1.436,
      "grad_norm": 1.4643441438674927,
      "learning_rate": 0.0002167297297297297,
      "loss": 1.5935,
      "step": 1077
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 1.521240472793579,
      "learning_rate": 0.0002166486486486486,
      "loss": 1.6795,
      "step": 1078
    },
    {
      "epoch": 1.4386666666666668,
      "grad_norm": 1.4058712720870972,
      "learning_rate": 0.00021656756756756754,
      "loss": 1.5519,
      "step": 1079
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.4442094564437866,
      "learning_rate": 0.00021648648648648646,
      "loss": 1.5844,
      "step": 1080
    },
    {
      "epoch": 1.4413333333333334,
      "grad_norm": 1.50504469871521,
      "learning_rate": 0.00021640540540540537,
      "loss": 2.0515,
      "step": 1081
    },
    {
      "epoch": 1.4426666666666668,
      "grad_norm": 1.4856204986572266,
      "learning_rate": 0.00021632432432432428,
      "loss": 1.8114,
      "step": 1082
    },
    {
      "epoch": 1.444,
      "grad_norm": 1.4278099536895752,
      "learning_rate": 0.00021624324324324322,
      "loss": 1.7943,
      "step": 1083
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 1.429173231124878,
      "learning_rate": 0.00021616216216216214,
      "loss": 1.6628,
      "step": 1084
    },
    {
      "epoch": 1.4466666666666668,
      "grad_norm": 1.462451457977295,
      "learning_rate": 0.00021608108108108105,
      "loss": 2.1207,
      "step": 1085
    },
    {
      "epoch": 1.448,
      "grad_norm": 1.3280962705612183,
      "learning_rate": 0.00021599999999999996,
      "loss": 1.3163,
      "step": 1086
    },
    {
      "epoch": 1.4493333333333334,
      "grad_norm": 1.410330057144165,
      "learning_rate": 0.0002159189189189189,
      "loss": 1.7441,
      "step": 1087
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 1.4383759498596191,
      "learning_rate": 0.00021583783783783782,
      "loss": 1.7319,
      "step": 1088
    },
    {
      "epoch": 1.452,
      "grad_norm": 1.8131641149520874,
      "learning_rate": 0.00021575675675675673,
      "loss": 1.8306,
      "step": 1089
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 1.730040431022644,
      "learning_rate": 0.00021567567567567565,
      "loss": 1.7837,
      "step": 1090
    },
    {
      "epoch": 1.4546666666666668,
      "grad_norm": 1.524369716644287,
      "learning_rate": 0.00021559459459459456,
      "loss": 2.1301,
      "step": 1091
    },
    {
      "epoch": 1.456,
      "grad_norm": 1.4135688543319702,
      "learning_rate": 0.0002155135135135135,
      "loss": 1.697,
      "step": 1092
    },
    {
      "epoch": 1.4573333333333334,
      "grad_norm": 1.474645733833313,
      "learning_rate": 0.0002154324324324324,
      "loss": 1.8808,
      "step": 1093
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 1.5713214874267578,
      "learning_rate": 0.00021535135135135133,
      "loss": 1.7435,
      "step": 1094
    },
    {
      "epoch": 1.46,
      "grad_norm": 1.4836593866348267,
      "learning_rate": 0.00021527027027027024,
      "loss": 1.8933,
      "step": 1095
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 1.4297925233840942,
      "learning_rate": 0.00021518918918918918,
      "loss": 1.7827,
      "step": 1096
    },
    {
      "epoch": 1.4626666666666668,
      "grad_norm": 1.5993062257766724,
      "learning_rate": 0.0002151081081081081,
      "loss": 1.9024,
      "step": 1097
    },
    {
      "epoch": 1.464,
      "grad_norm": 1.560893177986145,
      "learning_rate": 0.000215027027027027,
      "loss": 1.586,
      "step": 1098
    },
    {
      "epoch": 1.4653333333333334,
      "grad_norm": 1.4607489109039307,
      "learning_rate": 0.00021494594594594592,
      "loss": 1.8842,
      "step": 1099
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 1.5398954153060913,
      "learning_rate": 0.00021486486486486486,
      "loss": 1.943,
      "step": 1100
    },
    {
      "epoch": 1.468,
      "grad_norm": 1.5357532501220703,
      "learning_rate": 0.00021478378378378377,
      "loss": 2.0001,
      "step": 1101
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 1.3629701137542725,
      "learning_rate": 0.00021470270270270269,
      "loss": 1.5284,
      "step": 1102
    },
    {
      "epoch": 1.4706666666666668,
      "grad_norm": 1.5164083242416382,
      "learning_rate": 0.0002146216216216216,
      "loss": 1.5826,
      "step": 1103
    },
    {
      "epoch": 1.472,
      "grad_norm": 1.5529977083206177,
      "learning_rate": 0.00021454054054054054,
      "loss": 1.9526,
      "step": 1104
    },
    {
      "epoch": 1.4733333333333334,
      "grad_norm": 1.4576681852340698,
      "learning_rate": 0.00021445945945945945,
      "loss": 1.7029,
      "step": 1105
    },
    {
      "epoch": 1.4746666666666668,
      "grad_norm": 1.4969812631607056,
      "learning_rate": 0.00021437837837837837,
      "loss": 1.7644,
      "step": 1106
    },
    {
      "epoch": 1.476,
      "grad_norm": 1.4175059795379639,
      "learning_rate": 0.00021429729729729728,
      "loss": 1.8957,
      "step": 1107
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 1.6723545789718628,
      "learning_rate": 0.00021421621621621622,
      "loss": 1.77,
      "step": 1108
    },
    {
      "epoch": 1.4786666666666668,
      "grad_norm": 1.4601480960845947,
      "learning_rate": 0.00021413513513513513,
      "loss": 1.553,
      "step": 1109
    },
    {
      "epoch": 1.48,
      "grad_norm": 1.5258069038391113,
      "learning_rate": 0.00021405405405405405,
      "loss": 1.7756,
      "step": 1110
    },
    {
      "epoch": 1.4813333333333334,
      "grad_norm": 1.431286096572876,
      "learning_rate": 0.00021397297297297296,
      "loss": 1.6627,
      "step": 1111
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 1.5884109735488892,
      "learning_rate": 0.0002138918918918919,
      "loss": 1.9744,
      "step": 1112
    },
    {
      "epoch": 1.484,
      "grad_norm": 1.8215868473052979,
      "learning_rate": 0.0002138108108108108,
      "loss": 1.761,
      "step": 1113
    },
    {
      "epoch": 1.4853333333333334,
      "grad_norm": 1.6416022777557373,
      "learning_rate": 0.00021372972972972973,
      "loss": 2.0748,
      "step": 1114
    },
    {
      "epoch": 1.4866666666666668,
      "grad_norm": 1.7378374338150024,
      "learning_rate": 0.00021364864864864864,
      "loss": 1.5586,
      "step": 1115
    },
    {
      "epoch": 1.488,
      "grad_norm": 1.5033468008041382,
      "learning_rate": 0.00021356756756756753,
      "loss": 2.1587,
      "step": 1116
    },
    {
      "epoch": 1.4893333333333334,
      "grad_norm": 1.4435712099075317,
      "learning_rate": 0.00021348648648648644,
      "loss": 1.8099,
      "step": 1117
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 1.58512544631958,
      "learning_rate": 0.00021340540540540538,
      "loss": 1.9416,
      "step": 1118
    },
    {
      "epoch": 1.492,
      "grad_norm": 1.5293570756912231,
      "learning_rate": 0.0002133243243243243,
      "loss": 1.7761,
      "step": 1119
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 1.4015815258026123,
      "learning_rate": 0.0002132432432432432,
      "loss": 1.8079,
      "step": 1120
    },
    {
      "epoch": 1.4946666666666666,
      "grad_norm": 1.4899513721466064,
      "learning_rate": 0.00021316216216216212,
      "loss": 2.0113,
      "step": 1121
    },
    {
      "epoch": 1.496,
      "grad_norm": 1.3618419170379639,
      "learning_rate": 0.00021308108108108106,
      "loss": 1.7204,
      "step": 1122
    },
    {
      "epoch": 1.4973333333333334,
      "grad_norm": 1.5221196413040161,
      "learning_rate": 0.00021299999999999997,
      "loss": 1.7898,
      "step": 1123
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 1.4065934419631958,
      "learning_rate": 0.0002129189189189189,
      "loss": 1.9633,
      "step": 1124
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.6284804344177246,
      "learning_rate": 0.0002128378378378378,
      "loss": 2.0136,
      "step": 1125
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 1.2989000082015991,
      "learning_rate": 0.00021275675675675674,
      "loss": 1.656,
      "step": 1126
    },
    {
      "epoch": 1.5026666666666668,
      "grad_norm": 1.5831843614578247,
      "learning_rate": 0.00021267567567567565,
      "loss": 1.5036,
      "step": 1127
    },
    {
      "epoch": 1.504,
      "grad_norm": 1.5134533643722534,
      "learning_rate": 0.00021259459459459457,
      "loss": 1.7298,
      "step": 1128
    },
    {
      "epoch": 1.5053333333333332,
      "grad_norm": 1.4774510860443115,
      "learning_rate": 0.00021251351351351348,
      "loss": 1.7251,
      "step": 1129
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 1.6585136651992798,
      "learning_rate": 0.00021243243243243242,
      "loss": 1.6951,
      "step": 1130
    },
    {
      "epoch": 1.508,
      "grad_norm": 1.4610109329223633,
      "learning_rate": 0.00021235135135135133,
      "loss": 1.9204,
      "step": 1131
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 1.4443600177764893,
      "learning_rate": 0.00021227027027027025,
      "loss": 1.6301,
      "step": 1132
    },
    {
      "epoch": 1.5106666666666668,
      "grad_norm": 1.4254697561264038,
      "learning_rate": 0.00021218918918918916,
      "loss": 1.718,
      "step": 1133
    },
    {
      "epoch": 1.512,
      "grad_norm": 1.451874852180481,
      "learning_rate": 0.0002121081081081081,
      "loss": 1.8994,
      "step": 1134
    },
    {
      "epoch": 1.5133333333333332,
      "grad_norm": 1.4937812089920044,
      "learning_rate": 0.00021202702702702701,
      "loss": 2.0568,
      "step": 1135
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 1.6823481321334839,
      "learning_rate": 0.00021194594594594593,
      "loss": 1.9978,
      "step": 1136
    },
    {
      "epoch": 1.516,
      "grad_norm": 1.4267526865005493,
      "learning_rate": 0.00021186486486486484,
      "loss": 1.9093,
      "step": 1137
    },
    {
      "epoch": 1.5173333333333332,
      "grad_norm": 1.5202950239181519,
      "learning_rate": 0.00021178378378378375,
      "loss": 2.0967,
      "step": 1138
    },
    {
      "epoch": 1.5186666666666668,
      "grad_norm": 1.6404716968536377,
      "learning_rate": 0.0002117027027027027,
      "loss": 1.6947,
      "step": 1139
    },
    {
      "epoch": 1.52,
      "grad_norm": 1.5072004795074463,
      "learning_rate": 0.0002116216216216216,
      "loss": 1.6897,
      "step": 1140
    },
    {
      "epoch": 1.5213333333333332,
      "grad_norm": 1.5775736570358276,
      "learning_rate": 0.00021154054054054052,
      "loss": 1.6077,
      "step": 1141
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 1.290899634361267,
      "learning_rate": 0.00021145945945945943,
      "loss": 1.507,
      "step": 1142
    },
    {
      "epoch": 1.524,
      "grad_norm": 1.2455087900161743,
      "learning_rate": 0.00021137837837837837,
      "loss": 1.6408,
      "step": 1143
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 1.4742292165756226,
      "learning_rate": 0.0002112972972972973,
      "loss": 1.6814,
      "step": 1144
    },
    {
      "epoch": 1.5266666666666666,
      "grad_norm": 1.3909541368484497,
      "learning_rate": 0.0002112162162162162,
      "loss": 1.7614,
      "step": 1145
    },
    {
      "epoch": 1.528,
      "grad_norm": 1.4429600238800049,
      "learning_rate": 0.00021113513513513511,
      "loss": 1.7718,
      "step": 1146
    },
    {
      "epoch": 1.5293333333333332,
      "grad_norm": 1.620977759361267,
      "learning_rate": 0.00021105405405405405,
      "loss": 1.6178,
      "step": 1147
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 1.4514720439910889,
      "learning_rate": 0.00021097297297297297,
      "loss": 2.0891,
      "step": 1148
    },
    {
      "epoch": 1.532,
      "grad_norm": 1.4231171607971191,
      "learning_rate": 0.00021089189189189188,
      "loss": 1.923,
      "step": 1149
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 1.475218415260315,
      "learning_rate": 0.0002108108108108108,
      "loss": 1.959,
      "step": 1150
    },
    {
      "epoch": 1.5346666666666666,
      "grad_norm": 1.6556624174118042,
      "learning_rate": 0.00021072972972972973,
      "loss": 1.7704,
      "step": 1151
    },
    {
      "epoch": 1.536,
      "grad_norm": 1.5860106945037842,
      "learning_rate": 0.00021064864864864865,
      "loss": 1.7955,
      "step": 1152
    },
    {
      "epoch": 1.5373333333333332,
      "grad_norm": 1.3132847547531128,
      "learning_rate": 0.00021056756756756756,
      "loss": 1.7267,
      "step": 1153
    },
    {
      "epoch": 1.5386666666666666,
      "grad_norm": 1.47672438621521,
      "learning_rate": 0.00021048648648648647,
      "loss": 1.6617,
      "step": 1154
    },
    {
      "epoch": 1.54,
      "grad_norm": 1.5037047863006592,
      "learning_rate": 0.00021040540540540542,
      "loss": 1.9066,
      "step": 1155
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 1.5070888996124268,
      "learning_rate": 0.0002103243243243243,
      "loss": 1.8549,
      "step": 1156
    },
    {
      "epoch": 1.5426666666666666,
      "grad_norm": 1.6516454219818115,
      "learning_rate": 0.00021024324324324321,
      "loss": 2.0835,
      "step": 1157
    },
    {
      "epoch": 1.544,
      "grad_norm": 1.5060322284698486,
      "learning_rate": 0.00021016216216216213,
      "loss": 1.7074,
      "step": 1158
    },
    {
      "epoch": 1.5453333333333332,
      "grad_norm": 1.474053978919983,
      "learning_rate": 0.00021008108108108104,
      "loss": 1.6661,
      "step": 1159
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 1.418074369430542,
      "learning_rate": 0.00020999999999999998,
      "loss": 1.8149,
      "step": 1160
    },
    {
      "epoch": 1.548,
      "grad_norm": 1.3997308015823364,
      "learning_rate": 0.0002099189189189189,
      "loss": 1.7001,
      "step": 1161
    },
    {
      "epoch": 1.5493333333333332,
      "grad_norm": 1.4493924379348755,
      "learning_rate": 0.0002098378378378378,
      "loss": 1.6433,
      "step": 1162
    },
    {
      "epoch": 1.5506666666666666,
      "grad_norm": 1.4742119312286377,
      "learning_rate": 0.00020975675675675672,
      "loss": 1.8945,
      "step": 1163
    },
    {
      "epoch": 1.552,
      "grad_norm": 1.411008358001709,
      "learning_rate": 0.00020967567567567563,
      "loss": 1.6244,
      "step": 1164
    },
    {
      "epoch": 1.5533333333333332,
      "grad_norm": 1.4512330293655396,
      "learning_rate": 0.00020959459459459458,
      "loss": 1.8498,
      "step": 1165
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 1.4208115339279175,
      "learning_rate": 0.0002095135135135135,
      "loss": 1.8676,
      "step": 1166
    },
    {
      "epoch": 1.556,
      "grad_norm": 1.5599501132965088,
      "learning_rate": 0.0002094324324324324,
      "loss": 1.8466,
      "step": 1167
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 1.415928602218628,
      "learning_rate": 0.00020935135135135131,
      "loss": 1.8914,
      "step": 1168
    },
    {
      "epoch": 1.5586666666666666,
      "grad_norm": 1.414184331893921,
      "learning_rate": 0.00020927027027027026,
      "loss": 1.9616,
      "step": 1169
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.332465648651123,
      "learning_rate": 0.00020918918918918917,
      "loss": 1.6617,
      "step": 1170
    },
    {
      "epoch": 1.5613333333333332,
      "grad_norm": 1.5459372997283936,
      "learning_rate": 0.00020910810810810808,
      "loss": 1.9436,
      "step": 1171
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 1.4184621572494507,
      "learning_rate": 0.000209027027027027,
      "loss": 1.7826,
      "step": 1172
    },
    {
      "epoch": 1.564,
      "grad_norm": 1.4383772611618042,
      "learning_rate": 0.00020894594594594594,
      "loss": 2.0432,
      "step": 1173
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 1.321019172668457,
      "learning_rate": 0.00020886486486486485,
      "loss": 1.5594,
      "step": 1174
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 1.5125881433486938,
      "learning_rate": 0.00020878378378378376,
      "loss": 1.8434,
      "step": 1175
    },
    {
      "epoch": 1.568,
      "grad_norm": 1.7102071046829224,
      "learning_rate": 0.00020870270270270268,
      "loss": 2.1495,
      "step": 1176
    },
    {
      "epoch": 1.5693333333333332,
      "grad_norm": 1.6351507902145386,
      "learning_rate": 0.00020862162162162162,
      "loss": 2.1474,
      "step": 1177
    },
    {
      "epoch": 1.5706666666666667,
      "grad_norm": 1.5851035118103027,
      "learning_rate": 0.00020854054054054053,
      "loss": 1.9198,
      "step": 1178
    },
    {
      "epoch": 1.572,
      "grad_norm": 1.4675581455230713,
      "learning_rate": 0.00020845945945945944,
      "loss": 1.7103,
      "step": 1179
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 1.4335963726043701,
      "learning_rate": 0.00020837837837837836,
      "loss": 1.6995,
      "step": 1180
    },
    {
      "epoch": 1.5746666666666667,
      "grad_norm": 1.5551046133041382,
      "learning_rate": 0.0002082972972972973,
      "loss": 1.88,
      "step": 1181
    },
    {
      "epoch": 1.576,
      "grad_norm": 1.4825553894042969,
      "learning_rate": 0.0002082162162162162,
      "loss": 1.5772,
      "step": 1182
    },
    {
      "epoch": 1.5773333333333333,
      "grad_norm": 1.7555646896362305,
      "learning_rate": 0.00020813513513513512,
      "loss": 2.1444,
      "step": 1183
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 1.3508998155593872,
      "learning_rate": 0.00020805405405405404,
      "loss": 1.5544,
      "step": 1184
    },
    {
      "epoch": 1.58,
      "grad_norm": 1.3929338455200195,
      "learning_rate": 0.00020797297297297295,
      "loss": 1.4732,
      "step": 1185
    },
    {
      "epoch": 1.5813333333333333,
      "grad_norm": 1.3953090906143188,
      "learning_rate": 0.0002078918918918919,
      "loss": 1.8096,
      "step": 1186
    },
    {
      "epoch": 1.5826666666666667,
      "grad_norm": 1.3381766080856323,
      "learning_rate": 0.0002078108108108108,
      "loss": 1.5301,
      "step": 1187
    },
    {
      "epoch": 1.584,
      "grad_norm": 1.3870645761489868,
      "learning_rate": 0.00020772972972972972,
      "loss": 1.6802,
      "step": 1188
    },
    {
      "epoch": 1.5853333333333333,
      "grad_norm": 1.468057632446289,
      "learning_rate": 0.00020764864864864863,
      "loss": 2.041,
      "step": 1189
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 1.3177380561828613,
      "learning_rate": 0.00020756756756756757,
      "loss": 1.3637,
      "step": 1190
    },
    {
      "epoch": 1.588,
      "grad_norm": 1.4954596757888794,
      "learning_rate": 0.00020748648648648648,
      "loss": 1.8523,
      "step": 1191
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 1.4772493839263916,
      "learning_rate": 0.0002074054054054054,
      "loss": 1.5915,
      "step": 1192
    },
    {
      "epoch": 1.5906666666666667,
      "grad_norm": 1.4798848628997803,
      "learning_rate": 0.0002073243243243243,
      "loss": 1.7526,
      "step": 1193
    },
    {
      "epoch": 1.592,
      "grad_norm": 1.590354323387146,
      "learning_rate": 0.00020724324324324325,
      "loss": 1.9766,
      "step": 1194
    },
    {
      "epoch": 1.5933333333333333,
      "grad_norm": 1.4837250709533691,
      "learning_rate": 0.00020716216216216216,
      "loss": 1.8791,
      "step": 1195
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 1.7023752927780151,
      "learning_rate": 0.00020708108108108108,
      "loss": 1.7477,
      "step": 1196
    },
    {
      "epoch": 1.596,
      "grad_norm": 1.407876968383789,
      "learning_rate": 0.00020699999999999996,
      "loss": 1.6916,
      "step": 1197
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 1.413564920425415,
      "learning_rate": 0.00020691891891891888,
      "loss": 1.6266,
      "step": 1198
    },
    {
      "epoch": 1.5986666666666667,
      "grad_norm": 1.5630481243133545,
      "learning_rate": 0.00020683783783783782,
      "loss": 1.8781,
      "step": 1199
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.4445325136184692,
      "learning_rate": 0.00020675675675675673,
      "loss": 1.4404,
      "step": 1200
    },
    {
      "epoch": 1.6013333333333333,
      "grad_norm": 1.4967014789581299,
      "learning_rate": 0.00020667567567567564,
      "loss": 1.7853,
      "step": 1201
    },
    {
      "epoch": 1.6026666666666667,
      "grad_norm": 1.368016242980957,
      "learning_rate": 0.00020659459459459456,
      "loss": 1.576,
      "step": 1202
    },
    {
      "epoch": 1.604,
      "grad_norm": 1.5698360204696655,
      "learning_rate": 0.0002065135135135135,
      "loss": 1.8261,
      "step": 1203
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 1.4280672073364258,
      "learning_rate": 0.0002064324324324324,
      "loss": 1.674,
      "step": 1204
    },
    {
      "epoch": 1.6066666666666667,
      "grad_norm": 1.5967442989349365,
      "learning_rate": 0.00020635135135135132,
      "loss": 2.0921,
      "step": 1205
    },
    {
      "epoch": 1.608,
      "grad_norm": 1.5814521312713623,
      "learning_rate": 0.00020627027027027024,
      "loss": 1.92,
      "step": 1206
    },
    {
      "epoch": 1.6093333333333333,
      "grad_norm": 1.5039604902267456,
      "learning_rate": 0.00020618918918918915,
      "loss": 1.5842,
      "step": 1207
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 1.5268720388412476,
      "learning_rate": 0.0002061081081081081,
      "loss": 1.7388,
      "step": 1208
    },
    {
      "epoch": 1.612,
      "grad_norm": 1.5515632629394531,
      "learning_rate": 0.000206027027027027,
      "loss": 1.6938,
      "step": 1209
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 1.5557868480682373,
      "learning_rate": 0.00020594594594594592,
      "loss": 1.5079,
      "step": 1210
    },
    {
      "epoch": 1.6146666666666667,
      "grad_norm": 1.451385498046875,
      "learning_rate": 0.00020586486486486483,
      "loss": 1.9078,
      "step": 1211
    },
    {
      "epoch": 1.616,
      "grad_norm": 1.4995591640472412,
      "learning_rate": 0.00020578378378378377,
      "loss": 1.8995,
      "step": 1212
    },
    {
      "epoch": 1.6173333333333333,
      "grad_norm": 1.4793754816055298,
      "learning_rate": 0.00020570270270270268,
      "loss": 1.7477,
      "step": 1213
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 1.7142568826675415,
      "learning_rate": 0.0002056216216216216,
      "loss": 1.6806,
      "step": 1214
    },
    {
      "epoch": 1.62,
      "grad_norm": 1.4832346439361572,
      "learning_rate": 0.0002055405405405405,
      "loss": 1.8873,
      "step": 1215
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 1.5393836498260498,
      "learning_rate": 0.00020545945945945945,
      "loss": 1.7101,
      "step": 1216
    },
    {
      "epoch": 1.6226666666666667,
      "grad_norm": 1.5722172260284424,
      "learning_rate": 0.00020537837837837836,
      "loss": 1.8957,
      "step": 1217
    },
    {
      "epoch": 1.624,
      "grad_norm": 1.5565115213394165,
      "learning_rate": 0.00020529729729729728,
      "loss": 1.8977,
      "step": 1218
    },
    {
      "epoch": 1.6253333333333333,
      "grad_norm": 1.5038437843322754,
      "learning_rate": 0.0002052162162162162,
      "loss": 1.7576,
      "step": 1219
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 1.6566162109375,
      "learning_rate": 0.00020513513513513513,
      "loss": 1.6589,
      "step": 1220
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 1.686975359916687,
      "learning_rate": 0.00020505405405405404,
      "loss": 1.9262,
      "step": 1221
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 1.498616099357605,
      "learning_rate": 0.00020497297297297296,
      "loss": 1.6674,
      "step": 1222
    },
    {
      "epoch": 1.6306666666666667,
      "grad_norm": 1.394333004951477,
      "learning_rate": 0.00020489189189189187,
      "loss": 1.536,
      "step": 1223
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 1.5611917972564697,
      "learning_rate": 0.0002048108108108108,
      "loss": 2.0459,
      "step": 1224
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 1.7081737518310547,
      "learning_rate": 0.00020472972972972972,
      "loss": 1.8992,
      "step": 1225
    },
    {
      "epoch": 1.6346666666666667,
      "grad_norm": 1.741698145866394,
      "learning_rate": 0.00020464864864864864,
      "loss": 1.933,
      "step": 1226
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 2.675023317337036,
      "learning_rate": 0.00020456756756756755,
      "loss": 1.7723,
      "step": 1227
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 1.5462933778762817,
      "learning_rate": 0.0002044864864864865,
      "loss": 1.6137,
      "step": 1228
    },
    {
      "epoch": 1.6386666666666667,
      "grad_norm": 1.5566966533660889,
      "learning_rate": 0.0002044054054054054,
      "loss": 2.1068,
      "step": 1229
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1.6085125207901,
      "learning_rate": 0.00020432432432432432,
      "loss": 1.7936,
      "step": 1230
    },
    {
      "epoch": 1.6413333333333333,
      "grad_norm": 1.336765170097351,
      "learning_rate": 0.00020424324324324323,
      "loss": 1.7597,
      "step": 1231
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 1.5384279489517212,
      "learning_rate": 0.00020416216216216214,
      "loss": 1.6784,
      "step": 1232
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 1.4104559421539307,
      "learning_rate": 0.00020408108108108109,
      "loss": 1.6483,
      "step": 1233
    },
    {
      "epoch": 1.6453333333333333,
      "grad_norm": 1.53311026096344,
      "learning_rate": 0.000204,
      "loss": 1.7663,
      "step": 1234
    },
    {
      "epoch": 1.6466666666666665,
      "grad_norm": 1.4748456478118896,
      "learning_rate": 0.0002039189189189189,
      "loss": 1.538,
      "step": 1235
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 1.664318561553955,
      "learning_rate": 0.00020383783783783782,
      "loss": 1.9605,
      "step": 1236
    },
    {
      "epoch": 1.6493333333333333,
      "grad_norm": 1.5137258768081665,
      "learning_rate": 0.0002037567567567567,
      "loss": 1.5433,
      "step": 1237
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 1.419524908065796,
      "learning_rate": 0.00020367567567567565,
      "loss": 1.9107,
      "step": 1238
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 1.6273257732391357,
      "learning_rate": 0.00020359459459459456,
      "loss": 1.8353,
      "step": 1239
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 1.4233434200286865,
      "learning_rate": 0.00020351351351351348,
      "loss": 1.5967,
      "step": 1240
    },
    {
      "epoch": 1.6546666666666665,
      "grad_norm": 1.503658413887024,
      "learning_rate": 0.0002034324324324324,
      "loss": 1.7537,
      "step": 1241
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 1.5741561651229858,
      "learning_rate": 0.00020335135135135133,
      "loss": 2.0615,
      "step": 1242
    },
    {
      "epoch": 1.6573333333333333,
      "grad_norm": 1.5920705795288086,
      "learning_rate": 0.00020327027027027025,
      "loss": 2.1268,
      "step": 1243
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 1.4586949348449707,
      "learning_rate": 0.00020318918918918916,
      "loss": 1.7257,
      "step": 1244
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 1.4984369277954102,
      "learning_rate": 0.00020310810810810807,
      "loss": 1.948,
      "step": 1245
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 1.6194943189620972,
      "learning_rate": 0.000203027027027027,
      "loss": 1.6827,
      "step": 1246
    },
    {
      "epoch": 1.6626666666666665,
      "grad_norm": 1.2977713346481323,
      "learning_rate": 0.00020294594594594593,
      "loss": 1.7576,
      "step": 1247
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 1.4491535425186157,
      "learning_rate": 0.00020286486486486484,
      "loss": 1.7756,
      "step": 1248
    },
    {
      "epoch": 1.6653333333333333,
      "grad_norm": 1.5420984029769897,
      "learning_rate": 0.00020278378378378375,
      "loss": 2.0382,
      "step": 1249
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 1.4480454921722412,
      "learning_rate": 0.0002027027027027027,
      "loss": 1.9497,
      "step": 1250
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 1.5594768524169922,
      "learning_rate": 0.0002026216216216216,
      "loss": 2.0506,
      "step": 1251
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 1.4201151132583618,
      "learning_rate": 0.00020254054054054052,
      "loss": 1.8758,
      "step": 1252
    },
    {
      "epoch": 1.6706666666666665,
      "grad_norm": 1.4562835693359375,
      "learning_rate": 0.00020245945945945943,
      "loss": 1.7507,
      "step": 1253
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 1.4375543594360352,
      "learning_rate": 0.00020237837837837835,
      "loss": 1.5856,
      "step": 1254
    },
    {
      "epoch": 1.6733333333333333,
      "grad_norm": 1.3226433992385864,
      "learning_rate": 0.00020229729729729729,
      "loss": 1.7691,
      "step": 1255
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 1.3956817388534546,
      "learning_rate": 0.0002022162162162162,
      "loss": 2.0487,
      "step": 1256
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 1.3987904787063599,
      "learning_rate": 0.0002021351351351351,
      "loss": 1.7248,
      "step": 1257
    },
    {
      "epoch": 1.6773333333333333,
      "grad_norm": 1.4464718103408813,
      "learning_rate": 0.00020205405405405403,
      "loss": 1.4721,
      "step": 1258
    },
    {
      "epoch": 1.6786666666666665,
      "grad_norm": 1.5019004344940186,
      "learning_rate": 0.00020197297297297297,
      "loss": 1.7372,
      "step": 1259
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 1.4610669612884521,
      "learning_rate": 0.00020189189189189188,
      "loss": 1.7155,
      "step": 1260
    },
    {
      "epoch": 1.6813333333333333,
      "grad_norm": 1.3804740905761719,
      "learning_rate": 0.0002018108108108108,
      "loss": 1.9992,
      "step": 1261
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 1.3307791948318481,
      "learning_rate": 0.0002017297297297297,
      "loss": 1.7495,
      "step": 1262
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 1.3415454626083374,
      "learning_rate": 0.00020164864864864865,
      "loss": 1.6418,
      "step": 1263
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 1.3772436380386353,
      "learning_rate": 0.00020156756756756756,
      "loss": 1.8044,
      "step": 1264
    },
    {
      "epoch": 1.6866666666666665,
      "grad_norm": 1.6211416721343994,
      "learning_rate": 0.00020148648648648647,
      "loss": 1.6626,
      "step": 1265
    },
    {
      "epoch": 1.688,
      "grad_norm": 1.5375895500183105,
      "learning_rate": 0.00020140540540540539,
      "loss": 1.8498,
      "step": 1266
    },
    {
      "epoch": 1.6893333333333334,
      "grad_norm": 1.741750717163086,
      "learning_rate": 0.00020132432432432433,
      "loss": 1.885,
      "step": 1267
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 1.7574527263641357,
      "learning_rate": 0.00020124324324324324,
      "loss": 1.8512,
      "step": 1268
    },
    {
      "epoch": 1.692,
      "grad_norm": 1.4554964303970337,
      "learning_rate": 0.00020116216216216215,
      "loss": 1.5885,
      "step": 1269
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 1.422205924987793,
      "learning_rate": 0.00020108108108108107,
      "loss": 1.7484,
      "step": 1270
    },
    {
      "epoch": 1.6946666666666665,
      "grad_norm": 1.6812107563018799,
      "learning_rate": 0.000201,
      "loss": 1.5878,
      "step": 1271
    },
    {
      "epoch": 1.696,
      "grad_norm": 1.523667573928833,
      "learning_rate": 0.00020091891891891892,
      "loss": 1.592,
      "step": 1272
    },
    {
      "epoch": 1.6973333333333334,
      "grad_norm": 1.645257830619812,
      "learning_rate": 0.00020083783783783783,
      "loss": 1.8263,
      "step": 1273
    },
    {
      "epoch": 1.6986666666666665,
      "grad_norm": 1.4582233428955078,
      "learning_rate": 0.00020075675675675675,
      "loss": 1.6587,
      "step": 1274
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.5124388933181763,
      "learning_rate": 0.00020067567567567566,
      "loss": 1.6997,
      "step": 1275
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 1.3587208986282349,
      "learning_rate": 0.0002005945945945946,
      "loss": 1.6552,
      "step": 1276
    },
    {
      "epoch": 1.7026666666666666,
      "grad_norm": 1.3622339963912964,
      "learning_rate": 0.0002005135135135135,
      "loss": 1.5074,
      "step": 1277
    },
    {
      "epoch": 1.704,
      "grad_norm": 1.4463012218475342,
      "learning_rate": 0.0002004324324324324,
      "loss": 1.622,
      "step": 1278
    },
    {
      "epoch": 1.7053333333333334,
      "grad_norm": 1.4934566020965576,
      "learning_rate": 0.0002003513513513513,
      "loss": 1.6124,
      "step": 1279
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 1.417206883430481,
      "learning_rate": 0.00020027027027027023,
      "loss": 1.4464,
      "step": 1280
    },
    {
      "epoch": 1.708,
      "grad_norm": 1.493035078048706,
      "learning_rate": 0.00020018918918918917,
      "loss": 1.8903,
      "step": 1281
    },
    {
      "epoch": 1.7093333333333334,
      "grad_norm": 1.5944520235061646,
      "learning_rate": 0.00020010810810810808,
      "loss": 2.0677,
      "step": 1282
    },
    {
      "epoch": 1.7106666666666666,
      "grad_norm": 1.469293236732483,
      "learning_rate": 0.000200027027027027,
      "loss": 1.5163,
      "step": 1283
    },
    {
      "epoch": 1.712,
      "grad_norm": 1.7236799001693726,
      "learning_rate": 0.0001999459459459459,
      "loss": 2.1952,
      "step": 1284
    },
    {
      "epoch": 1.7133333333333334,
      "grad_norm": 1.7054249048233032,
      "learning_rate": 0.00019986486486486485,
      "loss": 2.2342,
      "step": 1285
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 1.4709393978118896,
      "learning_rate": 0.00019978378378378376,
      "loss": 1.7015,
      "step": 1286
    },
    {
      "epoch": 1.716,
      "grad_norm": 1.3587422370910645,
      "learning_rate": 0.00019970270270270267,
      "loss": 1.6484,
      "step": 1287
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 1.5806069374084473,
      "learning_rate": 0.0001996216216216216,
      "loss": 1.8769,
      "step": 1288
    },
    {
      "epoch": 1.7186666666666666,
      "grad_norm": 1.4491560459136963,
      "learning_rate": 0.00019954054054054053,
      "loss": 1.5676,
      "step": 1289
    },
    {
      "epoch": 1.72,
      "grad_norm": 1.451022982597351,
      "learning_rate": 0.00019945945945945944,
      "loss": 1.7417,
      "step": 1290
    },
    {
      "epoch": 1.7213333333333334,
      "grad_norm": 1.4259659051895142,
      "learning_rate": 0.00019937837837837835,
      "loss": 1.7676,
      "step": 1291
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 1.4856208562850952,
      "learning_rate": 0.00019929729729729727,
      "loss": 1.8017,
      "step": 1292
    },
    {
      "epoch": 1.724,
      "grad_norm": 1.4905548095703125,
      "learning_rate": 0.0001992162162162162,
      "loss": 1.6503,
      "step": 1293
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 1.4635587930679321,
      "learning_rate": 0.00019913513513513512,
      "loss": 1.6869,
      "step": 1294
    },
    {
      "epoch": 1.7266666666666666,
      "grad_norm": 1.503616452217102,
      "learning_rate": 0.00019905405405405403,
      "loss": 1.8567,
      "step": 1295
    },
    {
      "epoch": 1.728,
      "grad_norm": 1.4618439674377441,
      "learning_rate": 0.00019897297297297295,
      "loss": 1.8506,
      "step": 1296
    },
    {
      "epoch": 1.7293333333333334,
      "grad_norm": 1.5954997539520264,
      "learning_rate": 0.0001988918918918919,
      "loss": 1.8823,
      "step": 1297
    },
    {
      "epoch": 1.7306666666666666,
      "grad_norm": 1.5513725280761719,
      "learning_rate": 0.0001988108108108108,
      "loss": 1.8319,
      "step": 1298
    },
    {
      "epoch": 1.732,
      "grad_norm": 1.488101840019226,
      "learning_rate": 0.00019872972972972971,
      "loss": 2.0565,
      "step": 1299
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 1.6443068981170654,
      "learning_rate": 0.00019864864864864863,
      "loss": 1.8156,
      "step": 1300
    },
    {
      "epoch": 1.7346666666666666,
      "grad_norm": 1.52955162525177,
      "learning_rate": 0.00019856756756756754,
      "loss": 1.8826,
      "step": 1301
    },
    {
      "epoch": 1.736,
      "grad_norm": 1.539061188697815,
      "learning_rate": 0.00019848648648648648,
      "loss": 1.5126,
      "step": 1302
    },
    {
      "epoch": 1.7373333333333334,
      "grad_norm": 1.5041613578796387,
      "learning_rate": 0.0001984054054054054,
      "loss": 1.8575,
      "step": 1303
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 1.3877977132797241,
      "learning_rate": 0.0001983243243243243,
      "loss": 1.6461,
      "step": 1304
    },
    {
      "epoch": 1.74,
      "grad_norm": 1.427003264427185,
      "learning_rate": 0.00019824324324324322,
      "loss": 1.4944,
      "step": 1305
    },
    {
      "epoch": 1.7413333333333334,
      "grad_norm": 1.5460518598556519,
      "learning_rate": 0.00019816216216216216,
      "loss": 2.2686,
      "step": 1306
    },
    {
      "epoch": 1.7426666666666666,
      "grad_norm": 1.584089994430542,
      "learning_rate": 0.00019808108108108107,
      "loss": 1.7473,
      "step": 1307
    },
    {
      "epoch": 1.744,
      "grad_norm": 1.6361961364746094,
      "learning_rate": 0.000198,
      "loss": 2.0563,
      "step": 1308
    },
    {
      "epoch": 1.7453333333333334,
      "grad_norm": 1.5348972082138062,
      "learning_rate": 0.0001979189189189189,
      "loss": 1.9006,
      "step": 1309
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 1.4918053150177002,
      "learning_rate": 0.00019783783783783784,
      "loss": 1.8981,
      "step": 1310
    },
    {
      "epoch": 1.748,
      "grad_norm": 1.5835014581680298,
      "learning_rate": 0.00019775675675675676,
      "loss": 2.0021,
      "step": 1311
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 1.4031305313110352,
      "learning_rate": 0.00019767567567567567,
      "loss": 1.726,
      "step": 1312
    },
    {
      "epoch": 1.7506666666666666,
      "grad_norm": 1.5694127082824707,
      "learning_rate": 0.00019759459459459458,
      "loss": 1.8231,
      "step": 1313
    },
    {
      "epoch": 1.752,
      "grad_norm": 1.6172841787338257,
      "learning_rate": 0.00019751351351351352,
      "loss": 1.4144,
      "step": 1314
    },
    {
      "epoch": 1.7533333333333334,
      "grad_norm": 1.4532917737960815,
      "learning_rate": 0.00019743243243243244,
      "loss": 1.4834,
      "step": 1315
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 1.3655457496643066,
      "learning_rate": 0.00019735135135135135,
      "loss": 1.4159,
      "step": 1316
    },
    {
      "epoch": 1.756,
      "grad_norm": 1.479042410850525,
      "learning_rate": 0.00019727027027027023,
      "loss": 1.8734,
      "step": 1317
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 1.5737955570220947,
      "learning_rate": 0.00019718918918918915,
      "loss": 1.9885,
      "step": 1318
    },
    {
      "epoch": 1.7586666666666666,
      "grad_norm": 1.5298112630844116,
      "learning_rate": 0.0001971081081081081,
      "loss": 1.7979,
      "step": 1319
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.589214563369751,
      "learning_rate": 0.000197027027027027,
      "loss": 2.0009,
      "step": 1320
    },
    {
      "epoch": 1.7613333333333334,
      "grad_norm": 1.661351203918457,
      "learning_rate": 0.00019694594594594591,
      "loss": 1.648,
      "step": 1321
    },
    {
      "epoch": 1.7626666666666666,
      "grad_norm": 1.6245195865631104,
      "learning_rate": 0.00019686486486486483,
      "loss": 1.7311,
      "step": 1322
    },
    {
      "epoch": 1.764,
      "grad_norm": 1.6134892702102661,
      "learning_rate": 0.00019678378378378374,
      "loss": 2.0088,
      "step": 1323
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 1.439717411994934,
      "learning_rate": 0.00019670270270270268,
      "loss": 1.7129,
      "step": 1324
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 1.5762789249420166,
      "learning_rate": 0.0001966216216216216,
      "loss": 1.9598,
      "step": 1325
    },
    {
      "epoch": 1.768,
      "grad_norm": 1.5986882448196411,
      "learning_rate": 0.0001965405405405405,
      "loss": 2.0008,
      "step": 1326
    },
    {
      "epoch": 1.7693333333333334,
      "grad_norm": 1.5361188650131226,
      "learning_rate": 0.00019645945945945942,
      "loss": 1.9551,
      "step": 1327
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 1.63138747215271,
      "learning_rate": 0.00019637837837837836,
      "loss": 1.9447,
      "step": 1328
    },
    {
      "epoch": 1.772,
      "grad_norm": 1.5152593851089478,
      "learning_rate": 0.00019629729729729728,
      "loss": 1.676,
      "step": 1329
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 1.4243488311767578,
      "learning_rate": 0.0001962162162162162,
      "loss": 1.6408,
      "step": 1330
    },
    {
      "epoch": 1.7746666666666666,
      "grad_norm": 1.5443296432495117,
      "learning_rate": 0.0001961351351351351,
      "loss": 1.9619,
      "step": 1331
    },
    {
      "epoch": 1.776,
      "grad_norm": 1.5092612504959106,
      "learning_rate": 0.00019605405405405404,
      "loss": 1.7461,
      "step": 1332
    },
    {
      "epoch": 1.7773333333333334,
      "grad_norm": 1.485090970993042,
      "learning_rate": 0.00019597297297297296,
      "loss": 1.5482,
      "step": 1333
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 1.3923912048339844,
      "learning_rate": 0.00019589189189189187,
      "loss": 1.7741,
      "step": 1334
    },
    {
      "epoch": 1.78,
      "grad_norm": 1.529655933380127,
      "learning_rate": 0.00019581081081081078,
      "loss": 1.5158,
      "step": 1335
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 1.5148975849151611,
      "learning_rate": 0.00019572972972972972,
      "loss": 1.9511,
      "step": 1336
    },
    {
      "epoch": 1.7826666666666666,
      "grad_norm": 1.4230568408966064,
      "learning_rate": 0.00019564864864864864,
      "loss": 1.5289,
      "step": 1337
    },
    {
      "epoch": 1.784,
      "grad_norm": 1.403735637664795,
      "learning_rate": 0.00019556756756756755,
      "loss": 1.6596,
      "step": 1338
    },
    {
      "epoch": 1.7853333333333334,
      "grad_norm": 1.5789740085601807,
      "learning_rate": 0.00019548648648648646,
      "loss": 1.9659,
      "step": 1339
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 1.515839695930481,
      "learning_rate": 0.0001954054054054054,
      "loss": 1.6264,
      "step": 1340
    },
    {
      "epoch": 1.788,
      "grad_norm": 1.3290820121765137,
      "learning_rate": 0.00019532432432432432,
      "loss": 1.7667,
      "step": 1341
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 1.3942712545394897,
      "learning_rate": 0.00019524324324324323,
      "loss": 1.7082,
      "step": 1342
    },
    {
      "epoch": 1.7906666666666666,
      "grad_norm": 1.5302459001541138,
      "learning_rate": 0.00019516216216216214,
      "loss": 1.8823,
      "step": 1343
    },
    {
      "epoch": 1.792,
      "grad_norm": 1.6043802499771118,
      "learning_rate": 0.00019508108108108108,
      "loss": 1.6162,
      "step": 1344
    },
    {
      "epoch": 1.7933333333333334,
      "grad_norm": 1.6734675168991089,
      "learning_rate": 0.000195,
      "loss": 2.1525,
      "step": 1345
    },
    {
      "epoch": 1.7946666666666666,
      "grad_norm": 1.7428957223892212,
      "learning_rate": 0.0001949189189189189,
      "loss": 1.8774,
      "step": 1346
    },
    {
      "epoch": 1.796,
      "grad_norm": 1.520379900932312,
      "learning_rate": 0.00019483783783783782,
      "loss": 1.9915,
      "step": 1347
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 1.6266487836837769,
      "learning_rate": 0.00019475675675675674,
      "loss": 1.881,
      "step": 1348
    },
    {
      "epoch": 1.7986666666666666,
      "grad_norm": 1.3888640403747559,
      "learning_rate": 0.00019467567567567568,
      "loss": 1.7405,
      "step": 1349
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.5750941038131714,
      "learning_rate": 0.0001945945945945946,
      "loss": 1.6442,
      "step": 1350
    },
    {
      "epoch": 1.8013333333333335,
      "grad_norm": 1.563292384147644,
      "learning_rate": 0.0001945135135135135,
      "loss": 1.9541,
      "step": 1351
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 1.4926238059997559,
      "learning_rate": 0.00019443243243243242,
      "loss": 2.1172,
      "step": 1352
    },
    {
      "epoch": 1.804,
      "grad_norm": 1.4802420139312744,
      "learning_rate": 0.00019435135135135136,
      "loss": 1.9173,
      "step": 1353
    },
    {
      "epoch": 1.8053333333333335,
      "grad_norm": 1.6424431800842285,
      "learning_rate": 0.00019427027027027027,
      "loss": 1.8081,
      "step": 1354
    },
    {
      "epoch": 1.8066666666666666,
      "grad_norm": 1.556451439857483,
      "learning_rate": 0.00019418918918918918,
      "loss": 1.7948,
      "step": 1355
    },
    {
      "epoch": 1.808,
      "grad_norm": 1.5989713668823242,
      "learning_rate": 0.0001941081081081081,
      "loss": 1.8028,
      "step": 1356
    },
    {
      "epoch": 1.8093333333333335,
      "grad_norm": 1.6518886089324951,
      "learning_rate": 0.00019402702702702704,
      "loss": 2.0054,
      "step": 1357
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 1.5293585062026978,
      "learning_rate": 0.00019394594594594592,
      "loss": 2.0105,
      "step": 1358
    },
    {
      "epoch": 1.812,
      "grad_norm": 1.5453976392745972,
      "learning_rate": 0.00019386486486486484,
      "loss": 1.8529,
      "step": 1359
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 1.4609184265136719,
      "learning_rate": 0.00019378378378378375,
      "loss": 1.3904,
      "step": 1360
    },
    {
      "epoch": 1.8146666666666667,
      "grad_norm": 1.45531165599823,
      "learning_rate": 0.00019370270270270266,
      "loss": 1.6846,
      "step": 1361
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 1.524943232536316,
      "learning_rate": 0.0001936216216216216,
      "loss": 1.2193,
      "step": 1362
    },
    {
      "epoch": 1.8173333333333335,
      "grad_norm": 1.4370261430740356,
      "learning_rate": 0.00019354054054054052,
      "loss": 1.6225,
      "step": 1363
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 1.3964200019836426,
      "learning_rate": 0.00019345945945945943,
      "loss": 0.9903,
      "step": 1364
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 1.647408366203308,
      "learning_rate": 0.00019337837837837834,
      "loss": 1.9855,
      "step": 1365
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 1.5570968389511108,
      "learning_rate": 0.00019329729729729728,
      "loss": 1.8697,
      "step": 1366
    },
    {
      "epoch": 1.8226666666666667,
      "grad_norm": 1.4439088106155396,
      "learning_rate": 0.0001932162162162162,
      "loss": 1.6049,
      "step": 1367
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 1.4605138301849365,
      "learning_rate": 0.0001931351351351351,
      "loss": 1.679,
      "step": 1368
    },
    {
      "epoch": 1.8253333333333335,
      "grad_norm": 1.5687124729156494,
      "learning_rate": 0.00019305405405405402,
      "loss": 1.7751,
      "step": 1369
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 1.3868223428726196,
      "learning_rate": 0.00019297297297297294,
      "loss": 1.6776,
      "step": 1370
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 1.5718305110931396,
      "learning_rate": 0.00019289189189189188,
      "loss": 2.1701,
      "step": 1371
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 1.4346922636032104,
      "learning_rate": 0.0001928108108108108,
      "loss": 1.5344,
      "step": 1372
    },
    {
      "epoch": 1.8306666666666667,
      "grad_norm": 1.4358898401260376,
      "learning_rate": 0.0001927297297297297,
      "loss": 1.5307,
      "step": 1373
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 1.4676759243011475,
      "learning_rate": 0.00019264864864864862,
      "loss": 1.9636,
      "step": 1374
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 1.5235587358474731,
      "learning_rate": 0.00019256756756756756,
      "loss": 1.8334,
      "step": 1375
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 1.5245766639709473,
      "learning_rate": 0.00019248648648648647,
      "loss": 1.5746,
      "step": 1376
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 1.453437089920044,
      "learning_rate": 0.00019240540540540538,
      "loss": 1.6033,
      "step": 1377
    },
    {
      "epoch": 1.8373333333333335,
      "grad_norm": 1.4979441165924072,
      "learning_rate": 0.0001923243243243243,
      "loss": 1.5371,
      "step": 1378
    },
    {
      "epoch": 1.8386666666666667,
      "grad_norm": 1.513182282447815,
      "learning_rate": 0.00019224324324324324,
      "loss": 1.9162,
      "step": 1379
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.423868179321289,
      "learning_rate": 0.00019216216216216215,
      "loss": 1.9816,
      "step": 1380
    },
    {
      "epoch": 1.8413333333333335,
      "grad_norm": 1.7324104309082031,
      "learning_rate": 0.00019208108108108106,
      "loss": 1.6987,
      "step": 1381
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 1.5617767572402954,
      "learning_rate": 0.00019199999999999998,
      "loss": 1.8558,
      "step": 1382
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 1.4948668479919434,
      "learning_rate": 0.00019191891891891892,
      "loss": 1.9906,
      "step": 1383
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 1.4408801794052124,
      "learning_rate": 0.00019183783783783783,
      "loss": 1.7956,
      "step": 1384
    },
    {
      "epoch": 1.8466666666666667,
      "grad_norm": 1.5253825187683105,
      "learning_rate": 0.00019175675675675674,
      "loss": 1.5602,
      "step": 1385
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 1.4740065336227417,
      "learning_rate": 0.00019167567567567566,
      "loss": 1.9305,
      "step": 1386
    },
    {
      "epoch": 1.8493333333333335,
      "grad_norm": 1.5816195011138916,
      "learning_rate": 0.0001915945945945946,
      "loss": 1.9453,
      "step": 1387
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 1.3626327514648438,
      "learning_rate": 0.0001915135135135135,
      "loss": 1.3923,
      "step": 1388
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 1.5350620746612549,
      "learning_rate": 0.00019143243243243242,
      "loss": 1.9567,
      "step": 1389
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 1.5262030363082886,
      "learning_rate": 0.00019135135135135134,
      "loss": 2.0418,
      "step": 1390
    },
    {
      "epoch": 1.8546666666666667,
      "grad_norm": 1.5115886926651,
      "learning_rate": 0.00019127027027027025,
      "loss": 1.7957,
      "step": 1391
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 1.4060776233673096,
      "learning_rate": 0.0001911891891891892,
      "loss": 1.4815,
      "step": 1392
    },
    {
      "epoch": 1.8573333333333333,
      "grad_norm": 1.607085943222046,
      "learning_rate": 0.0001911081081081081,
      "loss": 2.0624,
      "step": 1393
    },
    {
      "epoch": 1.8586666666666667,
      "grad_norm": 1.526913046836853,
      "learning_rate": 0.00019102702702702702,
      "loss": 1.6301,
      "step": 1394
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 1.5492216348648071,
      "learning_rate": 0.00019094594594594593,
      "loss": 1.7711,
      "step": 1395
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 1.46433424949646,
      "learning_rate": 0.00019086486486486487,
      "loss": 1.7633,
      "step": 1396
    },
    {
      "epoch": 1.8626666666666667,
      "grad_norm": 1.5933773517608643,
      "learning_rate": 0.00019078378378378379,
      "loss": 2.092,
      "step": 1397
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 1.4759494066238403,
      "learning_rate": 0.00019070270270270267,
      "loss": 1.7533,
      "step": 1398
    },
    {
      "epoch": 1.8653333333333333,
      "grad_norm": 1.473698377609253,
      "learning_rate": 0.00019062162162162158,
      "loss": 1.8585,
      "step": 1399
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 1.6679391860961914,
      "learning_rate": 0.0001905405405405405,
      "loss": 1.8135,
      "step": 1400
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 1.5277589559555054,
      "learning_rate": 0.00019045945945945944,
      "loss": 1.8831,
      "step": 1401
    },
    {
      "epoch": 1.8693333333333333,
      "grad_norm": 1.5415401458740234,
      "learning_rate": 0.00019037837837837835,
      "loss": 1.9074,
      "step": 1402
    },
    {
      "epoch": 1.8706666666666667,
      "grad_norm": 1.4998278617858887,
      "learning_rate": 0.00019029729729729727,
      "loss": 1.2575,
      "step": 1403
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 1.716232419013977,
      "learning_rate": 0.00019021621621621618,
      "loss": 1.7132,
      "step": 1404
    },
    {
      "epoch": 1.8733333333333333,
      "grad_norm": 1.278572678565979,
      "learning_rate": 0.00019013513513513512,
      "loss": 1.2885,
      "step": 1405
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 1.500101923942566,
      "learning_rate": 0.00019005405405405403,
      "loss": 1.7902,
      "step": 1406
    },
    {
      "epoch": 1.876,
      "grad_norm": 1.4247740507125854,
      "learning_rate": 0.00018997297297297295,
      "loss": 1.7925,
      "step": 1407
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 1.5986733436584473,
      "learning_rate": 0.00018989189189189186,
      "loss": 1.7652,
      "step": 1408
    },
    {
      "epoch": 1.8786666666666667,
      "grad_norm": 1.4165865182876587,
      "learning_rate": 0.0001898108108108108,
      "loss": 1.8579,
      "step": 1409
    },
    {
      "epoch": 1.88,
      "grad_norm": 1.5324089527130127,
      "learning_rate": 0.0001897297297297297,
      "loss": 2.0258,
      "step": 1410
    },
    {
      "epoch": 1.8813333333333333,
      "grad_norm": 1.429477334022522,
      "learning_rate": 0.00018964864864864863,
      "loss": 2.027,
      "step": 1411
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 1.3240360021591187,
      "learning_rate": 0.00018956756756756754,
      "loss": 1.59,
      "step": 1412
    },
    {
      "epoch": 1.884,
      "grad_norm": 1.5138698816299438,
      "learning_rate": 0.00018948648648648648,
      "loss": 1.5089,
      "step": 1413
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 1.4753128290176392,
      "learning_rate": 0.0001894054054054054,
      "loss": 1.9579,
      "step": 1414
    },
    {
      "epoch": 1.8866666666666667,
      "grad_norm": 1.4233976602554321,
      "learning_rate": 0.0001893243243243243,
      "loss": 1.6845,
      "step": 1415
    },
    {
      "epoch": 1.888,
      "grad_norm": 1.4889847040176392,
      "learning_rate": 0.00018924324324324322,
      "loss": 1.9688,
      "step": 1416
    },
    {
      "epoch": 1.8893333333333333,
      "grad_norm": 1.5490645170211792,
      "learning_rate": 0.00018916216216216213,
      "loss": 1.6342,
      "step": 1417
    },
    {
      "epoch": 1.8906666666666667,
      "grad_norm": 1.3843804597854614,
      "learning_rate": 0.00018908108108108107,
      "loss": 1.6325,
      "step": 1418
    },
    {
      "epoch": 1.892,
      "grad_norm": 1.3986862897872925,
      "learning_rate": 0.00018899999999999999,
      "loss": 1.5961,
      "step": 1419
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 1.4442470073699951,
      "learning_rate": 0.0001889189189189189,
      "loss": 1.9104,
      "step": 1420
    },
    {
      "epoch": 1.8946666666666667,
      "grad_norm": 1.3849999904632568,
      "learning_rate": 0.0001888378378378378,
      "loss": 1.593,
      "step": 1421
    },
    {
      "epoch": 1.896,
      "grad_norm": 1.5186915397644043,
      "learning_rate": 0.00018875675675675675,
      "loss": 1.6597,
      "step": 1422
    },
    {
      "epoch": 1.8973333333333333,
      "grad_norm": 1.5293251276016235,
      "learning_rate": 0.00018867567567567567,
      "loss": 1.7521,
      "step": 1423
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 1.4030274152755737,
      "learning_rate": 0.00018859459459459458,
      "loss": 1.8376,
      "step": 1424
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.439387321472168,
      "learning_rate": 0.0001885135135135135,
      "loss": 1.774,
      "step": 1425
    },
    {
      "epoch": 1.9013333333333333,
      "grad_norm": 1.4654960632324219,
      "learning_rate": 0.00018843243243243243,
      "loss": 2.1002,
      "step": 1426
    },
    {
      "epoch": 1.9026666666666667,
      "grad_norm": 1.418086290359497,
      "learning_rate": 0.00018835135135135135,
      "loss": 2.055,
      "step": 1427
    },
    {
      "epoch": 1.904,
      "grad_norm": 1.414825439453125,
      "learning_rate": 0.00018827027027027026,
      "loss": 1.7065,
      "step": 1428
    },
    {
      "epoch": 1.9053333333333333,
      "grad_norm": 1.4965784549713135,
      "learning_rate": 0.00018818918918918917,
      "loss": 1.9482,
      "step": 1429
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 1.4463953971862793,
      "learning_rate": 0.00018810810810810811,
      "loss": 1.8661,
      "step": 1430
    },
    {
      "epoch": 1.908,
      "grad_norm": 1.249574065208435,
      "learning_rate": 0.00018802702702702703,
      "loss": 1.0446,
      "step": 1431
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 1.5314041376113892,
      "learning_rate": 0.00018794594594594594,
      "loss": 1.7007,
      "step": 1432
    },
    {
      "epoch": 1.9106666666666667,
      "grad_norm": 1.4454965591430664,
      "learning_rate": 0.00018786486486486485,
      "loss": 2.0508,
      "step": 1433
    },
    {
      "epoch": 1.912,
      "grad_norm": 1.5990080833435059,
      "learning_rate": 0.0001877837837837838,
      "loss": 2.1358,
      "step": 1434
    },
    {
      "epoch": 1.9133333333333333,
      "grad_norm": 1.4563310146331787,
      "learning_rate": 0.0001877027027027027,
      "loss": 1.1925,
      "step": 1435
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 1.4947665929794312,
      "learning_rate": 0.00018762162162162162,
      "loss": 1.9338,
      "step": 1436
    },
    {
      "epoch": 1.916,
      "grad_norm": 1.4518095254898071,
      "learning_rate": 0.00018754054054054053,
      "loss": 1.6047,
      "step": 1437
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 1.445123314857483,
      "learning_rate": 0.00018745945945945942,
      "loss": 1.7086,
      "step": 1438
    },
    {
      "epoch": 1.9186666666666667,
      "grad_norm": 1.4823417663574219,
      "learning_rate": 0.00018737837837837833,
      "loss": 1.9249,
      "step": 1439
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.4530924558639526,
      "learning_rate": 0.00018729729729729727,
      "loss": 1.9642,
      "step": 1440
    },
    {
      "epoch": 1.9213333333333333,
      "grad_norm": 1.4692672491073608,
      "learning_rate": 0.0001872162162162162,
      "loss": 1.5712,
      "step": 1441
    },
    {
      "epoch": 1.9226666666666667,
      "grad_norm": 1.4834668636322021,
      "learning_rate": 0.0001871351351351351,
      "loss": 1.7277,
      "step": 1442
    },
    {
      "epoch": 1.924,
      "grad_norm": 1.4810057878494263,
      "learning_rate": 0.000187054054054054,
      "loss": 2.113,
      "step": 1443
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 1.4623568058013916,
      "learning_rate": 0.00018697297297297295,
      "loss": 1.8525,
      "step": 1444
    },
    {
      "epoch": 1.9266666666666667,
      "grad_norm": 1.4759794473648071,
      "learning_rate": 0.00018689189189189187,
      "loss": 1.6861,
      "step": 1445
    },
    {
      "epoch": 1.928,
      "grad_norm": 1.4718955755233765,
      "learning_rate": 0.00018681081081081078,
      "loss": 1.7956,
      "step": 1446
    },
    {
      "epoch": 1.9293333333333333,
      "grad_norm": 1.4549552202224731,
      "learning_rate": 0.0001867297297297297,
      "loss": 1.807,
      "step": 1447
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 1.4986412525177002,
      "learning_rate": 0.00018664864864864863,
      "loss": 1.8775,
      "step": 1448
    },
    {
      "epoch": 1.932,
      "grad_norm": 1.5158746242523193,
      "learning_rate": 0.00018656756756756755,
      "loss": 1.5564,
      "step": 1449
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 1.5534597635269165,
      "learning_rate": 0.00018648648648648646,
      "loss": 1.8146,
      "step": 1450
    },
    {
      "epoch": 1.9346666666666668,
      "grad_norm": 1.567732810974121,
      "learning_rate": 0.00018640540540540537,
      "loss": 1.5541,
      "step": 1451
    },
    {
      "epoch": 1.936,
      "grad_norm": 1.495374083518982,
      "learning_rate": 0.00018632432432432431,
      "loss": 1.6024,
      "step": 1452
    },
    {
      "epoch": 1.9373333333333334,
      "grad_norm": 1.6140018701553345,
      "learning_rate": 0.00018624324324324323,
      "loss": 1.7888,
      "step": 1453
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 1.435775876045227,
      "learning_rate": 0.00018616216216216214,
      "loss": 1.857,
      "step": 1454
    },
    {
      "epoch": 1.94,
      "grad_norm": 1.5030032396316528,
      "learning_rate": 0.00018608108108108105,
      "loss": 1.6635,
      "step": 1455
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 1.418713092803955,
      "learning_rate": 0.000186,
      "loss": 2.0738,
      "step": 1456
    },
    {
      "epoch": 1.9426666666666668,
      "grad_norm": 1.43875253200531,
      "learning_rate": 0.0001859189189189189,
      "loss": 1.8221,
      "step": 1457
    },
    {
      "epoch": 1.944,
      "grad_norm": 1.4366109371185303,
      "learning_rate": 0.00018583783783783782,
      "loss": 2.0322,
      "step": 1458
    },
    {
      "epoch": 1.9453333333333334,
      "grad_norm": 1.468660831451416,
      "learning_rate": 0.00018575675675675673,
      "loss": 2.0864,
      "step": 1459
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 1.596169114112854,
      "learning_rate": 0.00018567567567567567,
      "loss": 1.7397,
      "step": 1460
    },
    {
      "epoch": 1.948,
      "grad_norm": 1.4440408945083618,
      "learning_rate": 0.0001855945945945946,
      "loss": 1.9459,
      "step": 1461
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 1.4800323247909546,
      "learning_rate": 0.0001855135135135135,
      "loss": 1.8233,
      "step": 1462
    },
    {
      "epoch": 1.9506666666666668,
      "grad_norm": 1.560029149055481,
      "learning_rate": 0.00018543243243243241,
      "loss": 1.9814,
      "step": 1463
    },
    {
      "epoch": 1.952,
      "grad_norm": 1.4618937969207764,
      "learning_rate": 0.00018535135135135133,
      "loss": 1.991,
      "step": 1464
    },
    {
      "epoch": 1.9533333333333334,
      "grad_norm": 1.4923006296157837,
      "learning_rate": 0.00018527027027027027,
      "loss": 1.9498,
      "step": 1465
    },
    {
      "epoch": 1.9546666666666668,
      "grad_norm": 1.4595636129379272,
      "learning_rate": 0.00018518918918918918,
      "loss": 1.6217,
      "step": 1466
    },
    {
      "epoch": 1.956,
      "grad_norm": 1.4193106889724731,
      "learning_rate": 0.0001851081081081081,
      "loss": 1.6824,
      "step": 1467
    },
    {
      "epoch": 1.9573333333333334,
      "grad_norm": 1.4337371587753296,
      "learning_rate": 0.000185027027027027,
      "loss": 1.6173,
      "step": 1468
    },
    {
      "epoch": 1.9586666666666668,
      "grad_norm": 1.4892101287841797,
      "learning_rate": 0.00018494594594594595,
      "loss": 1.83,
      "step": 1469
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.6357645988464355,
      "learning_rate": 0.00018486486486486486,
      "loss": 2.0027,
      "step": 1470
    },
    {
      "epoch": 1.9613333333333334,
      "grad_norm": 1.4519914388656616,
      "learning_rate": 0.00018478378378378378,
      "loss": 1.9696,
      "step": 1471
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 1.5106576681137085,
      "learning_rate": 0.0001847027027027027,
      "loss": 1.7911,
      "step": 1472
    },
    {
      "epoch": 1.964,
      "grad_norm": 1.6642975807189941,
      "learning_rate": 0.00018462162162162163,
      "loss": 1.8368,
      "step": 1473
    },
    {
      "epoch": 1.9653333333333334,
      "grad_norm": 1.564558744430542,
      "learning_rate": 0.00018454054054054054,
      "loss": 1.7903,
      "step": 1474
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 1.3939772844314575,
      "learning_rate": 0.00018445945945945946,
      "loss": 1.7486,
      "step": 1475
    },
    {
      "epoch": 1.968,
      "grad_norm": 1.5136852264404297,
      "learning_rate": 0.00018437837837837837,
      "loss": 1.7664,
      "step": 1476
    },
    {
      "epoch": 1.9693333333333334,
      "grad_norm": 1.5400562286376953,
      "learning_rate": 0.0001842972972972973,
      "loss": 1.638,
      "step": 1477
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 1.3616037368774414,
      "learning_rate": 0.0001842162162162162,
      "loss": 1.7167,
      "step": 1478
    },
    {
      "epoch": 1.972,
      "grad_norm": 1.4236239194869995,
      "learning_rate": 0.0001841351351351351,
      "loss": 1.683,
      "step": 1479
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 1.635716438293457,
      "learning_rate": 0.00018405405405405402,
      "loss": 1.9499,
      "step": 1480
    },
    {
      "epoch": 1.9746666666666668,
      "grad_norm": 1.3772870302200317,
      "learning_rate": 0.00018397297297297294,
      "loss": 1.5802,
      "step": 1481
    },
    {
      "epoch": 1.976,
      "grad_norm": 1.3554530143737793,
      "learning_rate": 0.00018389189189189188,
      "loss": 1.697,
      "step": 1482
    },
    {
      "epoch": 1.9773333333333334,
      "grad_norm": 1.4128817319869995,
      "learning_rate": 0.0001838108108108108,
      "loss": 1.6457,
      "step": 1483
    },
    {
      "epoch": 1.9786666666666668,
      "grad_norm": 1.427955985069275,
      "learning_rate": 0.0001837297297297297,
      "loss": 1.6648,
      "step": 1484
    },
    {
      "epoch": 1.98,
      "grad_norm": 1.4105199575424194,
      "learning_rate": 0.00018364864864864862,
      "loss": 1.7877,
      "step": 1485
    },
    {
      "epoch": 1.9813333333333332,
      "grad_norm": 1.6060662269592285,
      "learning_rate": 0.00018356756756756753,
      "loss": 1.9296,
      "step": 1486
    },
    {
      "epoch": 1.9826666666666668,
      "grad_norm": 1.4144598245620728,
      "learning_rate": 0.00018348648648648647,
      "loss": 1.7147,
      "step": 1487
    },
    {
      "epoch": 1.984,
      "grad_norm": 1.321277141571045,
      "learning_rate": 0.00018340540540540538,
      "loss": 1.7125,
      "step": 1488
    },
    {
      "epoch": 1.9853333333333332,
      "grad_norm": 1.519315481185913,
      "learning_rate": 0.0001833243243243243,
      "loss": 1.99,
      "step": 1489
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 1.3879189491271973,
      "learning_rate": 0.0001832432432432432,
      "loss": 1.7044,
      "step": 1490
    },
    {
      "epoch": 1.988,
      "grad_norm": 1.5050537586212158,
      "learning_rate": 0.00018316216216216215,
      "loss": 1.6793,
      "step": 1491
    },
    {
      "epoch": 1.9893333333333332,
      "grad_norm": 1.4402408599853516,
      "learning_rate": 0.00018308108108108106,
      "loss": 1.62,
      "step": 1492
    },
    {
      "epoch": 1.9906666666666668,
      "grad_norm": 1.451419472694397,
      "learning_rate": 0.00018299999999999998,
      "loss": 1.5073,
      "step": 1493
    },
    {
      "epoch": 1.992,
      "grad_norm": 1.4713846445083618,
      "learning_rate": 0.0001829189189189189,
      "loss": 1.3377,
      "step": 1494
    },
    {
      "epoch": 1.9933333333333332,
      "grad_norm": 1.7570747137069702,
      "learning_rate": 0.00018283783783783783,
      "loss": 1.9507,
      "step": 1495
    },
    {
      "epoch": 1.9946666666666668,
      "grad_norm": 1.6135326623916626,
      "learning_rate": 0.00018275675675675674,
      "loss": 1.9235,
      "step": 1496
    },
    {
      "epoch": 1.996,
      "grad_norm": 1.5978773832321167,
      "learning_rate": 0.00018267567567567566,
      "loss": 1.4527,
      "step": 1497
    },
    {
      "epoch": 1.9973333333333332,
      "grad_norm": 1.5819759368896484,
      "learning_rate": 0.00018259459459459457,
      "loss": 1.5394,
      "step": 1498
    },
    {
      "epoch": 1.9986666666666668,
      "grad_norm": 1.5701038837432861,
      "learning_rate": 0.0001825135135135135,
      "loss": 2.0432,
      "step": 1499
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.5040650367736816,
      "learning_rate": 0.00018243243243243242,
      "loss": 1.6597,
      "step": 1500
    }
  ],
  "logging_steps": 1,
  "max_steps": 3750,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.5104264732672e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
